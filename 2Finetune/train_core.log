nohup: ignoring input
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:50:12,848] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:50:16 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20479
[2024-11-18 21:50:18,815] torch.distributed.run: [WARNING] 
[2024-11-18 21:50:18,815] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 21:50:18,815] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 21:50:18,815] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:50:25,817] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 21:50:26,176] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:50:26 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:50:26 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 21:50:26,854 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:50:26,856 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:26,860 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:26,860 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:26,860 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:26,860 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:26,860 >> loading file tokenizer_config.json
11/18/2024 21:50:27 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:50:27 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 21:50:28,273 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:50:28,274 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:28,276 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:28,276 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:28,276 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:28,276 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:50:28,276 >> loading file tokenizer_config.json
11/18/2024 21:50:29 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
11/18/2024 21:50:29 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
11/18/2024 21:50:30 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 896 examples [00:00, 39996.34 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 532.79 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3500.86 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 21:50:33 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:03<00:49, 16.95 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:04<00:30, 25.78 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:06<00:14, 45.33 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:06<00:11, 52.51 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:08<00:11, 47.80 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:08<00:06, 71.22 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:09<00:05, 73.49 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:10<00:02, 96.08 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:10<00:02, 91.87 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:12<00:02, 71.62 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:12<00:01, 90.27 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:12<00:00, 93.97 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:13<00:00, 102.73 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:13<00:00, 66.94 examples/s] 
training example:
input_ids:
[2, 106, 1645, 108, 236343, 236007, 18390, 17689, 48982, 156055, 235695, 46555, 235370, 10687, 9263, 23258, 235292, 108, 81896, 235735, 21175, 235298, 9406, 866, 520, 235298, 195275, 131359, 11043, 236382, 235370, 99721, 235394, 153697, 235394, 48982, 235581, 238126, 237005, 235365, 236203, 24484, 44697, 235370, 6217, 235581, 29173, 235365, 22234, 236784, 44697, 29173, 76074, 107, 108, 106, 2516, 108, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
inputs:
<bos><start_of_turn>user
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<end_of_turn>
<start_of_turn>model
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
label_ids:
[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
labels:
<eos>match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
[INFO|configuration_utils.py:673] 2024-11-18 21:50:48,284 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:50:48,285 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

11/18/2024 21:50:48 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
[INFO|modeling_utils.py:3729] 2024-11-18 21:50:48,331 >> loading weights file /home/work/liuytest/demo/gemma-2-9b-it/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 21:50:48,331 >> Instantiating Gemma2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 21:50:48,333 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]11/18/2024 21:50:48 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 21:50:53,977 >> All model checkpoint weights were used when initializing Gemma2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 21:50:53,977 >> All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/gemma-2-9b-it.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 21:50:53,984 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 21:50:53,984 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "max_length": 8192,
  "pad_token_id": 0
}

11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:50:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:50:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,up_proj,v_proj,down_proj,o_proj,k_proj,q_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:50:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:50:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:50:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,o_proj,v_proj,k_proj,q_proj,down_proj
11/18/2024 21:50:54 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 21:50:54,968 >> Using auto half precision backend
11/18/2024 21:50:55 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
[INFO|trainer.py:2243] 2024-11-18 21:50:56,860 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 21:50:56,860 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 21:50:56,860 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 21:50:56,860 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 21:50:56,860 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 21:50:56,860 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 21:50:56,860 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 21:50:56,870 >>   Number of trainable parameters = 27,009,024
  0%|          | 0/150 [00:00<?, ?it/s]/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
  1%|          | 1/150 [00:06<15:29,  6.24s/it]  1%|▏         | 2/150 [00:12<14:52,  6.03s/it]  2%|▏         | 3/150 [00:17<14:34,  5.95s/it]  3%|▎         | 4/150 [00:23<14:17,  5.88s/it]  3%|▎         | 5/150 [00:29<13:58,  5.78s/it]  4%|▍         | 6/150 [00:35<13:47,  5.75s/it]  5%|▍         | 7/150 [00:40<13:37,  5.72s/it]  5%|▌         | 8/150 [00:46<13:29,  5.70s/it]  6%|▌         | 9/150 [00:51<13:21,  5.68s/it]  7%|▋         | 10/150 [00:57<13:15,  5.68s/it]                                                {'loss': 1.7346, 'grad_norm': 2.4213197231292725, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:57<13:15,  5.68s/it]  7%|▋         | 11/150 [01:03<13:07,  5.67s/it]  8%|▊         | 12/150 [01:08<12:58,  5.64s/it]  9%|▊         | 13/150 [01:14<12:53,  5.64s/it]  9%|▉         | 14/150 [01:20<12:47,  5.64s/it] 10%|█         | 15/150 [01:25<12:43,  5.66s/it] 11%|█         | 16/150 [01:31<12:39,  5.67s/it] 11%|█▏        | 17/150 [01:37<12:37,  5.69s/it] 12%|█▏        | 18/150 [01:43<12:31,  5.69s/it] 13%|█▎        | 19/150 [01:48<12:29,  5.72s/it] 13%|█▎        | 20/150 [01:54<12:27,  5.75s/it]                                                {'loss': 0.4261, 'grad_norm': 1.3046492338180542, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:54<12:27,  5.75s/it] 14%|█▍        | 21/150 [02:00<12:22,  5.75s/it] 15%|█▍        | 22/150 [02:06<12:14,  5.74s/it] 15%|█▌        | 23/150 [02:11<12:06,  5.72s/it] 16%|█▌        | 24/150 [02:17<12:01,  5.73s/it] 17%|█▋        | 25/150 [02:23<11:55,  5.73s/it] 17%|█▋        | 26/150 [02:29<11:52,  5.75s/it] 18%|█▊        | 27/150 [02:34<11:47,  5.75s/it] 19%|█▊        | 28/150 [02:40<11:44,  5.77s/it] 19%|█▉        | 29/150 [02:46<11:41,  5.80s/it] 20%|██        | 30/150 [02:52<11:38,  5.82s/it]                                                {'loss': 0.1362, 'grad_norm': 0.800064742565155, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:52<11:38,  5.82s/it] 21%|██        | 31/150 [02:58<11:30,  5.81s/it] 21%|██▏       | 32/150 [03:03<11:24,  5.80s/it] 22%|██▏       | 33/150 [03:09<11:17,  5.79s/it] 23%|██▎       | 34/150 [03:15<11:11,  5.79s/it] 23%|██▎       | 35/150 [03:21<11:06,  5.79s/it] 24%|██▍       | 36/150 [03:27<11:00,  5.79s/it] 25%|██▍       | 37/150 [03:32<10:53,  5.79s/it] 25%|██▌       | 38/150 [03:38<10:49,  5.80s/it] 26%|██▌       | 39/150 [03:44<10:46,  5.82s/it] 27%|██▋       | 40/150 [03:50<10:45,  5.87s/it]                                                {'loss': 0.0906, 'grad_norm': 0.5577713251113892, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [03:50<10:45,  5.87s/it] 27%|██▋       | 41/150 [03:56<10:44,  5.91s/it] 28%|██▊       | 42/150 [04:02<10:38,  5.92s/it] 29%|██▊       | 43/150 [04:08<10:28,  5.88s/it] 29%|██▉       | 44/150 [04:14<10:20,  5.85s/it] 30%|███       | 45/150 [04:19<10:14,  5.85s/it] 31%|███       | 46/150 [04:25<10:10,  5.87s/it] 31%|███▏      | 47/150 [04:31<10:15,  5.98s/it] 32%|███▏      | 48/150 [04:37<10:08,  5.97s/it] 33%|███▎      | 49/150 [04:43<09:58,  5.92s/it] 33%|███▎      | 50/150 [04:49<09:49,  5.90s/it]                                                {'loss': 0.0694, 'grad_norm': 0.35006359219551086, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [04:49<09:49,  5.90s/it] 34%|███▍      | 51/150 [04:55<09:41,  5.88s/it] 35%|███▍      | 52/150 [05:01<09:35,  5.88s/it] 35%|███▌      | 53/150 [05:07<09:30,  5.88s/it] 36%|███▌      | 54/150 [05:13<09:24,  5.88s/it] 37%|███▋      | 55/150 [05:18<09:18,  5.88s/it] 37%|███▋      | 56/150 [05:24<09:12,  5.87s/it] 38%|███▊      | 57/150 [05:30<09:05,  5.87s/it] 39%|███▊      | 58/150 [05:36<08:59,  5.87s/it] 39%|███▉      | 59/150 [05:42<08:52,  5.86s/it] 40%|████      | 60/150 [05:48<08:47,  5.86s/it]                                                {'loss': 0.0547, 'grad_norm': 0.19103947281837463, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [05:48<08:47,  5.86s/it] 41%|████      | 61/150 [05:54<08:42,  5.87s/it] 41%|████▏     | 62/150 [05:59<08:36,  5.87s/it] 42%|████▏     | 63/150 [06:05<08:32,  5.89s/it] 43%|████▎     | 64/150 [06:11<08:24,  5.87s/it] 43%|████▎     | 65/150 [06:17<08:18,  5.86s/it] 44%|████▍     | 66/150 [06:23<08:12,  5.86s/it] 45%|████▍     | 67/150 [06:29<08:05,  5.85s/it] 45%|████▌     | 68/150 [06:35<07:59,  5.85s/it] 46%|████▌     | 69/150 [06:40<07:53,  5.85s/it] 47%|████▋     | 70/150 [06:46<07:49,  5.87s/it]                                                {'loss': 0.0466, 'grad_norm': 0.2780781388282776, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [06:46<07:49,  5.87s/it] 47%|████▋     | 71/150 [06:52<07:43,  5.87s/it] 48%|████▊     | 72/150 [06:58<07:36,  5.86s/it] 49%|████▊     | 73/150 [07:04<07:29,  5.84s/it] 49%|████▉     | 74/150 [07:10<07:23,  5.84s/it] 50%|█████     | 75/150 [07:16<07:18,  5.84s/it] 51%|█████     | 76/150 [07:21<07:12,  5.85s/it] 51%|█████▏    | 77/150 [07:27<07:07,  5.85s/it] 52%|█████▏    | 78/150 [07:33<07:01,  5.85s/it] 53%|█████▎    | 79/150 [07:39<06:54,  5.84s/it] 53%|█████▎    | 80/150 [07:45<06:51,  5.87s/it]                                                {'loss': 0.0449, 'grad_norm': 0.33694249391555786, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [07:45<06:51,  5.87s/it] 54%|█████▍    | 81/150 [07:51<06:42,  5.84s/it] 55%|█████▍    | 82/150 [07:56<06:36,  5.83s/it] 55%|█████▌    | 83/150 [08:02<06:30,  5.82s/it] 56%|█████▌    | 84/150 [08:08<06:22,  5.80s/it] 57%|█████▋    | 85/150 [08:14<06:18,  5.82s/it] 57%|█████▋    | 86/150 [08:20<06:12,  5.83s/it] 58%|█████▊    | 87/150 [08:26<06:08,  5.85s/it] 59%|█████▊    | 88/150 [08:32<06:03,  5.86s/it] 59%|█████▉    | 89/150 [08:37<05:57,  5.86s/it] 60%|██████    | 90/150 [08:43<05:51,  5.86s/it]                                                {'loss': 0.0524, 'grad_norm': 0.26137301325798035, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [08:43<05:51,  5.86s/it] 61%|██████    | 91/150 [08:49<05:45,  5.85s/it] 61%|██████▏   | 92/150 [08:55<05:39,  5.86s/it] 62%|██████▏   | 93/150 [09:01<05:35,  5.89s/it] 63%|██████▎   | 94/150 [09:07<05:29,  5.88s/it] 63%|██████▎   | 95/150 [09:13<05:23,  5.88s/it] 64%|██████▍   | 96/150 [09:19<05:18,  5.90s/it] 65%|██████▍   | 97/150 [09:24<05:11,  5.89s/it] 65%|██████▌   | 98/150 [09:30<05:05,  5.87s/it] 66%|██████▌   | 99/150 [09:36<04:59,  5.88s/it] 67%|██████▋   | 100/150 [09:42<04:53,  5.87s/it]                                                 {'loss': 0.0478, 'grad_norm': 0.22210466861724854, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [09:42<04:53,  5.87s/it] 67%|██████▋   | 101/150 [09:48<04:46,  5.86s/it] 68%|██████▊   | 102/150 [09:54<04:41,  5.86s/it] 69%|██████▊   | 103/150 [10:00<04:34,  5.85s/it] 69%|██████▉   | 104/150 [10:05<04:29,  5.85s/it] 70%|███████   | 105/150 [10:11<04:23,  5.85s/it] 71%|███████   | 106/150 [10:17<04:17,  5.85s/it] 71%|███████▏  | 107/150 [10:23<04:10,  5.84s/it] 72%|███████▏  | 108/150 [10:29<04:04,  5.82s/it] 73%|███████▎  | 109/150 [10:35<03:58,  5.82s/it] 73%|███████▎  | 110/150 [10:40<03:52,  5.81s/it]                                                 {'loss': 0.0406, 'grad_norm': 0.21999125182628632, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [10:40<03:52,  5.81s/it] 74%|███████▍  | 111/150 [10:46<03:46,  5.81s/it] 75%|███████▍  | 112/150 [10:52<03:40,  5.81s/it] 75%|███████▌  | 113/150 [10:58<03:35,  5.82s/it] 76%|███████▌  | 114/150 [11:04<03:29,  5.82s/it] 77%|███████▋  | 115/150 [11:09<03:23,  5.82s/it] 77%|███████▋  | 116/150 [11:15<03:17,  5.82s/it] 78%|███████▊  | 117/150 [11:21<03:11,  5.81s/it] 79%|███████▊  | 118/150 [11:27<03:05,  5.79s/it] 79%|███████▉  | 119/150 [11:33<02:59,  5.79s/it] 80%|████████  | 120/150 [11:38<02:54,  5.80s/it]                                                 {'loss': 0.0441, 'grad_norm': 0.34325817227363586, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [11:38<02:54,  5.80s/it] 81%|████████  | 121/150 [11:44<02:48,  5.82s/it] 81%|████████▏ | 122/150 [11:50<02:43,  5.82s/it] 82%|████████▏ | 123/150 [11:56<02:39,  5.89s/it] 83%|████████▎ | 124/150 [12:02<02:31,  5.83s/it] 83%|████████▎ | 125/150 [12:08<02:24,  5.79s/it] 84%|████████▍ | 126/150 [12:13<02:18,  5.77s/it] 85%|████████▍ | 127/150 [12:19<02:12,  5.77s/it] 85%|████████▌ | 128/150 [12:25<02:06,  5.75s/it] 86%|████████▌ | 129/150 [12:30<02:00,  5.74s/it] 87%|████████▋ | 130/150 [12:36<01:55,  5.76s/it]                                                 {'loss': 0.0297, 'grad_norm': 0.13420946896076202, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [12:36<01:55,  5.76s/it] 87%|████████▋ | 131/150 [12:42<01:49,  5.78s/it] 88%|████████▊ | 132/150 [12:48<01:44,  5.79s/it] 89%|████████▊ | 133/150 [12:54<01:38,  5.79s/it] 89%|████████▉ | 134/150 [12:59<01:32,  5.79s/it] 90%|█████████ | 135/150 [13:05<01:26,  5.78s/it] 91%|█████████ | 136/150 [13:11<01:21,  5.81s/it] 91%|█████████▏| 137/150 [13:17<01:15,  5.81s/it] 92%|█████████▏| 138/150 [13:23<01:09,  5.80s/it] 93%|█████████▎| 139/150 [13:28<01:03,  5.79s/it] 93%|█████████▎| 140/150 [13:34<00:57,  5.78s/it]                                                 {'loss': 0.0305, 'grad_norm': 0.35889506340026855, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [13:34<00:57,  5.78s/it] 94%|█████████▍| 141/150 [13:40<00:51,  5.77s/it] 95%|█████████▍| 142/150 [13:46<00:46,  5.78s/it] 95%|█████████▌| 143/150 [13:51<00:40,  5.76s/it] 96%|█████████▌| 144/150 [13:57<00:34,  5.75s/it] 97%|█████████▋| 145/150 [14:03<00:28,  5.77s/it] 97%|█████████▋| 146/150 [14:09<00:23,  5.79s/it] 98%|█████████▊| 147/150 [14:15<00:17,  5.79s/it] 99%|█████████▊| 148/150 [14:20<00:11,  5.78s/it] 99%|█████████▉| 149/150 [14:26<00:05,  5.79s/it]100%|██████████| 150/150 [14:32<00:00,  5.82s/it]                                                 {'loss': 0.0412, 'grad_norm': 0.19996221363544464, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [14:32<00:00,  5.82s/it][INFO|trainer.py:3705] 2024-11-18 22:05:30,336 >> Saving model checkpoint to saves/gemma-2-9b-it/normal_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:05:30,380 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:05:30,382 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:05:30,602 >> tokenizer config file saved in saves/gemma-2-9b-it/normal_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:05:30,603 >> Special tokens file saved in saves/gemma-2-9b-it/normal_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:05:31,835 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 874.965, 'train_samples_per_second': 2.764, 'train_steps_per_second': 0.171, 'train_loss': 0.1926350231965383, 'epoch': 2.98}
100%|██████████| 150/150 [14:34<00:00,  5.82s/it]100%|██████████| 150/150 [14:34<00:00,  5.83s/it]
[INFO|trainer.py:3705] 2024-11-18 22:05:31,838 >> Saving model checkpoint to saves/gemma-2-9b-it/normal_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:05:31,877 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:05:31,878 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:05:32,086 >> tokenizer config file saved in saves/gemma-2-9b-it/normal_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:05:32,087 >> Special tokens file saved in saves/gemma-2-9b-it/normal_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 14004661GF
  train_loss               =     0.1926
  train_runtime            = 0:14:34.96
  train_samples_per_second =      2.764
  train_steps_per_second   =      0.171
Figure saved at: saves/gemma-2-9b-it/normal_prompt/lora/sft/training_loss.png
11/18/2024 22:05:32 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:05:32 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:05:32,924 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:05:32,924 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:05:32,925 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:03, 10.80it/s]  9%|▉         | 4/45 [00:00<00:05,  6.86it/s] 11%|█         | 5/45 [00:00<00:06,  6.40it/s] 13%|█▎        | 6/45 [00:00<00:06,  6.06it/s] 16%|█▌        | 7/45 [00:01<00:06,  5.86it/s] 18%|█▊        | 8/45 [00:01<00:06,  5.74it/s] 20%|██        | 9/45 [00:01<00:06,  5.69it/s] 22%|██▏       | 10/45 [00:01<00:06,  5.64it/s] 24%|██▍       | 11/45 [00:01<00:06,  5.62it/s] 27%|██▋       | 12/45 [00:02<00:05,  5.60it/s] 29%|██▉       | 13/45 [00:02<00:05,  5.59it/s] 31%|███       | 14/45 [00:02<00:05,  5.59it/s] 33%|███▎      | 15/45 [00:02<00:05,  5.59it/s] 36%|███▌      | 16/45 [00:02<00:05,  5.59it/s] 38%|███▊      | 17/45 [00:02<00:05,  5.59it/s] 40%|████      | 18/45 [00:03<00:04,  5.58it/s] 42%|████▏     | 19/45 [00:03<00:04,  5.59it/s] 44%|████▍     | 20/45 [00:03<00:04,  5.50it/s] 47%|████▋     | 21/45 [00:03<00:04,  5.51it/s] 49%|████▉     | 22/45 [00:03<00:04,  5.51it/s] 51%|█████     | 23/45 [00:03<00:03,  5.51it/s] 53%|█████▎    | 24/45 [00:04<00:03,  5.52it/s] 56%|█████▌    | 25/45 [00:04<00:03,  5.49it/s] 58%|█████▊    | 26/45 [00:04<00:03,  5.46it/s] 60%|██████    | 27/45 [00:04<00:03,  5.41it/s] 62%|██████▏   | 28/45 [00:04<00:03,  5.36it/s] 64%|██████▍   | 29/45 [00:05<00:02,  5.36it/s] 67%|██████▋   | 30/45 [00:05<00:02,  5.35it/s] 69%|██████▉   | 31/45 [00:05<00:02,  5.34it/s] 71%|███████   | 32/45 [00:05<00:02,  5.33it/s] 73%|███████▎  | 33/45 [00:05<00:02,  5.35it/s] 76%|███████▌  | 34/45 [00:06<00:02,  5.36it/s] 78%|███████▊  | 35/45 [00:06<00:01,  5.37it/s] 80%|████████  | 36/45 [00:06<00:01,  5.41it/s] 82%|████████▏ | 37/45 [00:06<00:01,  5.41it/s] 84%|████████▍ | 38/45 [00:06<00:01,  5.40it/s] 87%|████████▋ | 39/45 [00:06<00:01,  5.37it/s] 89%|████████▉ | 40/45 [00:07<00:00,  5.37it/s] 91%|█████████ | 41/45 [00:07<00:00,  5.39it/s] 93%|█████████▎| 42/45 [00:07<00:00,  5.37it/s] 96%|█████████▌| 43/45 [00:07<00:00,  5.36it/s] 98%|█████████▊| 44/45 [00:07<00:00,  5.42it/s]100%|██████████| 45/45 [00:08<00:00,  5.45it/s]100%|██████████| 45/45 [00:08<00:00,  5.57it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0614
  eval_runtime            = 0:00:08.26
  eval_samples_per_second =     10.895
  eval_steps_per_second   =      5.447
[INFO|modelcard.py:449] 2024-11-18 22:05:41,186 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:06:01,196] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:06:05 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:24875
[2024-11-18 22:06:07,362] torch.distributed.run: [WARNING] 
[2024-11-18 22:06:07,362] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:06:07,362] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:06:07,362] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:06:14,699] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:06:14,983] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:06:15 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:06:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:06:15,969 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:06:15,970 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:15,974 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:15,975 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:15,975 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:15,975 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:15,975 >> loading file tokenizer_config.json
11/18/2024 22:06:16 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:06:16 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:06:17,458 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:06:17,461 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:17,464 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:17,464 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:17,464 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:17,464 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:06:17,464 >> loading file tokenizer_config.json
11/18/2024 22:06:18 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
11/18/2024 22:06:18 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
11/18/2024 22:06:19 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 896 examples [00:00, 8962.06 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 350.80 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2700.10 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:06:23 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:03<00:49, 16.96 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:03<00:24, 31.71 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:05<00:20, 35.26 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:06<00:09, 64.38 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:06<00:08, 69.89 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:07<00:06, 74.75 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:07<00:05, 78.50 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:09<00:06, 64.46 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:09<00:04, 70.22 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:10<00:03, 73.72 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:10<00:02, 95.60 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:11<00:01, 88.67 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:12<00:01, 72.69 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:12<00:00, 79.96 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:13<00:00, 68.25 examples/s]
training example:
input_ids:
[2, 106, 1645, 108, 236023, 235812, 46555, 235370, 17810, 12620, 37492, 235465, 108, 9766, 26472, 1192, 10890, 70617, 23055, 235640, 235465, 7139, 3179, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 16861, 559, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 33683, 2729, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 16861, 559, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 10004, 824, 664, 70617, 23055, 235640, 235465, 33683, 10004, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 10004, 824, 664, 70617, 23055, 235640, 235465, 9377, 26558, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 44040, 824, 664, 70617, 23055, 235640, 235465, 1815, 482, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 44040, 824, 664, 70617, 23055, 235640, 235465, 3212, 3688, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 189295, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 46852, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 46852, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 147022, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 7879, 235269, 182624, 23055, 235640, 235292, 44040, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 6717, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 964, 32771, 235269, 182624, 23055, 235640, 235292, 3531, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 25364, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 3212, 18843, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 73132, 1109, 235269, 182624, 23055, 235640, 235292, 3123, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 1638, 824, 664, 70617, 23055, 235640, 235465, 1433, 657, 1638, 559, 235269, 182624, 23055, 235640, 235292, 3123, 1638, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 1638, 12229, 664, 16983, 1192, 10890, 46788, 23055, 235292, 10004, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2737, 920, 777, 3312, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3531, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 920, 777, 1425, 7525, 21569, 664, 46788, 23055, 235292, 6717, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 4614, 920, 777, 18960, 920, 777, 52995, 920, 777, 14815, 920, 777, 57311, 920, 777, 57721, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 44040, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 920, 777, 1425, 7525, 21569, 664, 46788, 23055, 235292, 25364, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2563, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3123, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 7525, 21569, 664, 46788, 23055, 235292, 2729, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2737, 920, 777, 3312, 920, 777, 14815, 920, 777, 2502, 1716, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3123, 1638, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 7525, 235275, 193101, 108, 236343, 236007, 18390, 17689, 48982, 156055, 235695, 46555, 235370, 10687, 9263, 23258, 235292, 108, 81896, 235735, 21175, 235298, 9406, 866, 520, 235298, 195275, 131359, 11043, 236382, 235370, 99721, 235394, 153697, 235394, 48982, 235581, 238126, 237005, 235365, 236203, 24484, 44697, 235370, 6217, 235581, 29173, 235365, 22234, 236784, 44697, 29173, 76074, 107, 108, 106, 2516, 108, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
inputs:
<bos><start_of_turn>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<end_of_turn>
<start_of_turn>model
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
label_ids:
[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
labels:
<eos>match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
[INFO|configuration_utils.py:673] 2024-11-18 22:06:38,116 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:06:38,117 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

11/18/2024 22:06:38 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
[INFO|modeling_utils.py:3729] 2024-11-18 22:06:38,173 >> loading weights file /home/work/liuytest/demo/gemma-2-9b-it/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:06:38,174 >> Instantiating Gemma2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:06:38,177 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]11/18/2024 22:06:38 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:06:43,409 >> All model checkpoint weights were used when initializing Gemma2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:06:43,409 >> All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/gemma-2-9b-it.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:06:43,415 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:06:43,416 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "max_length": 8192,
  "pad_token_id": 0
}

11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:06:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:06:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,v_proj,q_proj,down_proj,k_proj,o_proj,gate_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:06:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:06:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:06:43 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,gate_proj,q_proj,k_proj,o_proj,v_proj,up_proj
11/18/2024 22:06:44 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:06:44,412 >> Using auto half precision backend
11/18/2024 22:06:44 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
[INFO|trainer.py:2243] 2024-11-18 22:06:45,074 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:06:45,074 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:06:45,074 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:06:45,074 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:06:45,074 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:06:45,074 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:06:45,075 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:06:45,084 >>   Number of trainable parameters = 27,009,024
  0%|          | 0/150 [00:00<?, ?it/s]/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
  1%|          | 1/150 [00:06<16:41,  6.72s/it]  1%|▏         | 2/150 [00:13<15:56,  6.46s/it]  2%|▏         | 3/150 [00:19<15:34,  6.36s/it]  3%|▎         | 4/150 [00:25<15:16,  6.28s/it]  3%|▎         | 5/150 [00:31<15:06,  6.25s/it]  4%|▍         | 6/150 [00:37<15:01,  6.26s/it]  5%|▍         | 7/150 [00:44<14:54,  6.25s/it]  5%|▌         | 8/150 [00:50<14:44,  6.23s/it]  6%|▌         | 9/150 [00:56<14:37,  6.22s/it]  7%|▋         | 10/150 [01:02<14:27,  6.20s/it]                                                {'loss': 1.0359, 'grad_norm': 1.3729771375656128, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [01:02<14:27,  6.20s/it]  7%|▋         | 11/150 [01:08<14:20,  6.19s/it]  8%|▊         | 12/150 [01:14<14:14,  6.19s/it]  9%|▊         | 13/150 [01:21<14:07,  6.19s/it]  9%|▉         | 14/150 [01:27<14:02,  6.19s/it] 10%|█         | 15/150 [01:33<13:53,  6.17s/it] 11%|█         | 16/150 [01:39<13:45,  6.16s/it] 11%|█▏        | 17/150 [01:45<13:37,  6.14s/it] 12%|█▏        | 18/150 [01:51<13:30,  6.14s/it] 13%|█▎        | 19/150 [01:58<13:24,  6.14s/it] 13%|█▎        | 20/150 [02:04<13:19,  6.15s/it]                                                {'loss': 0.1717, 'grad_norm': 0.9416792988777161, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [02:04<13:19,  6.15s/it] 14%|█▍        | 21/150 [02:10<13:13,  6.15s/it] 15%|█▍        | 22/150 [02:16<13:11,  6.18s/it] 15%|█▌        | 23/150 [02:22<13:08,  6.21s/it] 16%|█▌        | 24/150 [02:29<13:03,  6.22s/it] 17%|█▋        | 25/150 [02:35<12:56,  6.21s/it] 17%|█▋        | 26/150 [02:41<12:57,  6.27s/it] 18%|█▊        | 27/150 [02:47<12:46,  6.23s/it] 19%|█▊        | 28/150 [02:54<12:40,  6.24s/it] 19%|█▉        | 29/150 [03:00<12:32,  6.22s/it] 20%|██        | 30/150 [03:06<12:25,  6.21s/it]                                                {'loss': 0.0874, 'grad_norm': 0.5237970948219299, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [03:06<12:25,  6.21s/it] 21%|██        | 31/150 [03:12<12:19,  6.22s/it] 21%|██▏       | 32/150 [03:18<12:11,  6.20s/it] 22%|██▏       | 33/150 [03:25<12:05,  6.20s/it] 23%|██▎       | 34/150 [03:31<11:58,  6.20s/it] 23%|██▎       | 35/150 [03:37<11:53,  6.20s/it] 24%|██▍       | 36/150 [03:43<11:47,  6.21s/it] 25%|██▍       | 37/150 [03:49<11:40,  6.20s/it] 25%|██▌       | 38/150 [03:56<11:33,  6.19s/it] 26%|██▌       | 39/150 [04:02<11:28,  6.21s/it] 27%|██▋       | 40/150 [04:08<11:22,  6.21s/it]                                                {'loss': 0.0786, 'grad_norm': 0.926433265209198, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [04:08<11:22,  6.21s/it] 27%|██▋       | 41/150 [04:14<11:18,  6.22s/it] 28%|██▊       | 42/150 [04:20<11:10,  6.21s/it] 29%|██▊       | 43/150 [04:27<11:04,  6.21s/it] 29%|██▉       | 44/150 [04:33<10:56,  6.20s/it] 30%|███       | 45/150 [04:39<10:53,  6.22s/it] 31%|███       | 46/150 [04:45<10:48,  6.23s/it] 31%|███▏      | 47/150 [04:52<10:41,  6.23s/it] 32%|███▏      | 48/150 [04:58<10:34,  6.22s/it] 33%|███▎      | 49/150 [05:04<10:29,  6.24s/it] 33%|███▎      | 50/150 [05:10<10:24,  6.24s/it]                                                {'loss': 0.0658, 'grad_norm': 0.5618735551834106, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [05:10<10:24,  6.24s/it] 34%|███▍      | 51/150 [05:17<10:16,  6.23s/it] 35%|███▍      | 52/150 [05:23<10:11,  6.24s/it] 35%|███▌      | 53/150 [05:29<10:06,  6.26s/it] 36%|███▌      | 54/150 [05:35<09:59,  6.24s/it] 37%|███▋      | 55/150 [05:41<09:51,  6.23s/it] 37%|███▋      | 56/150 [05:48<09:44,  6.22s/it] 38%|███▊      | 57/150 [05:54<09:38,  6.22s/it] 39%|███▊      | 58/150 [06:00<09:31,  6.22s/it] 39%|███▉      | 59/150 [06:06<09:27,  6.23s/it] 40%|████      | 60/150 [06:13<09:19,  6.22s/it]                                                {'loss': 0.0502, 'grad_norm': 0.17505665123462677, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [06:13<09:19,  6.22s/it] 41%|████      | 61/150 [06:19<09:14,  6.23s/it] 41%|████▏     | 62/150 [06:25<09:09,  6.25s/it] 42%|████▏     | 63/150 [06:31<09:05,  6.27s/it] 43%|████▎     | 64/150 [06:38<08:59,  6.27s/it] 43%|████▎     | 65/150 [06:44<08:52,  6.26s/it] 44%|████▍     | 66/150 [06:50<08:43,  6.23s/it] 45%|████▍     | 67/150 [06:56<08:35,  6.21s/it] 45%|████▌     | 68/150 [07:02<08:27,  6.19s/it] 46%|████▌     | 69/150 [07:09<08:22,  6.20s/it] 47%|████▋     | 70/150 [07:15<08:14,  6.18s/it]                                                {'loss': 0.0431, 'grad_norm': 0.18645936250686646, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [07:15<08:14,  6.18s/it] 47%|████▋     | 71/150 [07:21<08:08,  6.19s/it] 48%|████▊     | 72/150 [07:27<08:04,  6.21s/it] 49%|████▊     | 73/150 [07:33<07:57,  6.21s/it] 49%|████▉     | 74/150 [07:40<07:52,  6.21s/it] 50%|█████     | 75/150 [07:46<07:44,  6.20s/it] 51%|█████     | 76/150 [07:52<07:38,  6.19s/it] 51%|█████▏    | 77/150 [07:58<07:32,  6.20s/it] 52%|█████▏    | 78/150 [08:05<07:28,  6.24s/it] 53%|█████▎    | 79/150 [08:11<07:24,  6.26s/it] 53%|█████▎    | 80/150 [08:17<07:17,  6.25s/it]                                                {'loss': 0.0425, 'grad_norm': 0.2904014587402344, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [08:17<07:17,  6.25s/it] 54%|█████▍    | 81/150 [08:23<07:11,  6.25s/it] 55%|█████▍    | 82/150 [08:30<07:04,  6.24s/it] 55%|█████▌    | 83/150 [08:36<06:57,  6.22s/it] 56%|█████▌    | 84/150 [08:42<06:49,  6.21s/it] 57%|█████▋    | 85/150 [08:48<06:43,  6.20s/it] 57%|█████▋    | 86/150 [08:54<06:36,  6.20s/it] 58%|█████▊    | 87/150 [09:00<06:30,  6.19s/it] 59%|█████▊    | 88/150 [09:07<06:24,  6.21s/it] 59%|█████▉    | 89/150 [09:13<06:16,  6.18s/it] 60%|██████    | 90/150 [09:19<06:11,  6.20s/it]                                                {'loss': 0.0504, 'grad_norm': 0.32006514072418213, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [09:19<06:11,  6.20s/it] 61%|██████    | 91/150 [09:25<06:05,  6.19s/it] 61%|██████▏   | 92/150 [09:31<05:59,  6.20s/it] 62%|██████▏   | 93/150 [09:38<05:54,  6.22s/it] 63%|██████▎   | 94/150 [09:44<05:49,  6.23s/it] 63%|██████▎   | 95/150 [09:50<05:42,  6.22s/it] 64%|██████▍   | 96/150 [09:56<05:35,  6.22s/it] 65%|██████▍   | 97/150 [10:03<05:30,  6.23s/it] 65%|██████▌   | 98/150 [10:09<05:23,  6.22s/it] 66%|██████▌   | 99/150 [10:15<05:16,  6.21s/it] 67%|██████▋   | 100/150 [10:21<05:11,  6.22s/it]                                                 {'loss': 0.0476, 'grad_norm': 0.2247849851846695, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [10:21<05:11,  6.22s/it] 67%|██████▋   | 101/150 [10:27<05:04,  6.22s/it] 68%|██████▊   | 102/150 [10:34<04:57,  6.19s/it] 69%|██████▊   | 103/150 [10:40<04:50,  6.19s/it] 69%|██████▉   | 104/150 [10:46<04:44,  6.18s/it] 70%|███████   | 105/150 [10:52<04:37,  6.18s/it] 71%|███████   | 106/150 [10:58<04:32,  6.19s/it] 71%|███████▏  | 107/150 [11:05<04:28,  6.24s/it] 72%|███████▏  | 108/150 [11:11<04:22,  6.25s/it] 73%|███████▎  | 109/150 [11:17<04:15,  6.24s/it] 73%|███████▎  | 110/150 [11:23<04:10,  6.25s/it]                                                 {'loss': 0.0397, 'grad_norm': 0.21871215105056763, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [11:23<04:10,  6.25s/it] 74%|███████▍  | 111/150 [11:30<04:03,  6.25s/it] 75%|███████▍  | 112/150 [11:36<03:57,  6.26s/it] 75%|███████▌  | 113/150 [11:42<03:51,  6.25s/it] 76%|███████▌  | 114/150 [11:48<03:45,  6.25s/it] 77%|███████▋  | 115/150 [11:55<03:38,  6.25s/it] 77%|███████▋  | 116/150 [12:01<03:32,  6.24s/it] 78%|███████▊  | 117/150 [12:07<03:25,  6.23s/it] 79%|███████▊  | 118/150 [12:13<03:20,  6.25s/it] 79%|███████▉  | 119/150 [12:20<03:13,  6.25s/it] 80%|████████  | 120/150 [12:26<03:07,  6.25s/it]                                                 {'loss': 0.044, 'grad_norm': 0.2562936544418335, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [12:26<03:07,  6.25s/it] 81%|████████  | 121/150 [12:32<03:01,  6.26s/it] 81%|████████▏ | 122/150 [12:39<02:55,  6.26s/it] 82%|████████▏ | 123/150 [12:45<02:48,  6.25s/it] 83%|████████▎ | 124/150 [12:51<02:42,  6.24s/it] 83%|████████▎ | 125/150 [12:57<02:34,  6.20s/it] 84%|████████▍ | 126/150 [13:03<02:28,  6.21s/it] 85%|████████▍ | 127/150 [13:09<02:22,  6.20s/it] 85%|████████▌ | 128/150 [13:16<02:16,  6.22s/it] 86%|████████▌ | 129/150 [13:22<02:10,  6.21s/it] 87%|████████▋ | 130/150 [13:28<02:03,  6.20s/it]                                                 {'loss': 0.0293, 'grad_norm': 0.11811480671167374, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [13:28<02:03,  6.20s/it] 87%|████████▋ | 131/150 [13:34<01:58,  6.23s/it] 88%|████████▊ | 132/150 [13:41<01:52,  6.24s/it] 89%|████████▊ | 133/150 [13:47<01:45,  6.22s/it] 89%|████████▉ | 134/150 [13:53<01:39,  6.22s/it] 90%|█████████ | 135/150 [13:59<01:33,  6.21s/it] 91%|█████████ | 136/150 [14:05<01:26,  6.19s/it] 91%|█████████▏| 137/150 [14:12<01:20,  6.19s/it] 92%|█████████▏| 138/150 [14:18<01:14,  6.23s/it] 93%|█████████▎| 139/150 [14:24<01:08,  6.22s/it] 93%|█████████▎| 140/150 [14:30<01:02,  6.20s/it]                                                 {'loss': 0.0304, 'grad_norm': 0.2758987247943878, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [14:30<01:02,  6.20s/it] 94%|█████████▍| 141/150 [14:36<00:55,  6.17s/it] 95%|█████████▍| 142/150 [14:42<00:49,  6.17s/it] 95%|█████████▌| 143/150 [14:49<00:43,  6.17s/it] 96%|█████████▌| 144/150 [14:55<00:36,  6.15s/it] 97%|█████████▋| 145/150 [15:01<00:30,  6.15s/it] 97%|█████████▋| 146/150 [15:07<00:24,  6.17s/it] 98%|█████████▊| 147/150 [15:13<00:18,  6.17s/it] 99%|█████████▊| 148/150 [15:19<00:12,  6.18s/it] 99%|█████████▉| 149/150 [15:26<00:06,  6.21s/it]100%|██████████| 150/150 [15:32<00:00,  6.21s/it]                                                 {'loss': 0.0427, 'grad_norm': 0.1407431811094284, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [15:32<00:00,  6.21s/it][INFO|trainer.py:3705] 2024-11-18 22:22:18,471 >> Saving model checkpoint to saves/gemma-2-9b-it/schema_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:22:18,517 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:22:18,519 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:22:18,734 >> tokenizer config file saved in saves/gemma-2-9b-it/schema_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:22:18,735 >> Special tokens file saved in saves/gemma-2-9b-it/schema_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:22:19,983 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 934.8985, 'train_samples_per_second': 2.586, 'train_steps_per_second': 0.16, 'train_loss': 0.12395812451839447, 'epoch': 2.98}
100%|██████████| 150/150 [15:34<00:00,  6.21s/it]100%|██████████| 150/150 [15:34<00:00,  6.23s/it]
[INFO|trainer.py:3705] 2024-11-18 22:22:19,986 >> Saving model checkpoint to saves/gemma-2-9b-it/schema_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:22:20,026 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:22:20,027 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:22:20,233 >> tokenizer config file saved in saves/gemma-2-9b-it/schema_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:22:20,234 >> Special tokens file saved in saves/gemma-2-9b-it/schema_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 97224002GF
  train_loss               =      0.124
  train_runtime            = 0:15:34.89
  train_samples_per_second =      2.586
  train_steps_per_second   =       0.16
Figure saved at: saves/gemma-2-9b-it/schema_prompt/lora/sft/training_loss.png
11/18/2024 22:22:21 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:22:21 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:22:21,050 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:22:21,050 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:22:21,050 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:04,  9.71it/s]  7%|▋         | 3/45 [00:00<00:05,  7.10it/s]  9%|▉         | 4/45 [00:00<00:06,  6.20it/s] 11%|█         | 5/45 [00:00<00:06,  5.76it/s] 13%|█▎        | 6/45 [00:00<00:07,  5.49it/s] 16%|█▌        | 7/45 [00:01<00:07,  5.36it/s] 18%|█▊        | 8/45 [00:01<00:06,  5.29it/s] 20%|██        | 9/45 [00:01<00:06,  5.26it/s] 22%|██▏       | 10/45 [00:01<00:06,  5.10it/s] 24%|██▍       | 11/45 [00:01<00:06,  5.10it/s] 27%|██▋       | 12/45 [00:02<00:06,  5.07it/s] 29%|██▉       | 13/45 [00:02<00:06,  5.08it/s] 31%|███       | 14/45 [00:02<00:06,  5.09it/s] 33%|███▎      | 15/45 [00:02<00:05,  5.13it/s] 36%|███▌      | 16/45 [00:02<00:05,  5.08it/s] 38%|███▊      | 17/45 [00:03<00:05,  5.11it/s] 40%|████      | 18/45 [00:03<00:05,  5.05it/s] 42%|████▏     | 19/45 [00:03<00:05,  5.11it/s] 44%|████▍     | 20/45 [00:03<00:04,  5.10it/s] 47%|████▋     | 21/45 [00:03<00:04,  5.04it/s] 49%|████▉     | 22/45 [00:04<00:04,  5.00it/s] 51%|█████     | 23/45 [00:04<00:04,  5.07it/s] 53%|█████▎    | 24/45 [00:04<00:04,  5.06it/s] 56%|█████▌    | 25/45 [00:04<00:03,  5.02it/s] 58%|█████▊    | 26/45 [00:04<00:03,  5.06it/s] 60%|██████    | 27/45 [00:05<00:03,  5.11it/s] 62%|██████▏   | 28/45 [00:05<00:03,  5.08it/s] 64%|██████▍   | 29/45 [00:05<00:03,  5.11it/s] 67%|██████▋   | 30/45 [00:05<00:02,  5.12it/s] 69%|██████▉   | 31/45 [00:05<00:02,  5.16it/s] 71%|███████   | 32/45 [00:06<00:02,  5.16it/s] 73%|███████▎  | 33/45 [00:06<00:02,  5.16it/s] 76%|███████▌  | 34/45 [00:06<00:02,  5.20it/s] 78%|███████▊  | 35/45 [00:06<00:01,  5.12it/s] 80%|████████  | 36/45 [00:06<00:01,  5.12it/s] 82%|████████▏ | 37/45 [00:07<00:01,  5.08it/s] 84%|████████▍ | 38/45 [00:07<00:01,  5.04it/s] 87%|████████▋ | 39/45 [00:07<00:01,  5.07it/s] 89%|████████▉ | 40/45 [00:07<00:00,  5.09it/s] 91%|█████████ | 41/45 [00:07<00:00,  5.14it/s] 93%|█████████▎| 42/45 [00:08<00:00,  5.15it/s] 96%|█████████▌| 43/45 [00:08<00:00,  5.11it/s] 98%|█████████▊| 44/45 [00:08<00:00,  5.12it/s]100%|██████████| 45/45 [00:08<00:00,  5.12it/s]100%|██████████| 45/45 [00:08<00:00,  5.20it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0597
  eval_runtime            = 0:00:08.85
  eval_samples_per_second =     10.158
  eval_steps_per_second   =      5.079
[INFO|modelcard.py:449] 2024-11-18 22:22:29,910 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:22:55,246] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:22:59 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29921
[2024-11-18 22:23:01,308] torch.distributed.run: [WARNING] 
[2024-11-18 22:23:01,308] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:23:01,308] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:23:01,308] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:23:08,742] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:23:09,035] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:23:09 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:23:09 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:23:09,998 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:23:10,000 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:10,004 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:10,004 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:10,004 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:10,004 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:10,004 >> loading file tokenizer_config.json
11/18/2024 22:23:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:23:10 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:23:11,475 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:23:11,476 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:11,478 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:11,478 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:11,478 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:11,478 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:23:11,478 >> loading file tokenizer_config.json
11/18/2024 22:23:12 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
11/18/2024 22:23:13 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/18/2024 22:23:13 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 414.11 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3126.05 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:23:17 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:04<01:02, 13.47 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:05<00:18, 38.59 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:06<00:18, 36.05 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:07<00:14, 42.15 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:08<00:08, 62.43 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:09<00:07, 62.98 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:10<00:06, 61.44 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:11<00:05, 59.69 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:12<00:04, 58.35 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:13<00:03, 58.99 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:14<00:02, 57.90 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:15<00:01, 63.60 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:16<00:00, 62.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:17<00:00, 61.56 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:17<00:00, 52.30 examples/s]
training example:
input_ids:
[2, 106, 1645, 108, 236023, 235812, 46555, 235370, 17810, 12620, 37492, 235465, 108, 9766, 26472, 1192, 10890, 70617, 23055, 235640, 235465, 7139, 3179, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 16861, 559, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 33683, 2729, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 2729, 824, 664, 70617, 23055, 235640, 235465, 16861, 559, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 10004, 824, 664, 70617, 23055, 235640, 235465, 33683, 10004, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 10004, 824, 664, 70617, 23055, 235640, 235465, 9377, 26558, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 44040, 824, 664, 70617, 23055, 235640, 235465, 1815, 482, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 44040, 824, 664, 70617, 23055, 235640, 235465, 3212, 3688, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 189295, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 46852, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 3212, 46852, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 147022, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 6717, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 7879, 235269, 182624, 23055, 235640, 235292, 44040, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 49803, 473, 6717, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 502, 964, 32771, 235269, 182624, 23055, 235640, 235292, 3531, 235269, 236932, 235733, 23055, 235640, 235292, 3531, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 25364, 235269, 182624, 23055, 235640, 235292, 25364, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 2729, 235269, 182624, 23055, 235640, 235292, 2729, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 527, 18191, 10004, 235269, 182624, 23055, 235640, 235292, 10004, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 3212, 18843, 235269, 182624, 23055, 235640, 235292, 6717, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 824, 664, 70617, 23055, 235640, 235465, 73132, 1109, 235269, 182624, 23055, 235640, 235292, 3123, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 1638, 824, 664, 70617, 23055, 235640, 235465, 1433, 657, 1638, 559, 235269, 182624, 23055, 235640, 235292, 3123, 1638, 235269, 236932, 235733, 23055, 235640, 235292, 3123, 1638, 12229, 664, 16983, 1192, 10890, 46788, 23055, 235292, 10004, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2737, 920, 777, 3312, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3531, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 920, 777, 1425, 7525, 21569, 664, 46788, 23055, 235292, 6717, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 4614, 920, 777, 18960, 920, 777, 52995, 920, 777, 14815, 920, 777, 57311, 920, 777, 57721, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 44040, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 920, 777, 1425, 7525, 21569, 664, 46788, 23055, 235292, 25364, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2563, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3123, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 7525, 21569, 664, 46788, 23055, 235292, 2729, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2737, 920, 777, 3312, 920, 777, 14815, 920, 777, 2502, 1716, 920, 777, 4228, 1314, 920, 777, 22262, 3909, 920, 777, 47037, 1545, 7525, 21569, 664, 46788, 23055, 235292, 3123, 1638, 235269, 53166, 33095, 12620, 11647, 1829, 539, 920, 777, 2457, 920, 777, 1067, 7525, 235275, 193101, 108, 236023, 235812, 46555, 235493, 17830, 18390, 9785, 12620, 235292, 108, 2097, 235640, 25364, 235269, 33095, 2563, 2875, 3613, 604, 12906, 235298, 136987, 235298, 118638, 235298, 1949, 7338, 575, 14225, 5535, 235303, 235370, 46788, 108, 2097, 235640, 25364, 235269, 33095, 2563, 2875, 3613, 604, 21752, 235298, 142680, 575, 16306, 236143, 235306, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 235319, 480, 198532, 235298, 235382, 1972, 235298, 15957, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 60966, 477, 235298, 185063, 235298, 15957, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 67900, 235298, 235299, 62418, 235298, 64005, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 38956, 235298, 136987, 235298, 118638, 235298, 1949, 7338, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 3154, 235298, 15390, 235298, 178780, 38108, 235303, 235370, 46788, 109, 236343, 236007, 18390, 17689, 48982, 73583, 235640, 235735, 236440, 46555, 235370, 10687, 9263, 23258, 235292, 108, 81896, 235735, 21175, 235298, 9406, 866, 520, 235298, 195275, 131359, 11043, 236382, 235370, 99721, 235394, 153697, 235394, 48982, 235581, 238126, 237005, 235365, 236203, 24484, 44697, 235370, 6217, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
inputs:
<bos><start_of_turn>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URLmatch (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
label_ids:
[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
labels:
<eos>match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
[INFO|configuration_utils.py:673] 2024-11-18 22:23:36,022 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:23:36,023 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

11/18/2024 22:23:36 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
[INFO|modeling_utils.py:3729] 2024-11-18 22:23:36,076 >> loading weights file /home/work/liuytest/demo/gemma-2-9b-it/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:23:36,076 >> Instantiating Gemma2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:23:36,079 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]11/18/2024 22:23:36 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:23:41,926 >> All model checkpoint weights were used when initializing Gemma2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:23:41,926 >> All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/gemma-2-9b-it.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:23:41,933 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:23:41,933 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "max_length": 8192,
  "pad_token_id": 0
}

11/18/2024 22:23:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:23:41 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:23:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:23:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:23:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,gate_proj,o_proj,up_proj,down_proj,v_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
11/18/2024 22:23:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:23:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:23:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:23:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:23:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,v_proj,o_proj,up_proj,gate_proj,down_proj
11/18/2024 22:23:42 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:23:42,941 >> Using auto half precision backend
11/18/2024 22:23:43 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
[INFO|trainer.py:2243] 2024-11-18 22:23:43,819 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:23:43,819 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:23:43,819 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:23:43,819 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:23:43,819 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:23:43,819 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:23:43,819 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:23:43,829 >>   Number of trainable parameters = 27,009,024
  0%|          | 0/150 [00:00<?, ?it/s]/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
  1%|          | 1/150 [00:06<17:16,  6.95s/it]  1%|▏         | 2/150 [00:13<16:21,  6.63s/it]  2%|▏         | 3/150 [00:19<16:01,  6.54s/it]  3%|▎         | 4/150 [00:26<15:47,  6.49s/it]  3%|▎         | 5/150 [00:32<15:37,  6.46s/it]  4%|▍         | 6/150 [00:39<15:26,  6.43s/it]  5%|▍         | 7/150 [00:45<15:19,  6.43s/it]  5%|▌         | 8/150 [00:51<15:11,  6.42s/it]  6%|▌         | 9/150 [00:58<15:04,  6.42s/it]  7%|▋         | 10/150 [01:04<14:56,  6.41s/it]                                                {'loss': 1.3698, 'grad_norm': 1.7379564046859741, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [01:04<14:56,  6.41s/it]  7%|▋         | 11/150 [01:11<14:50,  6.41s/it]  8%|▊         | 12/150 [01:17<14:45,  6.41s/it]  9%|▊         | 13/150 [01:23<14:38,  6.41s/it]  9%|▉         | 14/150 [01:30<14:31,  6.41s/it] 10%|█         | 15/150 [01:36<14:25,  6.41s/it] 11%|█         | 16/150 [01:43<14:19,  6.41s/it] 11%|█▏        | 17/150 [01:49<14:14,  6.42s/it] 12%|█▏        | 18/150 [01:55<14:04,  6.40s/it] 13%|█▎        | 19/150 [02:02<13:59,  6.41s/it] 13%|█▎        | 20/150 [02:08<13:53,  6.41s/it]                                                {'loss': 0.4543, 'grad_norm': 1.068831443786621, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [02:08<13:53,  6.41s/it] 14%|█▍        | 21/150 [02:15<13:46,  6.40s/it] 15%|█▍        | 22/150 [02:21<13:38,  6.40s/it] 15%|█▌        | 23/150 [02:27<13:32,  6.40s/it] 16%|█▌        | 24/150 [02:34<13:25,  6.39s/it] 17%|█▋        | 25/150 [02:40<13:17,  6.38s/it] 17%|█▋        | 26/150 [02:47<13:12,  6.39s/it] 18%|█▊        | 27/150 [02:53<13:05,  6.39s/it] 19%|█▊        | 28/150 [02:59<12:59,  6.39s/it] 19%|█▉        | 29/150 [03:06<12:55,  6.41s/it] 20%|██        | 30/150 [03:12<12:49,  6.41s/it]                                                {'loss': 0.2479, 'grad_norm': 0.697287917137146, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [03:12<12:49,  6.41s/it] 21%|██        | 31/150 [03:19<12:41,  6.40s/it] 21%|██▏       | 32/150 [03:25<12:35,  6.41s/it] 22%|██▏       | 33/150 [03:31<12:27,  6.39s/it] 23%|██▎       | 34/150 [03:38<12:21,  6.39s/it] 23%|██▎       | 35/150 [03:44<12:15,  6.39s/it] 24%|██▍       | 36/150 [03:51<12:08,  6.39s/it] 25%|██▍       | 37/150 [03:57<12:02,  6.39s/it] 25%|██▌       | 38/150 [04:03<11:55,  6.39s/it] 26%|██▌       | 39/150 [04:10<11:50,  6.40s/it] 27%|██▋       | 40/150 [04:16<11:44,  6.41s/it]                                                {'loss': 0.2255, 'grad_norm': 0.5162644982337952, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [04:16<11:44,  6.41s/it] 27%|██▋       | 41/150 [04:23<11:37,  6.40s/it] 28%|██▊       | 42/150 [04:29<11:30,  6.40s/it] 29%|██▊       | 43/150 [04:35<11:25,  6.40s/it] 29%|██▉       | 44/150 [04:42<11:16,  6.38s/it] 30%|███       | 45/150 [04:48<11:11,  6.40s/it] 31%|███       | 46/150 [04:54<11:05,  6.40s/it] 31%|███▏      | 47/150 [05:01<11:00,  6.41s/it] 32%|███▏      | 48/150 [05:07<10:53,  6.41s/it] 33%|███▎      | 49/150 [05:14<10:47,  6.41s/it] 33%|███▎      | 50/150 [05:20<10:41,  6.41s/it]                                                {'loss': 0.1984, 'grad_norm': 0.48762503266334534, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [05:20<10:41,  6.41s/it] 34%|███▍      | 51/150 [05:27<10:35,  6.42s/it] 35%|███▍      | 52/150 [05:33<10:28,  6.41s/it] 35%|███▌      | 53/150 [05:39<10:21,  6.41s/it] 36%|███▌      | 54/150 [05:46<10:15,  6.41s/it] 37%|███▋      | 55/150 [05:52<10:08,  6.41s/it] 37%|███▋      | 56/150 [05:59<10:01,  6.40s/it] 38%|███▊      | 57/150 [06:05<09:54,  6.39s/it] 39%|███▊      | 58/150 [06:11<09:47,  6.39s/it] 39%|███▉      | 59/150 [06:18<09:42,  6.40s/it] 40%|████      | 60/150 [06:24<09:35,  6.40s/it]                                                {'loss': 0.1759, 'grad_norm': 0.40817952156066895, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [06:24<09:35,  6.40s/it] 41%|████      | 61/150 [06:31<09:29,  6.40s/it] 41%|████▏     | 62/150 [06:37<09:22,  6.40s/it] 42%|████▏     | 63/150 [06:43<09:16,  6.39s/it] 43%|████▎     | 64/150 [06:50<09:09,  6.39s/it] 43%|████▎     | 65/150 [06:56<09:04,  6.41s/it] 44%|████▍     | 66/150 [07:03<08:58,  6.41s/it] 45%|████▍     | 67/150 [07:09<08:51,  6.41s/it] 45%|████▌     | 68/150 [07:15<08:44,  6.40s/it] 46%|████▌     | 69/150 [07:22<08:39,  6.41s/it] 47%|████▋     | 70/150 [07:28<08:34,  6.43s/it]                                                {'loss': 0.1582, 'grad_norm': 0.507072925567627, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [07:28<08:34,  6.43s/it] 47%|████▋     | 71/150 [07:35<08:28,  6.44s/it] 48%|████▊     | 72/150 [07:41<08:22,  6.44s/it] 49%|████▊     | 73/150 [07:48<08:15,  6.43s/it] 49%|████▉     | 74/150 [07:54<08:08,  6.42s/it] 50%|█████     | 75/150 [08:00<08:00,  6.41s/it] 51%|█████     | 76/150 [08:07<07:53,  6.40s/it] 51%|█████▏    | 77/150 [08:13<07:47,  6.40s/it] 52%|█████▏    | 78/150 [08:20<07:40,  6.40s/it] 53%|█████▎    | 79/150 [08:26<07:33,  6.39s/it] 53%|█████▎    | 80/150 [08:32<07:27,  6.39s/it]                                                {'loss': 0.149, 'grad_norm': 0.37747347354888916, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [08:32<07:27,  6.39s/it] 54%|█████▍    | 81/150 [08:39<07:19,  6.38s/it] 55%|█████▍    | 82/150 [08:45<07:13,  6.37s/it] 55%|█████▌    | 83/150 [08:51<07:07,  6.38s/it] 56%|█████▌    | 84/150 [08:58<07:01,  6.39s/it] 57%|█████▋    | 85/150 [09:04<06:54,  6.38s/it] 57%|█████▋    | 86/150 [09:11<06:48,  6.38s/it] 58%|█████▊    | 87/150 [09:17<06:42,  6.39s/it] 59%|█████▊    | 88/150 [09:23<06:35,  6.39s/it] 59%|█████▉    | 89/150 [09:30<06:29,  6.38s/it] 60%|██████    | 90/150 [09:36<06:23,  6.39s/it]                                                {'loss': 0.1576, 'grad_norm': 0.43957072496414185, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [09:36<06:23,  6.39s/it] 61%|██████    | 91/150 [09:42<06:16,  6.38s/it] 61%|██████▏   | 92/150 [09:49<06:09,  6.37s/it] 62%|██████▏   | 93/150 [09:55<06:03,  6.37s/it] 63%|██████▎   | 94/150 [10:02<05:56,  6.36s/it] 63%|██████▎   | 95/150 [10:08<05:50,  6.38s/it] 64%|██████▍   | 96/150 [10:14<05:44,  6.39s/it] 65%|██████▍   | 97/150 [10:21<05:38,  6.39s/it] 65%|██████▌   | 98/150 [10:27<05:32,  6.39s/it] 66%|██████▌   | 99/150 [10:33<05:24,  6.37s/it] 67%|██████▋   | 100/150 [10:40<05:19,  6.38s/it]                                                 {'loss': 0.1649, 'grad_norm': 0.318549245595932, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [10:40<05:19,  6.38s/it] 67%|██████▋   | 101/150 [10:46<05:13,  6.41s/it] 68%|██████▊   | 102/150 [10:53<05:07,  6.41s/it] 69%|██████▊   | 103/150 [10:59<05:00,  6.39s/it] 69%|██████▉   | 104/150 [11:06<04:54,  6.40s/it] 70%|███████   | 105/150 [11:12<04:47,  6.39s/it] 71%|███████   | 106/150 [11:18<04:41,  6.39s/it] 71%|███████▏  | 107/150 [11:25<04:34,  6.39s/it] 72%|███████▏  | 108/150 [11:31<04:27,  6.38s/it] 73%|███████▎  | 109/150 [11:37<04:21,  6.38s/it] 73%|███████▎  | 110/150 [11:44<04:15,  6.39s/it]                                                 {'loss': 0.1364, 'grad_norm': 0.5252922773361206, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [11:44<04:15,  6.39s/it] 74%|███████▍  | 111/150 [11:50<04:09,  6.40s/it] 75%|███████▍  | 112/150 [11:57<04:02,  6.38s/it] 75%|███████▌  | 113/150 [12:03<03:56,  6.38s/it] 76%|███████▌  | 114/150 [12:09<03:50,  6.40s/it] 77%|███████▋  | 115/150 [12:16<03:43,  6.38s/it] 77%|███████▋  | 116/150 [12:22<03:37,  6.39s/it] 78%|███████▊  | 117/150 [12:29<03:31,  6.40s/it] 79%|███████▊  | 118/150 [12:35<03:24,  6.40s/it] 79%|███████▉  | 119/150 [12:41<03:18,  6.40s/it] 80%|████████  | 120/150 [12:48<03:12,  6.40s/it]                                                 {'loss': 0.1618, 'grad_norm': 0.4780241847038269, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [12:48<03:12,  6.40s/it] 81%|████████  | 121/150 [12:54<03:05,  6.41s/it] 81%|████████▏ | 122/150 [13:01<02:59,  6.40s/it] 82%|████████▏ | 123/150 [13:07<02:52,  6.39s/it] 83%|████████▎ | 124/150 [13:13<02:46,  6.40s/it] 83%|████████▎ | 125/150 [13:20<02:39,  6.39s/it] 84%|████████▍ | 126/150 [13:26<02:33,  6.40s/it] 85%|████████▍ | 127/150 [13:33<02:26,  6.39s/it] 85%|████████▌ | 128/150 [13:39<02:20,  6.40s/it] 86%|████████▌ | 129/150 [13:45<02:14,  6.40s/it] 87%|████████▋ | 130/150 [13:52<02:07,  6.38s/it]                                                 {'loss': 0.1215, 'grad_norm': 0.29680192470550537, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [13:52<02:07,  6.38s/it] 87%|████████▋ | 131/150 [13:58<02:01,  6.38s/it] 88%|████████▊ | 132/150 [14:04<01:54,  6.38s/it] 89%|████████▊ | 133/150 [14:11<01:48,  6.38s/it] 89%|████████▉ | 134/150 [14:17<01:42,  6.39s/it] 90%|█████████ | 135/150 [14:24<01:36,  6.41s/it] 91%|█████████ | 136/150 [14:30<01:29,  6.40s/it] 91%|█████████▏| 137/150 [14:37<01:23,  6.41s/it] 92%|█████████▏| 138/150 [14:43<01:16,  6.41s/it] 93%|█████████▎| 139/150 [14:49<01:10,  6.41s/it] 93%|█████████▎| 140/150 [14:56<01:03,  6.39s/it]                                                 {'loss': 0.142, 'grad_norm': 0.5591790676116943, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [14:56<01:03,  6.39s/it] 94%|█████████▍| 141/150 [15:02<00:57,  6.38s/it] 95%|█████████▍| 142/150 [15:08<00:51,  6.38s/it] 95%|█████████▌| 143/150 [15:15<00:44,  6.39s/it] 96%|█████████▌| 144/150 [15:21<00:38,  6.39s/it] 97%|█████████▋| 145/150 [15:28<00:31,  6.39s/it] 97%|█████████▋| 146/150 [15:34<00:25,  6.39s/it] 98%|█████████▊| 147/150 [15:40<00:19,  6.38s/it] 99%|█████████▊| 148/150 [15:47<00:12,  6.39s/it] 99%|█████████▉| 149/150 [15:53<00:06,  6.38s/it]100%|██████████| 150/150 [16:00<00:00,  6.39s/it]                                                 {'loss': 0.1237, 'grad_norm': 0.34563374519348145, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [16:00<00:00,  6.39s/it][INFO|trainer.py:3705] 2024-11-18 22:39:44,757 >> Saving model checkpoint to saves/gemma-2-9b-it/my_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:39:44,802 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:39:44,803 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:39:45,025 >> tokenizer config file saved in saves/gemma-2-9b-it/my_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:39:45,026 >> Special tokens file saved in saves/gemma-2-9b-it/my_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:39:46,241 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 962.4115, 'train_samples_per_second': 2.512, 'train_steps_per_second': 0.156, 'train_loss': 0.26579912900924685, 'epoch': 2.98}
100%|██████████| 150/150 [16:01<00:00,  6.39s/it]100%|██████████| 150/150 [16:01<00:00,  6.41s/it]
[INFO|trainer.py:3705] 2024-11-18 22:39:46,244 >> Saving model checkpoint to saves/gemma-2-9b-it/my_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:39:46,283 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:39:46,284 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:39:46,500 >> tokenizer config file saved in saves/gemma-2-9b-it/my_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:39:46,501 >> Special tokens file saved in saves/gemma-2-9b-it/my_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9777
  total_flos               = 111145412GF
  train_loss               =      0.2658
  train_runtime            =  0:16:02.41
  train_samples_per_second =       2.512
  train_steps_per_second   =       0.156
Figure saved at: saves/gemma-2-9b-it/my_prompt/lora/sft/training_loss.png
11/18/2024 22:39:47 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:39:47 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:39:47,314 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:39:47,314 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:39:47,314 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:04, 10.04it/s]  9%|▉         | 4/45 [00:00<00:06,  6.25it/s] 11%|█         | 5/45 [00:00<00:06,  5.76it/s] 13%|█▎        | 6/45 [00:01<00:07,  5.52it/s] 16%|█▌        | 7/45 [00:01<00:07,  5.36it/s] 18%|█▊        | 8/45 [00:01<00:07,  5.25it/s] 20%|██        | 9/45 [00:01<00:06,  5.16it/s] 22%|██▏       | 10/45 [00:01<00:06,  5.12it/s] 24%|██▍       | 11/45 [00:02<00:06,  5.08it/s] 27%|██▋       | 12/45 [00:02<00:06,  5.06it/s] 29%|██▉       | 13/45 [00:02<00:06,  5.02it/s] 31%|███       | 14/45 [00:02<00:06,  5.01it/s] 33%|███▎      | 15/45 [00:02<00:05,  5.00it/s] 36%|███▌      | 16/45 [00:03<00:05,  5.00it/s] 38%|███▊      | 17/45 [00:03<00:05,  5.00it/s] 40%|████      | 18/45 [00:03<00:05,  5.00it/s] 42%|████▏     | 19/45 [00:03<00:05,  5.00it/s] 44%|████▍     | 20/45 [00:03<00:05,  5.00it/s] 47%|████▋     | 21/45 [00:04<00:04,  4.99it/s] 49%|████▉     | 22/45 [00:04<00:04,  4.98it/s] 51%|█████     | 23/45 [00:04<00:04,  4.97it/s] 53%|█████▎    | 24/45 [00:04<00:04,  4.95it/s] 56%|█████▌    | 25/45 [00:04<00:04,  4.94it/s] 58%|█████▊    | 26/45 [00:05<00:03,  4.93it/s] 60%|██████    | 27/45 [00:05<00:03,  4.93it/s] 62%|██████▏   | 28/45 [00:05<00:03,  4.92it/s] 64%|██████▍   | 29/45 [00:05<00:03,  4.92it/s] 67%|██████▋   | 30/45 [00:05<00:03,  4.92it/s] 69%|██████▉   | 31/45 [00:06<00:02,  4.95it/s] 71%|███████   | 32/45 [00:06<00:02,  4.94it/s] 73%|███████▎  | 33/45 [00:06<00:02,  4.93it/s] 76%|███████▌  | 34/45 [00:06<00:02,  4.93it/s] 78%|███████▊  | 35/45 [00:06<00:02,  4.92it/s] 80%|████████  | 36/45 [00:07<00:01,  4.92it/s] 82%|████████▏ | 37/45 [00:07<00:01,  4.92it/s] 84%|████████▍ | 38/45 [00:07<00:01,  4.90it/s] 87%|████████▋ | 39/45 [00:07<00:01,  4.90it/s] 89%|████████▉ | 40/45 [00:07<00:01,  4.90it/s] 91%|█████████ | 41/45 [00:08<00:00,  4.90it/s] 93%|█████████▎| 42/45 [00:08<00:00,  4.90it/s] 96%|█████████▌| 43/45 [00:08<00:00,  4.89it/s] 98%|█████████▊| 44/45 [00:08<00:00,  4.88it/s]100%|██████████| 45/45 [00:08<00:00,  4.90it/s]100%|██████████| 45/45 [00:08<00:00,  5.06it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1498
  eval_runtime            = 0:00:09.12
  eval_samples_per_second =      9.868
  eval_steps_per_second   =      4.934
[INFO|modelcard.py:449] 2024-11-18 22:39:56,434 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:40:18,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:40:22 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29426
[2024-11-18 22:40:24,622] torch.distributed.run: [WARNING] 
[2024-11-18 22:40:24,622] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:40:24,622] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:40:24,622] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:40:32,105] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:40:32,138] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:40:33 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:40:33 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:40:33,280 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:40:33,282 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:33,286 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:33,286 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:33,286 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:33,286 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:33,286 >> loading file tokenizer_config.json
11/18/2024 22:40:33 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:40:33 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:40:34,755 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:40:34,756 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:34,758 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:34,758 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:34,758 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:34,758 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:40:34,758 >> loading file tokenizer_config.json
11/18/2024 22:40:36 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
11/18/2024 22:40:36 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
11/18/2024 22:40:36 - WARNING - llamafactory.model.loader - Processor was not found: 'Gemma2Config' object has no attribute 'vision_config'.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 896 examples [00:00, 25057.65 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 457.56 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3109.80 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:40:40 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:03<00:49, 17.11 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:03<00:24, 32.45 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:05<00:20, 34.76 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:06<00:09, 63.79 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:06<00:08, 68.84 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:07<00:06, 72.76 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:07<00:05, 75.95 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:08<00:04, 78.69 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:09<00:04, 80.43 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:10<00:04, 62.31 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:11<00:01, 91.25 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:11<00:01, 89.25 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:12<00:00, 89.21 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:13<00:00, 76.82 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:13<00:00, 65.65 examples/s]
training example:
input_ids:
[2, 106, 1645, 108, 236023, 235812, 46555, 235493, 17830, 18390, 9785, 12620, 235292, 108, 2097, 235640, 25364, 235269, 33095, 2563, 2875, 3613, 604, 12906, 235298, 136987, 235298, 118638, 235298, 1949, 7338, 575, 14225, 5535, 235303, 235370, 46788, 108, 2097, 235640, 25364, 235269, 33095, 2563, 2875, 3613, 604, 21752, 235298, 142680, 575, 16306, 236143, 235306, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 235319, 480, 198532, 235298, 235382, 1972, 235298, 15957, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 60966, 477, 235298, 185063, 235298, 15957, 235303, 235370, 46788, 108, 2097, 235640, 44040, 235269, 33095, 1067, 2875, 67900, 235298, 235299, 62418, 235298, 64005, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 38956, 235298, 136987, 235298, 118638, 235298, 1949, 7338, 235303, 235370, 46788, 108, 2097, 235640, 3123, 235269, 33095, 1067, 2875, 3154, 235298, 15390, 235298, 178780, 38108, 235303, 235370, 46788, 109, 236343, 236007, 18390, 17689, 48982, 73583, 235640, 235735, 236440, 46555, 235370, 10687, 9263, 23258, 235292, 108, 81896, 235735, 21175, 235298, 9406, 866, 520, 235298, 195275, 131359, 11043, 236382, 235370, 99721, 235394, 153697, 235394, 48982, 235581, 238126, 237005, 235365, 236203, 24484, 44697, 235370, 6217, 235581, 29173, 235365, 22234, 236784, 44697, 29173, 76074, 107, 108, 106, 2516, 108, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
inputs:
<bos><start_of_turn>user
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<end_of_turn>
<start_of_turn>model
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
label_ids:
[1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 7644, 591, 235254, 235274, 235292, 6717, 7817, 235309, 235255, 235274, 235292, 3212, 18843, 24751, 235278, 235254, 235284, 235292, 3123, 235275, 139, 2635, 552, 235284, 235265, 1067, 2875, 21175, 235298, 9406, 866, 520, 235298, 195275, 235303, 2203, 552, 235284, 235265, 2457, 235269, 552, 235274, 235265, 4614, 235269, 552, 235284, 235265, 1067, 235269, 552, 235274, 235265, 57311, 235269, 552, 235274, 235265, 18960, 235269, 552, 235274, 235265, 14815, 2184, 731, 552, 235284, 235265, 1067, 235289, 1]
labels:
<eos>match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<eos>
[INFO|configuration_utils.py:673] 2024-11-18 22:40:55,737 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:40:55,738 >> Model config Gemma2Config {
  "_name_or_path": "/home/work/liuytest/demo/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

11/18/2024 22:40:55 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
[INFO|modeling_utils.py:3729] 2024-11-18 22:40:55,786 >> loading weights file /home/work/liuytest/demo/gemma-2-9b-it/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:40:55,787 >> Instantiating Gemma2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:40:55,788 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]11/18/2024 22:40:56 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed, use eager attention.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:41:01,162 >> All model checkpoint weights were used when initializing Gemma2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:41:01,163 >> All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/gemma-2-9b-it.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:41:01,169 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:41:01,170 >> Generate config GenerationConfig {
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "max_length": 8192,
  "pad_token_id": 0
}

11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:41:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:41:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,down_proj,gate_proj,k_proj,v_proj,o_proj,q_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:41:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:41:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:41:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,up_proj,o_proj,q_proj,v_proj,down_proj
11/18/2024 22:41:02 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:41:02,214 >> Using auto half precision backend
11/18/2024 22:41:02 - INFO - llamafactory.model.loader - trainable params: 27,009,024 || all params: 9,268,715,008 || trainable%: 0.2914
[INFO|trainer.py:2243] 2024-11-18 22:41:03,074 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:41:03,074 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:41:03,074 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:41:03,074 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:41:03,074 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:41:03,074 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:41:03,074 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:41:03,084 >>   Number of trainable parameters = 27,009,024
  0%|          | 0/150 [00:00<?, ?it/s]/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/transformers/models/gemma2/modeling_gemma2.py:595: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at build/CMakeFiles/torch_npu.dir/compiler_depend.ts:74.)
  attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)
  1%|          | 1/150 [00:06<15:25,  6.21s/it]  1%|▏         | 2/150 [00:12<14:43,  5.97s/it]  2%|▏         | 3/150 [00:17<14:23,  5.88s/it]  3%|▎         | 4/150 [00:23<14:12,  5.84s/it]  3%|▎         | 5/150 [00:29<14:04,  5.82s/it]  4%|▍         | 6/150 [00:35<13:55,  5.80s/it]  5%|▍         | 7/150 [00:40<13:47,  5.78s/it]  5%|▌         | 8/150 [00:46<13:41,  5.78s/it]  6%|▌         | 9/150 [00:52<13:35,  5.79s/it]  7%|▋         | 10/150 [00:58<13:29,  5.78s/it]                                                {'loss': 1.5321, 'grad_norm': 2.5167150497436523, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:58<13:29,  5.78s/it]  7%|▋         | 11/150 [01:04<13:25,  5.80s/it]  8%|▊         | 12/150 [01:09<13:18,  5.79s/it]  9%|▊         | 13/150 [01:15<13:13,  5.79s/it]  9%|▉         | 14/150 [01:21<13:09,  5.81s/it] 10%|█         | 15/150 [01:27<13:03,  5.81s/it] 11%|█         | 16/150 [01:33<12:58,  5.81s/it] 11%|█▏        | 17/150 [01:38<12:52,  5.80s/it] 12%|█▏        | 18/150 [01:44<12:46,  5.80s/it] 13%|█▎        | 19/150 [01:50<12:41,  5.81s/it] 13%|█▎        | 20/150 [01:56<12:36,  5.82s/it]                                                {'loss': 0.3338, 'grad_norm': 0.9224963188171387, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:56<12:36,  5.82s/it] 14%|█▍        | 21/150 [02:02<12:25,  5.78s/it] 15%|█▍        | 22/150 [02:07<12:19,  5.77s/it] 15%|█▌        | 23/150 [02:13<12:10,  5.75s/it] 16%|█▌        | 24/150 [02:19<12:03,  5.74s/it] 17%|█▋        | 25/150 [02:24<11:57,  5.74s/it] 17%|█▋        | 26/150 [02:30<11:51,  5.74s/it] 18%|█▊        | 27/150 [02:36<11:46,  5.74s/it] 19%|█▊        | 28/150 [02:42<11:42,  5.76s/it] 19%|█▉        | 29/150 [02:48<11:39,  5.78s/it] 20%|██        | 30/150 [02:53<11:35,  5.80s/it]                                                {'loss': 0.1217, 'grad_norm': 0.7691504955291748, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:53<11:35,  5.80s/it] 21%|██        | 31/150 [02:59<11:31,  5.81s/it] 21%|██▏       | 32/150 [03:05<11:27,  5.82s/it] 22%|██▏       | 33/150 [03:11<11:20,  5.82s/it] 23%|██▎       | 34/150 [03:17<11:12,  5.80s/it] 23%|██▎       | 35/150 [03:22<11:05,  5.79s/it] 24%|██▍       | 36/150 [03:28<10:58,  5.78s/it] 25%|██▍       | 37/150 [03:34<10:52,  5.77s/it] 25%|██▌       | 38/150 [03:40<10:45,  5.77s/it] 26%|██▌       | 39/150 [03:45<10:38,  5.76s/it] 27%|██▋       | 40/150 [03:51<10:30,  5.73s/it]                                                {'loss': 0.0876, 'grad_norm': 0.48209348320961, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [03:51<10:30,  5.73s/it] 27%|██▋       | 41/150 [03:57<10:26,  5.74s/it] 28%|██▊       | 42/150 [04:03<10:20,  5.74s/it] 29%|██▊       | 43/150 [04:08<10:13,  5.74s/it] 29%|██▉       | 44/150 [04:14<10:08,  5.74s/it] 30%|███       | 45/150 [04:20<10:06,  5.77s/it] 31%|███       | 46/150 [04:26<10:02,  5.80s/it] 31%|███▏      | 47/150 [04:32<09:57,  5.80s/it] 32%|███▏      | 48/150 [04:37<09:52,  5.81s/it] 33%|███▎      | 49/150 [04:43<09:46,  5.81s/it] 33%|███▎      | 50/150 [04:49<09:41,  5.81s/it]                                                {'loss': 0.0671, 'grad_norm': 0.3608033061027527, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [04:49<09:41,  5.81s/it] 34%|███▍      | 51/150 [04:55<09:35,  5.81s/it] 35%|███▍      | 52/150 [05:01<09:28,  5.81s/it] 35%|███▌      | 53/150 [05:06<09:22,  5.79s/it] 36%|███▌      | 54/150 [05:12<09:16,  5.80s/it] 37%|███▋      | 55/150 [05:18<09:10,  5.79s/it] 37%|███▋      | 56/150 [05:24<09:05,  5.80s/it] 38%|███▊      | 57/150 [05:30<08:58,  5.80s/it] 39%|███▊      | 58/150 [05:35<08:50,  5.77s/it] 39%|███▉      | 59/150 [05:41<08:45,  5.77s/it] 40%|████      | 60/150 [05:47<08:38,  5.77s/it]                                                {'loss': 0.0504, 'grad_norm': 0.19444437325000763, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [05:47<08:38,  5.77s/it] 41%|████      | 61/150 [05:53<08:34,  5.78s/it] 41%|████▏     | 62/150 [05:58<08:30,  5.80s/it] 42%|████▏     | 63/150 [06:04<08:25,  5.81s/it] 43%|████▎     | 64/150 [06:10<08:20,  5.82s/it] 43%|████▎     | 65/150 [06:16<08:14,  5.82s/it] 44%|████▍     | 66/150 [06:22<08:06,  5.79s/it] 45%|████▍     | 67/150 [06:27<07:58,  5.77s/it] 45%|████▌     | 68/150 [06:33<07:53,  5.78s/it] 46%|████▌     | 69/150 [06:39<07:49,  5.80s/it] 47%|████▋     | 70/150 [06:45<07:45,  5.81s/it]                                                {'loss': 0.0457, 'grad_norm': 0.5545856952667236, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [06:45<07:45,  5.81s/it] 47%|████▋     | 71/150 [06:51<07:39,  5.82s/it] 48%|████▊     | 72/150 [06:57<07:34,  5.82s/it] 49%|████▊     | 73/150 [07:02<07:28,  5.82s/it] 49%|████▉     | 74/150 [07:08<07:23,  5.83s/it] 50%|█████     | 75/150 [07:14<07:17,  5.84s/it] 51%|█████     | 76/150 [07:20<07:12,  5.84s/it] 51%|█████▏    | 77/150 [07:26<07:06,  5.84s/it] 52%|█████▏    | 78/150 [07:32<06:59,  5.83s/it] 53%|█████▎    | 79/150 [07:37<06:53,  5.82s/it] 53%|█████▎    | 80/150 [07:43<06:47,  5.82s/it]                                                {'loss': 0.0453, 'grad_norm': 0.302438884973526, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [07:43<06:47,  5.82s/it] 54%|█████▍    | 81/150 [07:49<06:41,  5.82s/it] 55%|█████▍    | 82/150 [07:55<06:35,  5.81s/it] 55%|█████▌    | 83/150 [08:01<06:29,  5.81s/it] 56%|█████▌    | 84/150 [08:06<06:21,  5.78s/it] 57%|█████▋    | 85/150 [08:12<06:15,  5.77s/it] 57%|█████▋    | 86/150 [08:18<06:10,  5.78s/it] 58%|█████▊    | 87/150 [08:24<06:05,  5.80s/it] 59%|█████▊    | 88/150 [08:30<06:00,  5.81s/it] 59%|█████▉    | 89/150 [08:36<05:56,  5.84s/it] 60%|██████    | 90/150 [08:41<05:52,  5.87s/it]                                                {'loss': 0.0533, 'grad_norm': 0.22442148625850677, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [08:41<05:52,  5.87s/it] 61%|██████    | 91/150 [08:48<05:51,  5.95s/it] 61%|██████▏   | 92/150 [08:54<05:50,  6.04s/it] 62%|██████▏   | 93/150 [09:00<05:48,  6.12s/it] 63%|██████▎   | 94/150 [09:06<05:44,  6.15s/it] 63%|██████▎   | 95/150 [09:13<05:38,  6.15s/it] 64%|██████▍   | 96/150 [09:19<05:33,  6.17s/it] 65%|██████▍   | 97/150 [09:25<05:24,  6.12s/it] 65%|██████▌   | 98/150 [09:31<05:16,  6.10s/it] 66%|██████▌   | 99/150 [09:37<05:07,  6.02s/it] 67%|██████▋   | 100/150 [09:42<04:57,  5.95s/it]                                                 {'loss': 0.0468, 'grad_norm': 0.3277588486671448, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [09:42<04:57,  5.95s/it] 67%|██████▋   | 101/150 [09:48<04:49,  5.91s/it] 68%|██████▊   | 102/150 [09:54<04:42,  5.88s/it] 69%|██████▊   | 103/150 [10:00<04:35,  5.86s/it] 69%|██████▉   | 104/150 [10:06<04:29,  5.86s/it] 70%|███████   | 105/150 [10:12<04:23,  5.85s/it] 71%|███████   | 106/150 [10:17<04:17,  5.86s/it] 71%|███████▏  | 107/150 [10:23<04:11,  5.85s/it] 72%|███████▏  | 108/150 [10:29<04:05,  5.83s/it] 73%|███████▎  | 109/150 [10:35<03:59,  5.83s/it] 73%|███████▎  | 110/150 [10:41<03:53,  5.83s/it]                                                 {'loss': 0.0393, 'grad_norm': 0.399319052696228, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [10:41<03:53,  5.83s/it] 74%|███████▍  | 111/150 [10:47<03:47,  5.83s/it] 75%|███████▍  | 112/150 [10:52<03:42,  5.85s/it] 75%|███████▌  | 113/150 [10:58<03:36,  5.86s/it] 76%|███████▌  | 114/150 [11:04<03:30,  5.86s/it] 77%|███████▋  | 115/150 [11:10<03:24,  5.84s/it] 77%|███████▋  | 116/150 [11:16<03:18,  5.84s/it] 78%|███████▊  | 117/150 [11:22<03:12,  5.85s/it] 79%|███████▊  | 118/150 [11:28<03:07,  5.85s/it] 79%|███████▉  | 119/150 [11:33<03:01,  5.84s/it] 80%|████████  | 120/150 [11:39<02:54,  5.83s/it]                                                 {'loss': 0.0424, 'grad_norm': 0.3060346245765686, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [11:39<02:54,  5.83s/it] 81%|████████  | 121/150 [11:45<02:49,  5.83s/it] 81%|████████▏ | 122/150 [11:51<02:43,  5.83s/it] 82%|████████▏ | 123/150 [11:57<02:37,  5.84s/it] 83%|████████▎ | 124/150 [12:02<02:31,  5.83s/it] 83%|████████▎ | 125/150 [12:08<02:26,  5.86s/it] 84%|████████▍ | 126/150 [12:14<02:20,  5.86s/it] 85%|████████▍ | 127/150 [12:20<02:14,  5.85s/it] 85%|████████▌ | 128/150 [12:26<02:08,  5.85s/it] 86%|████████▌ | 129/150 [12:32<02:02,  5.85s/it] 87%|████████▋ | 130/150 [12:38<01:56,  5.85s/it]                                                 {'loss': 0.0301, 'grad_norm': 0.1189461424946785, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [12:38<01:56,  5.85s/it] 87%|████████▋ | 131/150 [12:43<01:51,  5.84s/it] 88%|████████▊ | 132/150 [12:49<01:45,  5.84s/it] 89%|████████▊ | 133/150 [12:55<01:39,  5.85s/it] 89%|████████▉ | 134/150 [13:01<01:33,  5.84s/it] 90%|█████████ | 135/150 [13:07<01:27,  5.84s/it] 91%|█████████ | 136/150 [13:13<01:21,  5.84s/it] 91%|█████████▏| 137/150 [13:19<01:16,  5.86s/it] 92%|█████████▏| 138/150 [13:24<01:10,  5.86s/it] 93%|█████████▎| 139/150 [13:30<01:04,  5.86s/it] 93%|█████████▎| 140/150 [13:36<00:58,  5.86s/it]                                                 {'loss': 0.0307, 'grad_norm': 0.3103671371936798, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [13:36<00:58,  5.86s/it] 94%|█████████▍| 141/150 [13:42<00:52,  5.88s/it] 95%|█████████▍| 142/150 [13:48<00:47,  5.88s/it] 95%|█████████▌| 143/150 [13:54<00:41,  5.88s/it] 96%|█████████▌| 144/150 [14:00<00:35,  5.88s/it] 97%|█████████▋| 145/150 [14:06<00:29,  5.86s/it] 97%|█████████▋| 146/150 [14:11<00:23,  5.84s/it] 98%|█████████▊| 147/150 [14:17<00:17,  5.83s/it] 99%|█████████▊| 148/150 [14:23<00:11,  5.84s/it] 99%|█████████▉| 149/150 [14:29<00:05,  5.84s/it]100%|██████████| 150/150 [14:35<00:00,  5.85s/it]                                                 {'loss': 0.0417, 'grad_norm': 0.14869527518749237, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [14:35<00:00,  5.85s/it][INFO|trainer.py:3705] 2024-11-18 22:55:39,175 >> Saving model checkpoint to saves/gemma-2-9b-it/vector_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:55:39,219 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:55:39,221 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:55:39,441 >> tokenizer config file saved in saves/gemma-2-9b-it/vector_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:55:39,442 >> Special tokens file saved in saves/gemma-2-9b-it/vector_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:55:40,693 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 877.609, 'train_samples_per_second': 2.755, 'train_steps_per_second': 0.171, 'train_loss': 0.1712010775009791, 'epoch': 2.98}
100%|██████████| 150/150 [14:36<00:00,  5.85s/it]100%|██████████| 150/150 [14:36<00:00,  5.84s/it]
[INFO|trainer.py:3705] 2024-11-18 22:55:40,696 >> Saving model checkpoint to saves/gemma-2-9b-it/vector_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:55:40,735 >> loading configuration file /home/work/liuytest/demo/gemma-2-9b-it/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:55:40,737 >> Model config Gemma2Config {
  "_name_or_path": "unsloth/gemma-2-9b-it",
  "architectures": [
    "Gemma2ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "attn_logit_softcapping": 50.0,
  "bos_token_id": 2,
  "cache_implementation": "hybrid",
  "eos_token_id": 1,
  "final_logit_softcapping": 30.0,
  "head_dim": 256,
  "hidden_act": "gelu_pytorch_tanh",
  "hidden_activation": "gelu_pytorch_tanh",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "gemma2",
  "num_attention_heads": 16,
  "num_hidden_layers": 42,
  "num_key_value_heads": 8,
  "pad_token_id": 0,
  "query_pre_attn_scalar": 256,
  "rms_norm_eps": 1e-06,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "sliding_window_size": 4096,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 256000
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:55:40,947 >> tokenizer config file saved in saves/gemma-2-9b-it/vector_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:55:40,947 >> Special tokens file saved in saves/gemma-2-9b-it/vector_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 30740965GF
  train_loss               =     0.1712
  train_runtime            = 0:14:37.60
  train_samples_per_second =      2.755
  train_steps_per_second   =      0.171
Figure saved at: saves/gemma-2-9b-it/vector_prompt/lora/sft/training_loss.png
11/18/2024 22:55:41 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:55:41 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:55:41,787 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:55:41,787 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:55:41,788 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:03, 11.04it/s]  9%|▉         | 4/45 [00:00<00:05,  6.87it/s] 11%|█         | 5/45 [00:00<00:06,  6.42it/s] 13%|█▎        | 6/45 [00:00<00:06,  6.13it/s] 16%|█▌        | 7/45 [00:01<00:06,  5.95it/s] 18%|█▊        | 8/45 [00:01<00:06,  5.83it/s] 20%|██        | 9/45 [00:01<00:06,  5.74it/s] 22%|██▏       | 10/45 [00:01<00:06,  5.67it/s] 24%|██▍       | 11/45 [00:01<00:06,  5.63it/s] 27%|██▋       | 12/45 [00:01<00:05,  5.59it/s] 29%|██▉       | 13/45 [00:02<00:05,  5.58it/s] 31%|███       | 14/45 [00:02<00:05,  5.58it/s] 33%|███▎      | 15/45 [00:02<00:05,  5.57it/s] 36%|███▌      | 16/45 [00:02<00:05,  5.54it/s] 38%|███▊      | 17/45 [00:02<00:05,  5.53it/s] 40%|████      | 18/45 [00:03<00:04,  5.53it/s] 42%|████▏     | 19/45 [00:03<00:04,  5.53it/s] 44%|████▍     | 20/45 [00:03<00:04,  5.54it/s] 47%|████▋     | 21/45 [00:03<00:04,  5.50it/s] 49%|████▉     | 22/45 [00:03<00:04,  5.47it/s] 51%|█████     | 23/45 [00:03<00:04,  5.45it/s] 53%|█████▎    | 24/45 [00:04<00:03,  5.45it/s] 56%|█████▌    | 25/45 [00:04<00:03,  5.45it/s] 58%|█████▊    | 26/45 [00:04<00:03,  5.45it/s] 60%|██████    | 27/45 [00:04<00:03,  5.45it/s] 62%|██████▏   | 28/45 [00:04<00:03,  5.46it/s] 64%|██████▍   | 29/45 [00:05<00:02,  5.45it/s] 67%|██████▋   | 30/45 [00:05<00:02,  5.45it/s] 69%|██████▉   | 31/45 [00:05<00:02,  5.48it/s] 71%|███████   | 32/45 [00:05<00:02,  5.48it/s] 73%|███████▎  | 33/45 [00:05<00:02,  5.48it/s] 76%|███████▌  | 34/45 [00:06<00:02,  5.49it/s] 78%|███████▊  | 35/45 [00:06<00:01,  5.50it/s] 80%|████████  | 36/45 [00:06<00:01,  5.50it/s] 82%|████████▏ | 37/45 [00:06<00:01,  5.49it/s] 84%|████████▍ | 38/45 [00:06<00:01,  5.47it/s] 87%|████████▋ | 39/45 [00:06<00:01,  5.46it/s] 89%|████████▉ | 40/45 [00:07<00:00,  5.47it/s] 91%|█████████ | 41/45 [00:07<00:00,  5.44it/s] 93%|█████████▎| 42/45 [00:07<00:00,  5.44it/s] 96%|█████████▌| 43/45 [00:07<00:00,  5.44it/s] 98%|█████████▊| 44/45 [00:07<00:00,  5.46it/s]100%|██████████| 45/45 [00:08<00:00,  5.47it/s]100%|██████████| 45/45 [00:08<00:00,  5.61it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0634
  eval_runtime            = 0:00:08.21
  eval_samples_per_second =     10.949
  eval_steps_per_second   =      5.475
[INFO|modelcard.py:449] 2024-11-18 22:55:50,008 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:56:12,527] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:56:16 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22598
[2024-11-18 22:56:18,711] torch.distributed.run: [WARNING] 
[2024-11-18 22:56:18,711] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:56:18,711] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:56:18,711] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:56:25,805] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:56:26,309] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:56:26 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:56:26 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:56:26,890 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 22:56:26,892 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:56:26,893 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:26,898 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:26,898 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:26,898 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:26,898 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:26,898 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:56:27,459 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:56:27,460 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 22:56:27,461 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:56:27,462 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:27,463 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:27,464 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:27,464 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:27,464 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:56:27,464 >> loading file tokenizer.json
11/18/2024 22:56:27 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:56:27 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:56:28,020 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:56:28 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 22:56:28 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
11/18/2024 22:56:28 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
11/18/2024 22:56:28 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 22:56:28 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 429.60 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3205.09 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:56:31 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:02<00:34, 24.53 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:04<00:27, 28.53 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:05<00:24, 29.59 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:07<00:22, 30.13 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:09<00:20, 30.28 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:11<00:18, 30.74 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:13<00:16, 30.90 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:14<00:14, 30.81 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:16<00:12, 30.59 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:18<00:10, 31.09 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:20<00:09, 31.05 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:22<00:07, 31.15 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:23<00:05, 31.24 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:25<00:03, 31.02 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:27<00:01, 30.96 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 31.18 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 30.54 examples/s]
training example:
input_ids:
[151331, 151333, 151336, 198, 98964, 98493, 99977, 99369, 100132, 122347, 98598, 102811, 98314, 56333, 27985, 102961, 510, 108820, 98346, 38744, 1400, 81, 1776, 292, 2039, 36927, 105306, 106422, 98314, 108384, 5373, 105358, 5373, 100132, 98327, 117260, 3837, 98512, 104559, 104172, 98314, 3144, 98327, 100695, 3837, 99312, 99067, 104172, 100695, 111279, 151337, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
inputs:
[gMASK] <sop> <|user|> 
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序 <|assistant|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
label_ids:
[151329, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
labels:
<|endoftext|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
[INFO|configuration_utils.py:673] 2024-11-18 22:57:04,269 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 22:57:04,271 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:57:04,272 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:57:04,340 >> loading weights file /home/work/liuytest/demo/glm-4-9b-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:57:04,341 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:57:04,343 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:06,  1.32it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.41it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:02<00:05,  1.32it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:04,  1.33it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:05,  1.54it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.36it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:04,  1.61it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.56it/s]Loading checkpoint shards:  60%|██████    | 6/10 [00:04<00:02,  1.34it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.55it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:05<00:02,  1.35it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.56it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:05<00:01,  1.38it/s]Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.54it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:06<00:00,  1.36it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.55it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:07<00:00,  1.37it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:57:11,772 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:4582] 2024-11-18 22:57:11,772 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/work/liuytest/demo/glm-4-9b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:57:11,780 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:57:11,780 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "max_length": 128000,
  "pad_token_id": 151329,
  "temperature": 0.8,
  "top_p": 0.8
}

11/18/2024 22:57:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:57:11 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:57:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:57:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:57:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: query_key_value,dense_4h_to_h,dense,dense_h_to_4h
Loading checkpoint shards:  80%|████████  | 8/10 [00:05<00:01,  1.59it/s]11/18/2024 22:57:12 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:57:12,448 >> Using auto half precision backend
Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.59it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]
11/18/2024 22:57:13 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:57:13 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:57:13 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:57:13 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:57:13 - INFO - llamafactory.model.model_utils.misc - Found linear modules: query_key_value,dense_h_to_4h,dense_4h_to_h,dense
11/18/2024 22:57:13 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
[INFO|trainer.py:2243] 2024-11-18 22:57:14,484 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:57:14,484 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:57:14,484 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:57:14,484 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:57:14,484 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:57:14,484 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:57:14,484 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:57:14,493 >>   Number of trainable parameters = 21,176,320
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:05<14:31,  5.85s/it]  1%|▏         | 2/150 [00:11<13:53,  5.63s/it]  2%|▏         | 3/150 [00:16<13:08,  5.36s/it]  3%|▎         | 4/150 [00:21<13:04,  5.38s/it]  3%|▎         | 5/150 [00:26<12:41,  5.25s/it]  4%|▍         | 6/150 [00:32<12:41,  5.29s/it]  5%|▍         | 7/150 [00:37<12:37,  5.30s/it]  5%|▌         | 8/150 [00:42<12:20,  5.22s/it]  6%|▌         | 9/150 [00:47<12:20,  5.25s/it]  7%|▋         | 10/150 [00:52<12:06,  5.19s/it]                                                {'loss': 1.9392, 'grad_norm': 5.366310119628906, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:52<12:06,  5.19s/it]  7%|▋         | 11/150 [00:58<12:06,  5.23s/it]  8%|▊         | 12/150 [01:03<11:50,  5.15s/it]  9%|▊         | 13/150 [01:08<11:49,  5.18s/it]  9%|▉         | 14/150 [01:13<11:45,  5.19s/it] 10%|█         | 15/150 [01:18<11:33,  5.14s/it] 11%|█         | 16/150 [01:23<11:34,  5.18s/it] 11%|█▏        | 17/150 [01:28<11:23,  5.14s/it] 12%|█▏        | 18/150 [01:34<11:24,  5.18s/it] 13%|█▎        | 19/150 [01:39<11:23,  5.22s/it] 13%|█▎        | 20/150 [01:44<11:15,  5.20s/it]                                                {'loss': 0.3453, 'grad_norm': 3.310049295425415, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:44<11:15,  5.20s/it] 14%|█▍        | 21/150 [01:49<11:13,  5.22s/it] 15%|█▍        | 22/150 [01:54<10:59,  5.15s/it] 15%|█▌        | 23/150 [02:00<10:58,  5.19s/it] 16%|█▌        | 24/150 [02:05<10:46,  5.13s/it] 17%|█▋        | 25/150 [02:10<10:45,  5.17s/it] 17%|█▋        | 26/150 [02:15<10:34,  5.12s/it] 18%|█▊        | 27/150 [02:20<10:36,  5.18s/it] 19%|█▊        | 28/150 [02:26<10:34,  5.20s/it] 19%|█▉        | 29/150 [02:31<10:24,  5.16s/it] 20%|██        | 30/150 [02:36<10:25,  5.21s/it]                                                {'loss': 0.1137, 'grad_norm': 1.2667255401611328, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:36<10:25,  5.21s/it] 21%|██        | 31/150 [02:41<10:13,  5.16s/it] 21%|██▏       | 32/150 [02:46<10:14,  5.21s/it] 22%|██▏       | 33/150 [02:52<10:12,  5.23s/it] 23%|██▎       | 34/150 [02:57<10:00,  5.18s/it] 23%|██▎       | 35/150 [03:02<09:59,  5.21s/it] 24%|██▍       | 36/150 [03:07<09:48,  5.16s/it] 25%|██▍       | 37/150 [03:12<09:47,  5.20s/it] 25%|██▌       | 38/150 [03:17<09:37,  5.16s/it] 26%|██▌       | 39/150 [03:23<09:36,  5.19s/it] 27%|██▋       | 40/150 [03:28<09:34,  5.22s/it]                                                {'loss': 0.0818, 'grad_norm': 0.8890020847320557, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [03:28<09:34,  5.22s/it] 27%|██▋       | 41/150 [03:33<09:25,  5.19s/it] 28%|██▊       | 42/150 [03:38<09:26,  5.25s/it] 29%|██▊       | 43/150 [03:44<09:16,  5.20s/it] 29%|██▉       | 44/150 [03:49<09:17,  5.26s/it] 30%|███       | 45/150 [03:54<09:08,  5.22s/it] 31%|███       | 46/150 [03:59<09:07,  5.27s/it] 31%|███▏      | 47/150 [04:05<08:59,  5.24s/it] 32%|███▏      | 48/150 [04:10<08:58,  5.28s/it] 33%|███▎      | 49/150 [04:15<08:57,  5.32s/it] 33%|███▎      | 50/150 [04:20<08:46,  5.26s/it]                                                {'loss': 0.0709, 'grad_norm': 0.9540853500366211, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [04:20<08:46,  5.26s/it] 34%|███▍      | 51/150 [04:26<08:45,  5.31s/it] 35%|███▍      | 52/150 [04:31<08:33,  5.24s/it] 35%|███▌      | 53/150 [04:36<08:31,  5.27s/it] 36%|███▌      | 54/150 [04:42<08:27,  5.29s/it] 37%|███▋      | 55/150 [04:47<08:16,  5.23s/it] 37%|███▋      | 56/150 [04:52<08:15,  5.27s/it] 38%|███▊      | 57/150 [04:57<08:07,  5.25s/it] 39%|███▊      | 58/150 [05:03<08:07,  5.30s/it] 39%|███▉      | 59/150 [05:08<08:04,  5.33s/it] 40%|████      | 60/150 [05:13<07:54,  5.27s/it]                                                {'loss': 0.0537, 'grad_norm': 0.4790442883968353, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [05:13<07:54,  5.27s/it] 41%|████      | 61/150 [05:19<07:55,  5.34s/it] 41%|████▏     | 62/150 [05:24<07:43,  5.27s/it] 42%|████▏     | 63/150 [05:29<07:42,  5.31s/it] 43%|████▎     | 64/150 [05:35<07:38,  5.33s/it] 43%|████▎     | 65/150 [05:40<07:35,  5.36s/it] 44%|████▍     | 66/150 [05:45<07:29,  5.35s/it] 45%|████▍     | 67/150 [05:51<07:19,  5.29s/it] 45%|████▌     | 68/150 [05:56<07:16,  5.32s/it] 46%|████▌     | 69/150 [06:01<07:05,  5.25s/it] 47%|████▋     | 70/150 [06:06<07:03,  5.30s/it]                                                {'loss': 0.0471, 'grad_norm': 0.6107298135757446, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [06:06<07:03,  5.30s/it] 47%|████▋     | 71/150 [06:12<06:54,  5.25s/it] 48%|████▊     | 72/150 [06:17<06:49,  5.25s/it] 49%|████▊     | 73/150 [06:22<06:46,  5.28s/it] 49%|████▉     | 74/150 [06:27<06:38,  5.24s/it] 50%|█████     | 75/150 [06:33<06:36,  5.29s/it] 51%|█████     | 76/150 [06:38<06:28,  5.26s/it] 51%|█████▏    | 77/150 [06:43<06:27,  5.31s/it] 52%|█████▏    | 78/150 [06:49<06:23,  5.33s/it] 53%|█████▎    | 79/150 [06:54<06:16,  5.30s/it] 53%|█████▎    | 80/150 [07:00<06:17,  5.40s/it]                                                {'loss': 0.0518, 'grad_norm': 0.6024383306503296, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [07:00<06:17,  5.40s/it] 54%|█████▍    | 81/150 [07:05<06:06,  5.32s/it] 55%|█████▍    | 82/150 [07:10<06:04,  5.36s/it] 55%|█████▌    | 83/150 [07:15<05:54,  5.30s/it] 56%|█████▌    | 84/150 [07:21<05:52,  5.35s/it] 57%|█████▋    | 85/150 [07:26<05:47,  5.35s/it] 57%|█████▋    | 86/150 [07:31<05:38,  5.29s/it] 58%|█████▊    | 87/150 [07:37<05:35,  5.33s/it] 59%|█████▊    | 88/150 [07:42<05:26,  5.27s/it] 59%|█████▉    | 89/150 [07:47<05:24,  5.32s/it] 60%|██████    | 90/150 [07:52<05:15,  5.26s/it]                                                {'loss': 0.0528, 'grad_norm': 0.5343631505966187, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [07:52<05:15,  5.26s/it] 61%|██████    | 91/150 [07:58<05:13,  5.31s/it] 61%|██████▏   | 92/150 [08:03<05:04,  5.25s/it] 62%|██████▏   | 93/150 [08:08<05:01,  5.29s/it] 63%|██████▎   | 94/150 [08:14<04:57,  5.31s/it] 63%|██████▎   | 95/150 [08:19<04:49,  5.26s/it] 64%|██████▍   | 96/150 [08:24<04:46,  5.31s/it] 65%|██████▍   | 97/150 [08:29<04:37,  5.24s/it] 65%|██████▌   | 98/150 [08:35<04:34,  5.29s/it] 66%|██████▌   | 99/150 [08:40<04:31,  5.32s/it] 67%|██████▋   | 100/150 [08:45<04:22,  5.26s/it]                                                 {'loss': 0.052, 'grad_norm': 0.47637102007865906, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [08:45<04:22,  5.26s/it] 67%|██████▋   | 101/150 [08:51<04:19,  5.29s/it] 68%|██████▊   | 102/150 [08:56<04:11,  5.24s/it] 69%|██████▊   | 103/150 [09:01<04:08,  5.29s/it] 69%|██████▉   | 104/150 [09:07<04:04,  5.32s/it] 70%|███████   | 105/150 [09:12<03:56,  5.26s/it] 71%|███████   | 106/150 [09:17<03:53,  5.30s/it] 71%|███████▏  | 107/150 [09:22<03:46,  5.26s/it] 72%|███████▏  | 108/150 [09:28<03:42,  5.31s/it] 73%|███████▎  | 109/150 [09:33<03:38,  5.34s/it] 73%|███████▎  | 110/150 [09:38<03:34,  5.37s/it]                                                 {'loss': 0.0395, 'grad_norm': 0.37660130858421326, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [09:38<03:34,  5.37s/it] 74%|███████▍  | 111/150 [09:44<03:29,  5.37s/it] 75%|███████▍  | 112/150 [09:49<03:21,  5.30s/it] 75%|███████▌  | 113/150 [09:54<03:16,  5.32s/it] 76%|███████▌  | 114/150 [09:59<03:09,  5.26s/it] 77%|███████▋  | 115/150 [10:05<03:05,  5.30s/it] 77%|███████▋  | 116/150 [10:10<02:58,  5.26s/it] 78%|███████▊  | 117/150 [10:15<02:55,  5.31s/it] 79%|███████▊  | 118/150 [10:21<02:51,  5.35s/it] 79%|███████▉  | 119/150 [10:26<02:43,  5.28s/it] 80%|████████  | 120/150 [10:31<02:39,  5.33s/it]                                                 {'loss': 0.0406, 'grad_norm': 0.6547417640686035, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [10:31<02:39,  5.33s/it] 81%|████████  | 121/150 [10:37<02:33,  5.28s/it] 81%|████████▏ | 122/150 [10:42<02:28,  5.32s/it] 82%|████████▏ | 123/150 [10:47<02:24,  5.35s/it] 83%|████████▎ | 124/150 [10:53<02:17,  5.29s/it] 83%|████████▎ | 125/150 [10:58<02:13,  5.33s/it] 84%|████████▍ | 126/150 [11:03<02:06,  5.27s/it] 85%|████████▍ | 127/150 [11:09<02:01,  5.30s/it] 85%|████████▌ | 128/150 [11:14<01:55,  5.25s/it] 86%|████████▌ | 129/150 [11:19<01:51,  5.29s/it] 87%|████████▋ | 130/150 [11:25<01:46,  5.34s/it]                                                 {'loss': 0.0287, 'grad_norm': 0.31276965141296387, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [11:25<01:46,  5.34s/it] 87%|████████▋ | 131/150 [11:30<01:40,  5.27s/it] 88%|████████▊ | 132/150 [11:35<01:35,  5.31s/it] 89%|████████▊ | 133/150 [11:40<01:29,  5.26s/it] 89%|████████▉ | 134/150 [11:46<01:24,  5.31s/it] 90%|█████████ | 135/150 [11:51<01:19,  5.28s/it] 91%|█████████ | 136/150 [11:56<01:14,  5.34s/it] 91%|█████████▏| 137/150 [12:01<01:08,  5.28s/it] 92%|█████████▏| 138/150 [12:07<01:03,  5.30s/it] 93%|█████████▎| 139/150 [12:12<00:58,  5.35s/it] 93%|█████████▎| 140/150 [12:17<00:52,  5.28s/it]                                                 {'loss': 0.0281, 'grad_norm': 0.5996454358100891, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [12:17<00:52,  5.28s/it] 94%|█████████▍| 141/150 [12:23<00:47,  5.33s/it] 95%|█████████▍| 142/150 [12:28<00:42,  5.27s/it] 95%|█████████▌| 143/150 [12:33<00:37,  5.31s/it] 96%|█████████▌| 144/150 [12:39<00:32,  5.34s/it] 97%|█████████▋| 145/150 [12:44<00:26,  5.28s/it] 97%|█████████▋| 146/150 [12:49<00:21,  5.32s/it] 98%|█████████▊| 147/150 [12:54<00:15,  5.27s/it] 99%|█████████▊| 148/150 [13:00<00:10,  5.31s/it] 99%|█████████▉| 149/150 [13:05<00:05,  5.35s/it]100%|██████████| 150/150 [13:10<00:00,  5.29s/it]                                                 {'loss': 0.0395, 'grad_norm': 0.2734386622905731, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [13:10<00:00,  5.29s/it][INFO|trainer.py:3705] 2024-11-18 23:10:26,369 >> Saving model checkpoint to saves/glm-4-9b-chat/normal_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 23:10:26,404 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:10:26,405 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:10:26,591 >> tokenizer config file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:10:26,591 >> Special tokens file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:10:26,591 >> added tokens file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/checkpoint-150/added_tokens.json
[INFO|trainer.py:2505] 2024-11-18 23:10:27,029 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 792.5362, 'train_samples_per_second': 3.051, 'train_steps_per_second': 0.189, 'train_loss': 0.19899005393187205, 'epoch': 2.98}
100%|██████████| 150/150 [13:11<00:00,  5.29s/it]100%|██████████| 150/150 [13:11<00:00,  5.28s/it]
[INFO|trainer.py:3705] 2024-11-18 23:10:27,032 >> Saving model checkpoint to saves/glm-4-9b-chat/normal_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 23:10:27,063 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:10:27,065 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:10:27,245 >> tokenizer config file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:10:27,245 >> Special tokens file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:10:27,245 >> added tokens file saved in saves/glm-4-9b-chat/normal_prompt/lora/sft/added_tokens.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 13455314GF
  train_loss               =      0.199
  train_runtime            = 0:13:12.53
  train_samples_per_second =      3.051
  train_steps_per_second   =      0.189
Figure saved at: saves/glm-4-9b-chat/normal_prompt/lora/sft/training_loss.png
11/18/2024 23:10:27 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 23:10:27 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 23:10:27,441 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 23:10:27,441 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 23:10:27,441 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 16.78it/s]  9%|▉         | 4/45 [00:00<00:03, 10.68it/s] 13%|█▎        | 6/45 [00:00<00:04,  9.59it/s] 18%|█▊        | 8/45 [00:00<00:04,  9.10it/s] 20%|██        | 9/45 [00:00<00:03,  9.02it/s] 22%|██▏       | 10/45 [00:01<00:03,  8.95it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.90it/s] 27%|██▋       | 12/45 [00:01<00:03,  8.86it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.78it/s] 31%|███       | 14/45 [00:01<00:03,  8.65it/s] 33%|███▎      | 15/45 [00:01<00:03,  8.66it/s] 36%|███▌      | 16/45 [00:01<00:03,  8.58it/s] 38%|███▊      | 17/45 [00:01<00:03,  8.58it/s] 40%|████      | 18/45 [00:01<00:03,  8.65it/s] 42%|████▏     | 19/45 [00:02<00:02,  8.70it/s] 44%|████▍     | 20/45 [00:02<00:02,  8.75it/s] 47%|████▋     | 21/45 [00:02<00:02,  8.73it/s] 49%|████▉     | 22/45 [00:02<00:02,  8.69it/s] 51%|█████     | 23/45 [00:02<00:02,  8.68it/s] 53%|█████▎    | 24/45 [00:02<00:02,  8.67it/s] 56%|█████▌    | 25/45 [00:02<00:02,  8.74it/s] 58%|█████▊    | 26/45 [00:02<00:02,  8.74it/s] 60%|██████    | 27/45 [00:03<00:02,  8.72it/s] 62%|██████▏   | 28/45 [00:03<00:01,  8.69it/s] 64%|██████▍   | 29/45 [00:03<00:01,  8.67it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.62it/s] 69%|██████▉   | 31/45 [00:03<00:01,  8.60it/s] 71%|███████   | 32/45 [00:03<00:01,  8.61it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.65it/s] 76%|███████▌  | 34/45 [00:03<00:01,  8.62it/s] 78%|███████▊  | 35/45 [00:03<00:01,  8.58it/s] 80%|████████  | 36/45 [00:04<00:01,  8.59it/s] 82%|████████▏ | 37/45 [00:04<00:00,  8.56it/s] 84%|████████▍ | 38/45 [00:04<00:00,  8.58it/s] 87%|████████▋ | 39/45 [00:04<00:00,  8.62it/s] 89%|████████▉ | 40/45 [00:04<00:00,  8.57it/s] 91%|█████████ | 41/45 [00:04<00:00,  8.53it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.57it/s] 96%|█████████▌| 43/45 [00:04<00:00,  8.60it/s] 98%|█████████▊| 44/45 [00:04<00:00,  8.59it/s]100%|██████████| 45/45 [00:05<00:00,  8.61it/s]100%|██████████| 45/45 [00:05<00:00,  8.81it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0684
  eval_runtime            = 0:00:05.24
  eval_samples_per_second =     17.162
  eval_steps_per_second   =      8.581
[INFO|modelcard.py:449] 2024-11-18 23:10:32,686 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:10:52,030] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:10:56 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22371
[2024-11-18 23:10:58,097] torch.distributed.run: [WARNING] 
[2024-11-18 23:10:58,097] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 23:10:58,097] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 23:10:58,097] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:11:05,300] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 23:11:05,572] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:11:06 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:11:06 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 23:11:07 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:11:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 23:11:07,102 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:11:07,104 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:11:07,105 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,110 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,110 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,110 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,110 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,110 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:11:07,655 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 23:11:07,657 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:11:07,658 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:11:07,659 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,661 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,661 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,661 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,661 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:11:07,661 >> loading file tokenizer.json
11/18/2024 23:11:07 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:11:07 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:11:08,218 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 23:11:08 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:11:08 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
11/18/2024 23:11:08 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 320.19 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2548.41 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 23:11:12 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:02<00:36, 22.83 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:04<00:29, 26.37 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:06<00:25, 28.10 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:08<00:23, 28.52 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:09<00:21, 29.21 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:11<00:18, 29.84 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:13<00:16, 30.22 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:15<00:14, 30.49 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:17<00:12, 30.59 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:18<00:10, 30.64 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:20<00:09, 30.65 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:22<00:07, 30.69 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:24<00:05, 30.63 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:26<00:03, 30.70 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:28<00:01, 30.81 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 31.07 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 29.91 examples/s]
training example:
input_ids:
[151331, 151333, 151336, 198, 113947, 102811, 98314, 17327, 98870, 101223, 28213, 4913, 16881, 788, 4383, 103889, 100173, 98323, 5122, 3586, 2124, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 20987, 1055, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 24963, 2203, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 20987, 1055, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 6182, 497, 330, 103889, 100173, 98323, 5122, 24963, 6182, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 6182, 497, 330, 103889, 100173, 98323, 5122, 54596, 266, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 57409, 497, 330, 103889, 100173, 98323, 5122, 1778, 266, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 57409, 497, 330, 103889, 100173, 98323, 5122, 4648, 9593, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 2593, 39307, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 32252, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 32252, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 31893, 82, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 8461, 11, 103204, 100173, 98323, 25, 57409, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 8984, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 4480, 1055, 11, 103204, 100173, 98323, 25, 2007, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 22526, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 4648, 12718, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 21702, 11, 103204, 100173, 98323, 25, 4578, 11, 109162, 100173, 98323, 25, 4578, 1040, 497, 330, 103889, 100173, 98323, 5122, 1038, 392, 1040, 1055, 11, 103204, 100173, 98323, 25, 4578, 1040, 11, 109162, 100173, 98323, 25, 4578, 1040, 7914, 330, 19969, 788, 4383, 104104, 100173, 25, 6182, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 2007, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11578, 330, 104104, 100173, 25, 8984, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 2332, 516, 364, 12961, 516, 364, 50927, 516, 364, 11523, 516, 364, 29099, 516, 364, 28325, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 57409, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11578, 330, 104104, 100173, 25, 22526, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 2102, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 4578, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11578, 330, 104104, 100173, 25, 2203, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11523, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 4578, 1040, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 91239, 98964, 98493, 99977, 99369, 100132, 122347, 98598, 102811, 98314, 56333, 27985, 102961, 510, 108820, 98346, 38744, 1400, 81, 1776, 292, 2039, 36927, 105306, 106422, 98314, 108384, 5373, 105358, 5373, 100132, 98327, 117260, 3837, 98512, 104559, 104172, 98314, 3144, 98327, 100695, 3837, 99312, 99067, 104172, 100695, 111279, 151337, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
inputs:
[gMASK] <sop> <|user|> 
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序 <|assistant|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
label_ids:
[151329, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
labels:
<|endoftext|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
[INFO|configuration_utils.py:673] 2024-11-18 23:11:45,351 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:11:45,353 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:11:45,354 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|modeling_utils.py:3729] 2024-11-18 23:11:45,417 >> loading weights file /home/work/liuytest/demo/glm-4-9b-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 23:11:45,418 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 23:11:45,419 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:05,  1.53it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.59it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.63it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.66it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:02,  1.67it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:07,  1.23it/s]Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.67it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.47it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.64it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:02<00:04,  1.51it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:04<00:01,  1.63it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.53it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.63it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.57it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.64it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 23:11:51,620 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:4582] 2024-11-18 23:11:51,620 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/work/liuytest/demo/glm-4-9b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 23:11:51,627 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 23:11:51,628 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "max_length": 128000,
  "pad_token_id": 151329,
  "temperature": 0.8,
  "top_p": 0.8
}

11/18/2024 23:11:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:11:51 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:11:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:11:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:11:51 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense_h_to_4h,query_key_value,dense,dense_4h_to_h
Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.59it/s]11/18/2024 23:11:52 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 23:11:52,310 >> Using auto half precision backend
Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.62it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:05<00:01,  1.65it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.58it/s]
11/18/2024 23:11:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:11:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:11:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:11:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:11:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense,dense_4h_to_h,dense_h_to_4h,query_key_value
11/18/2024 23:11:55 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
[INFO|trainer.py:2243] 2024-11-18 23:11:55,820 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 23:11:55,820 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 23:11:55,820 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 23:11:55,820 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 23:11:55,821 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 23:11:55,821 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 23:11:55,821 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 23:11:55,826 >>   Number of trainable parameters = 21,176,320
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:08<21:05,  8.49s/it]  1%|▏         | 2/150 [00:16<20:56,  8.49s/it]  2%|▏         | 3/150 [00:25<20:18,  8.29s/it]  3%|▎         | 4/150 [00:33<20:01,  8.23s/it]  3%|▎         | 5/150 [00:41<19:38,  8.13s/it]  4%|▍         | 6/150 [00:49<19:41,  8.21s/it]  5%|▍         | 7/150 [00:57<19:45,  8.29s/it]  5%|▌         | 8/150 [01:05<19:22,  8.19s/it]  6%|▌         | 9/150 [01:14<19:16,  8.20s/it]  7%|▋         | 10/150 [01:22<18:55,  8.11s/it]                                                {'loss': 1.3107, 'grad_norm': 4.469825744628906, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [01:22<18:55,  8.11s/it]  7%|▋         | 11/150 [01:30<18:52,  8.15s/it]  8%|▊         | 12/150 [01:38<18:43,  8.14s/it]  9%|▊         | 13/150 [01:46<18:39,  8.17s/it]  9%|▉         | 14/150 [01:54<18:31,  8.17s/it] 10%|█         | 15/150 [02:02<18:12,  8.09s/it] 11%|█         | 16/150 [02:10<18:06,  8.11s/it] 11%|█▏        | 17/150 [02:18<17:48,  8.04s/it] 12%|█▏        | 18/150 [02:26<17:43,  8.06s/it] 13%|█▎        | 19/150 [02:34<17:39,  8.09s/it] 13%|█▎        | 20/150 [02:43<17:38,  8.14s/it]                                                {'loss': 0.1592, 'grad_norm': 2.0757956504821777, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [02:43<17:38,  8.14s/it] 14%|█▍        | 21/150 [02:51<17:33,  8.16s/it] 15%|█▍        | 22/150 [02:59<17:17,  8.10s/it] 15%|█▌        | 23/150 [03:07<17:16,  8.16s/it] 16%|█▌        | 24/150 [03:15<16:58,  8.08s/it] 17%|█▋        | 25/150 [03:23<16:55,  8.12s/it] 17%|█▋        | 26/150 [03:31<16:43,  8.09s/it] 18%|█▊        | 27/150 [03:40<16:39,  8.12s/it] 19%|█▊        | 28/150 [03:48<16:31,  8.13s/it] 19%|█▉        | 29/150 [03:56<16:17,  8.08s/it] 20%|██        | 30/150 [04:04<16:18,  8.15s/it]                                                {'loss': 0.0842, 'grad_norm': 0.7089632749557495, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [04:04<16:18,  8.15s/it] 21%|██        | 31/150 [04:12<16:06,  8.12s/it] 21%|██▏       | 32/150 [04:20<15:59,  8.13s/it] 22%|██▏       | 33/150 [04:28<15:54,  8.16s/it] 23%|██▎       | 34/150 [04:36<15:41,  8.12s/it] 23%|██▎       | 35/150 [04:45<15:35,  8.13s/it] 24%|██▍       | 36/150 [04:53<15:21,  8.09s/it] 25%|██▍       | 37/150 [05:01<15:15,  8.10s/it] 25%|██▌       | 38/150 [05:09<15:05,  8.08s/it] 26%|██▌       | 39/150 [05:17<15:01,  8.12s/it] 27%|██▋       | 40/150 [05:25<14:54,  8.13s/it]                                                {'loss': 0.0749, 'grad_norm': 0.8071678876876831, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [05:25<14:54,  8.13s/it] 27%|██▋       | 41/150 [05:33<14:45,  8.12s/it] 28%|██▊       | 42/150 [05:41<14:38,  8.13s/it] 29%|██▊       | 43/150 [05:49<14:27,  8.11s/it] 29%|██▉       | 44/150 [05:58<14:18,  8.10s/it] 30%|███       | 45/150 [06:06<14:08,  8.08s/it] 31%|███       | 46/150 [06:14<14:05,  8.13s/it] 31%|███▏      | 47/150 [06:22<13:52,  8.08s/it] 32%|███▏      | 48/150 [06:30<13:44,  8.08s/it] 33%|███▎      | 49/150 [06:38<13:37,  8.09s/it] 33%|███▎      | 50/150 [06:46<13:24,  8.05s/it]                                                {'loss': 0.0643, 'grad_norm': 0.5733384490013123, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [06:46<13:24,  8.05s/it] 34%|███▍      | 51/150 [06:54<13:16,  8.05s/it] 35%|███▍      | 52/150 [07:02<13:07,  8.04s/it] 35%|███▌      | 53/150 [07:10<13:08,  8.13s/it] 36%|███▌      | 54/150 [07:19<13:03,  8.16s/it] 37%|███▋      | 55/150 [07:27<12:50,  8.11s/it] 37%|███▋      | 56/150 [07:35<12:42,  8.11s/it] 38%|███▊      | 57/150 [07:43<12:36,  8.13s/it] 39%|███▊      | 58/150 [07:51<12:30,  8.16s/it] 39%|███▉      | 59/150 [07:59<12:25,  8.20s/it] 40%|████      | 60/150 [08:07<12:10,  8.12s/it]                                                {'loss': 0.0494, 'grad_norm': 0.4986552298069, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [08:07<12:10,  8.12s/it] 41%|████      | 61/150 [08:16<12:07,  8.17s/it] 41%|████▏     | 62/150 [08:24<11:53,  8.10s/it] 42%|████▏     | 63/150 [08:32<11:46,  8.12s/it] 43%|████▎     | 64/150 [08:40<11:37,  8.12s/it] 43%|████▎     | 65/150 [08:48<11:30,  8.13s/it] 44%|████▍     | 66/150 [08:56<11:21,  8.12s/it] 45%|████▍     | 67/150 [09:04<11:04,  8.01s/it] 45%|████▌     | 68/150 [09:12<10:57,  8.02s/it] 46%|████▌     | 69/150 [09:20<10:47,  8.00s/it] 47%|████▋     | 70/150 [09:28<10:44,  8.06s/it]                                                {'loss': 0.0433, 'grad_norm': 0.5696475505828857, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [09:28<10:44,  8.06s/it] 47%|████▋     | 71/150 [09:36<10:33,  8.02s/it] 48%|████▊     | 72/150 [09:44<10:29,  8.07s/it] 49%|████▊     | 73/150 [09:52<10:23,  8.10s/it] 49%|████▉     | 74/150 [10:00<10:11,  8.05s/it] 50%|█████     | 75/150 [10:08<10:06,  8.09s/it] 51%|█████     | 76/150 [10:16<09:56,  8.06s/it] 51%|█████▏    | 77/150 [10:24<09:48,  8.07s/it] 52%|█████▏    | 78/150 [10:33<09:48,  8.17s/it] 53%|█████▎    | 79/150 [10:41<09:38,  8.15s/it] 53%|█████▎    | 80/150 [10:49<09:32,  8.17s/it]                                                {'loss': 0.0462, 'grad_norm': 0.46157774329185486, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [10:49<09:32,  8.17s/it] 54%|█████▍    | 81/150 [10:57<09:18,  8.09s/it] 55%|█████▍    | 82/150 [11:05<09:14,  8.16s/it] 55%|█████▌    | 83/150 [11:13<09:04,  8.12s/it] 56%|█████▌    | 84/150 [11:22<08:58,  8.15s/it] 57%|█████▋    | 85/150 [11:30<08:49,  8.14s/it] 57%|█████▋    | 86/150 [11:38<08:38,  8.10s/it] 58%|█████▊    | 87/150 [11:46<08:31,  8.12s/it] 59%|█████▊    | 88/150 [11:54<08:21,  8.09s/it] 59%|█████▉    | 89/150 [12:02<08:13,  8.09s/it] 60%|██████    | 90/150 [12:10<08:06,  8.12s/it]                                                {'loss': 0.0501, 'grad_norm': 0.44363102316856384, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [12:10<08:06,  8.12s/it] 61%|██████    | 91/150 [12:18<07:58,  8.11s/it] 61%|██████▏   | 92/150 [12:26<07:46,  8.04s/it] 62%|██████▏   | 93/150 [12:34<07:42,  8.11s/it] 63%|██████▎   | 94/150 [12:43<07:36,  8.16s/it] 63%|██████▎   | 95/150 [12:51<07:24,  8.09s/it] 64%|██████▍   | 96/150 [12:59<07:17,  8.09s/it] 65%|██████▍   | 97/150 [13:07<07:07,  8.07s/it] 65%|██████▌   | 98/150 [13:15<07:01,  8.11s/it] 66%|██████▌   | 99/150 [13:23<06:55,  8.15s/it] 67%|██████▋   | 100/150 [13:31<06:46,  8.13s/it]                                                 {'loss': 0.0466, 'grad_norm': 0.4409795105457306, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [13:31<06:46,  8.13s/it] 67%|██████▋   | 101/150 [13:40<06:40,  8.18s/it] 68%|██████▊   | 102/150 [13:48<06:29,  8.12s/it] 69%|██████▊   | 103/150 [13:56<06:23,  8.17s/it] 69%|██████▉   | 104/150 [14:04<06:16,  8.18s/it] 70%|███████   | 105/150 [14:12<06:05,  8.13s/it] 71%|███████   | 106/150 [14:20<05:59,  8.17s/it] 71%|███████▏  | 107/150 [14:28<05:49,  8.12s/it] 72%|███████▏  | 108/150 [14:37<05:43,  8.19s/it] 73%|███████▎  | 109/150 [14:45<05:36,  8.20s/it] 73%|███████▎  | 110/150 [14:53<05:26,  8.15s/it]                                                 {'loss': 0.038, 'grad_norm': 0.509580671787262, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [14:53<05:26,  8.15s/it] 74%|███████▍  | 111/150 [15:01<05:19,  8.20s/it] 75%|███████▍  | 112/150 [15:09<05:10,  8.17s/it] 75%|███████▌  | 113/150 [15:18<05:03,  8.21s/it] 76%|███████▌  | 114/150 [15:26<04:52,  8.13s/it] 77%|███████▋  | 115/150 [15:34<04:45,  8.16s/it] 77%|███████▋  | 116/150 [15:42<04:35,  8.10s/it] 78%|███████▊  | 117/150 [15:50<04:28,  8.13s/it] 79%|███████▊  | 118/150 [15:58<04:20,  8.14s/it] 79%|███████▉  | 119/150 [16:06<04:10,  8.09s/it] 80%|████████  | 120/150 [16:14<04:03,  8.12s/it]                                                 {'loss': 0.0402, 'grad_norm': 0.43397825956344604, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [16:14<04:03,  8.12s/it] 81%|████████  | 121/150 [16:22<03:54,  8.08s/it] 81%|████████▏ | 122/150 [16:31<03:46,  8.10s/it] 82%|████████▏ | 123/150 [16:39<03:39,  8.12s/it] 83%|████████▎ | 124/150 [16:47<03:28,  8.03s/it] 83%|████████▎ | 125/150 [16:55<03:20,  8.03s/it] 84%|████████▍ | 126/150 [17:03<03:12,  8.03s/it] 85%|████████▍ | 127/150 [17:11<03:05,  8.06s/it] 85%|████████▌ | 128/150 [17:19<02:57,  8.05s/it] 86%|████████▌ | 129/150 [17:27<02:49,  8.09s/it] 87%|████████▋ | 130/150 [17:35<02:41,  8.08s/it]                                                 {'loss': 0.0262, 'grad_norm': 0.23576225340366364, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [17:35<02:41,  8.08s/it] 87%|████████▋ | 131/150 [17:43<02:33,  8.06s/it] 88%|████████▊ | 132/150 [17:51<02:26,  8.14s/it] 89%|████████▊ | 133/150 [17:59<02:17,  8.06s/it] 89%|████████▉ | 134/150 [18:07<02:09,  8.12s/it] 90%|█████████ | 135/150 [18:15<02:01,  8.08s/it] 91%|█████████ | 136/150 [18:24<01:53,  8.08s/it] 91%|█████████▏| 137/150 [18:32<01:44,  8.05s/it] 92%|█████████▏| 138/150 [18:40<01:37,  8.15s/it] 93%|█████████▎| 139/150 [18:48<01:29,  8.16s/it] 93%|█████████▎| 140/150 [18:56<01:20,  8.09s/it]                                                 {'loss': 0.0283, 'grad_norm': 0.5799323320388794, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [18:56<01:20,  8.09s/it] 94%|█████████▍| 141/150 [19:04<01:12,  8.08s/it] 95%|█████████▍| 142/150 [19:12<01:04,  8.05s/it] 95%|█████████▌| 143/150 [19:20<00:56,  8.11s/it] 96%|█████████▌| 144/150 [19:28<00:48,  8.12s/it] 97%|█████████▋| 145/150 [19:36<00:40,  8.09s/it] 97%|█████████▋| 146/150 [19:45<00:32,  8.16s/it] 98%|█████████▊| 147/150 [19:53<00:24,  8.14s/it] 99%|█████████▊| 148/150 [20:01<00:16,  8.17s/it] 99%|█████████▉| 149/150 [20:09<00:08,  8.21s/it]100%|██████████| 150/150 [20:17<00:00,  8.11s/it]                                                 {'loss': 0.0381, 'grad_norm': 0.2911889851093292, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [20:17<00:00,  8.11s/it][INFO|trainer.py:3705] 2024-11-18 23:32:14,541 >> Saving model checkpoint to saves/glm-4-9b-chat/schema_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 23:32:14,576 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:32:14,577 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:32:14,766 >> tokenizer config file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:32:14,766 >> Special tokens file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:32:14,766 >> added tokens file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/checkpoint-150/added_tokens.json
[INFO|trainer.py:2505] 2024-11-18 23:32:15,198 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1219.3721, 'train_samples_per_second': 1.983, 'train_steps_per_second': 0.123, 'train_loss': 0.13998681386311848, 'epoch': 2.98}
100%|██████████| 150/150 [20:18<00:00,  8.11s/it]100%|██████████| 150/150 [20:18<00:00,  8.12s/it]
[INFO|trainer.py:3705] 2024-11-18 23:32:15,201 >> Saving model checkpoint to saves/glm-4-9b-chat/schema_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 23:32:15,233 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:32:15,234 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:32:15,420 >> tokenizer config file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:32:15,420 >> Special tokens file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:32:15,420 >> added tokens file saved in saves/glm-4-9b-chat/schema_prompt/lora/sft/added_tokens.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 98672040GF
  train_loss               =       0.14
  train_runtime            = 0:20:19.37
  train_samples_per_second =      1.983
  train_steps_per_second   =      0.123
Figure saved at: saves/glm-4-9b-chat/schema_prompt/lora/sft/training_loss.png
11/18/2024 23:32:15 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 23:32:15 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 23:32:15,608 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 23:32:15,608 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 23:32:15,608 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:06,  6.57it/s]  7%|▋         | 3/45 [00:00<00:09,  4.54it/s]  9%|▉         | 4/45 [00:00<00:10,  4.06it/s] 11%|█         | 5/45 [00:01<00:10,  3.80it/s] 13%|█▎        | 6/45 [00:01<00:11,  3.54it/s] 16%|█▌        | 7/45 [00:01<00:11,  3.40it/s] 18%|█▊        | 8/45 [00:02<00:10,  3.40it/s] 20%|██        | 9/45 [00:02<00:10,  3.41it/s] 22%|██▏       | 10/45 [00:02<00:10,  3.38it/s] 24%|██▍       | 11/45 [00:03<00:09,  3.42it/s] 27%|██▋       | 12/45 [00:03<00:09,  3.41it/s] 29%|██▉       | 13/45 [00:03<00:09,  3.41it/s] 31%|███       | 14/45 [00:03<00:09,  3.44it/s] 33%|███▎      | 15/45 [00:04<00:08,  3.44it/s] 36%|███▌      | 16/45 [00:04<00:08,  3.32it/s] 38%|███▊      | 17/45 [00:04<00:08,  3.34it/s] 40%|████      | 18/45 [00:05<00:08,  3.33it/s] 42%|████▏     | 19/45 [00:05<00:07,  3.39it/s] 44%|████▍     | 20/45 [00:05<00:07,  3.36it/s] 47%|████▋     | 21/45 [00:06<00:07,  3.30it/s] 49%|████▉     | 22/45 [00:06<00:06,  3.31it/s] 51%|█████     | 23/45 [00:06<00:06,  3.35it/s] 53%|█████▎    | 24/45 [00:06<00:06,  3.35it/s] 56%|█████▌    | 25/45 [00:07<00:06,  3.32it/s] 58%|█████▊    | 26/45 [00:07<00:05,  3.35it/s] 60%|██████    | 27/45 [00:07<00:05,  3.38it/s] 62%|██████▏   | 28/45 [00:08<00:05,  3.31it/s] 64%|██████▍   | 29/45 [00:08<00:04,  3.25it/s] 67%|██████▋   | 30/45 [00:08<00:04,  3.21it/s] 69%|██████▉   | 31/45 [00:09<00:04,  3.31it/s] 71%|███████   | 32/45 [00:09<00:03,  3.36it/s] 73%|███████▎  | 33/45 [00:09<00:03,  3.33it/s] 76%|███████▌  | 34/45 [00:09<00:03,  3.39it/s] 78%|███████▊  | 35/45 [00:10<00:02,  3.37it/s] 80%|████████  | 36/45 [00:10<00:02,  3.37it/s] 82%|████████▏ | 37/45 [00:10<00:02,  3.38it/s] 84%|████████▍ | 38/45 [00:11<00:02,  3.36it/s] 87%|████████▋ | 39/45 [00:11<00:01,  3.33it/s] 89%|████████▉ | 40/45 [00:11<00:01,  3.36it/s] 91%|█████████ | 41/45 [00:11<00:01,  3.42it/s] 93%|█████████▎| 42/45 [00:12<00:00,  3.44it/s] 96%|█████████▌| 43/45 [00:12<00:00,  3.34it/s] 98%|█████████▊| 44/45 [00:12<00:00,  3.36it/s]100%|██████████| 45/45 [00:13<00:00,  3.31it/s]100%|██████████| 45/45 [00:13<00:00,  3.41it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0679
  eval_runtime            = 0:00:13.47
  eval_samples_per_second =      6.677
  eval_steps_per_second   =      3.338
[INFO|modelcard.py:449] 2024-11-18 23:32:29,089 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:32:50,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:32:54 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23937
[2024-11-18 23:32:56,678] torch.distributed.run: [WARNING] 
[2024-11-18 23:32:56,678] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 23:32:56,678] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 23:32:56,678] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:33:03,806] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 23:33:04,183] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:33:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:33:05 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 23:33:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:33:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 23:33:05,435 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:33:05,437 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:33:05,438 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,443 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,443 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,443 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,443 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,443 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:33:05,994 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 23:33:05,995 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:33:05,996 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:33:05,997 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,999 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,999 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,999 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,999 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:33:05,999 >> loading file tokenizer.json
11/18/2024 23:33:06 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:33:06 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:33:06,543 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 23:33:06 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:33:06 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
11/18/2024 23:33:06 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 370.85 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2853.74 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 23:33:11 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:02<00:35, 23.74 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:04<00:28, 27.50 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:05<00:24, 29.15 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:07<00:22, 29.79 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:09<00:20, 30.35 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:11<00:18, 30.83 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:13<00:16, 30.75 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:14<00:14, 30.76 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:16<00:12, 31.16 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:18<00:10, 31.18 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:20<00:08, 31.12 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:22<00:07, 31.18 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:23<00:05, 31.24 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:25<00:03, 31.15 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:27<00:01, 31.20 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 31.07 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 30.46 examples/s]
training example:
input_ids:
[151331, 151333, 151336, 198, 113947, 102811, 98314, 17327, 98870, 101223, 28213, 4913, 16881, 788, 4383, 103889, 100173, 98323, 5122, 3586, 2124, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 20987, 1055, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 24963, 2203, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 2203, 497, 330, 103889, 100173, 98323, 5122, 20987, 1055, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 6182, 497, 330, 103889, 100173, 98323, 5122, 24963, 6182, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 6182, 497, 330, 103889, 100173, 98323, 5122, 54596, 266, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 57409, 497, 330, 103889, 100173, 98323, 5122, 1778, 266, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 57409, 497, 330, 103889, 100173, 98323, 5122, 4648, 9593, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 2593, 39307, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 32252, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 4648, 32252, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 31893, 82, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 8984, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 8461, 11, 103204, 100173, 98323, 25, 57409, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 39248, 258, 8984, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 285, 4480, 1055, 11, 103204, 100173, 98323, 25, 2007, 11, 109162, 100173, 98323, 25, 2007, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 22526, 11, 103204, 100173, 98323, 25, 22526, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 2203, 11, 103204, 100173, 98323, 25, 2203, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 559, 351, 6182, 11, 103204, 100173, 98323, 25, 6182, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 4648, 12718, 11, 103204, 100173, 98323, 25, 8984, 11, 109162, 100173, 98323, 25, 4578, 497, 330, 103889, 100173, 98323, 5122, 71, 21702, 11, 103204, 100173, 98323, 25, 4578, 11, 109162, 100173, 98323, 25, 4578, 1040, 497, 330, 103889, 100173, 98323, 5122, 1038, 392, 1040, 1055, 11, 103204, 100173, 98323, 25, 4578, 1040, 11, 109162, 100173, 98323, 25, 4578, 1040, 7914, 330, 19969, 788, 4383, 104104, 100173, 25, 6182, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 2007, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11578, 330, 104104, 100173, 25, 8984, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 2332, 516, 364, 12961, 516, 364, 50927, 516, 364, 11523, 516, 364, 29099, 516, 364, 28325, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 57409, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11578, 330, 104104, 100173, 25, 22526, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 2102, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 4578, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11578, 330, 104104, 100173, 25, 2203, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11523, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22409, 2591, 516, 364, 37181, 1028, 4089, 11578, 330, 104104, 100173, 25, 4578, 1040, 11, 101934, 103304, 98870, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 91239, 113947, 102811, 98322, 99295, 99977, 99080, 98870, 510, 1502, 98323, 22526, 11, 103304, 2102, 1131, 2808, 369, 13318, 2763, 321, 51558, 1400, 4487, 13842, 2039, 148486, 304, 23892, 19226, 6, 98314, 104104, 198, 1502, 98323, 22526, 11, 103304, 2102, 1131, 2808, 369, 19832, 2039, 7443, 288, 304, 22384, 77357, 89, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 38, 383, 1775, 383, 12296, 2145, 62, 30055, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 38, 94194, 1668, 261, 1389, 62, 30055, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 74054, 21982, 1557, 89075, 25891, 7660, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 9495, 1775, 2763, 321, 51558, 1400, 4487, 13842, 2039, 148486, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 3608, 1139, 2032, 62, 84349, 20825, 6, 98314, 104104, 271, 98964, 98493, 99977, 99369, 100132, 102696, 98323, 109916, 102811, 98314, 56333, 27985, 102961, 510, 108820, 98346, 38744, 1400, 81, 1776, 292, 2039, 36927, 105306, 106422, 98314, 108384, 5373, 105358, 5373, 100132, 98327, 117260, 3837, 98512, 104559, 104172, 98314, 3144, 98327, 100695, 3837, 99312, 99067, 104172, 100695, 111279, 151337, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
inputs:
[gMASK] <sop> <|user|> 
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序 <|assistant|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
label_ids:
[151329, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
labels:
<|endoftext|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
[INFO|configuration_utils.py:673] 2024-11-18 23:33:43,699 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:33:43,701 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:33:43,702 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|modeling_utils.py:3729] 2024-11-18 23:33:43,763 >> loading weights file /home/work/liuytest/demo/glm-4-9b-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 23:33:43,763 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 23:33:43,766 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:06,  1.44it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.60it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.62it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.63it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.59it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:06,  1.50it/s]Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.59it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.59it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.59it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.57it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:05<00:01,  1.61it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.58it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.60it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.59it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.68it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 23:33:50,056 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:4582] 2024-11-18 23:33:50,057 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/work/liuytest/demo/glm-4-9b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 23:33:50,062 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 23:33:50,063 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "max_length": 128000,
  "pad_token_id": 151329,
  "temperature": 0.8,
  "top_p": 0.8
}

11/18/2024 23:33:50 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:33:50 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:33:50 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:33:50 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:33:50 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense_h_to_4h,dense_4h_to_h,query_key_value,dense
Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.59it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.62it/s]11/18/2024 23:33:50 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 23:33:50,878 >> Using auto half precision backend
Loading checkpoint shards:  80%|████████  | 8/10 [00:04<00:01,  1.65it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.64it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s]
11/18/2024 23:33:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:33:52 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:33:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:33:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:33:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense_4h_to_h,dense,query_key_value,dense_h_to_4h
11/18/2024 23:33:53 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
[INFO|trainer.py:2243] 2024-11-18 23:33:53,713 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 23:33:53,713 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 23:33:53,713 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 23:33:53,713 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 23:33:53,713 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 23:33:53,714 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 23:33:53,714 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 23:33:53,719 >>   Number of trainable parameters = 21,176,320
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:09<23:29,  9.46s/it]  1%|▏         | 2/150 [00:18<23:01,  9.33s/it]  2%|▏         | 3/150 [00:27<22:26,  9.16s/it]  3%|▎         | 4/150 [00:36<22:20,  9.18s/it]  3%|▎         | 5/150 [00:45<21:55,  9.07s/it]  4%|▍         | 6/150 [00:54<21:45,  9.07s/it]  5%|▍         | 7/150 [01:03<21:39,  9.09s/it]  5%|▌         | 8/150 [01:12<21:19,  9.01s/it]  6%|▌         | 9/150 [01:21<21:10,  9.01s/it]  7%|▋         | 10/150 [01:30<20:56,  8.97s/it]                                                {'loss': 1.4905, 'grad_norm': 3.6310970783233643, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [01:30<20:56,  8.97s/it]  7%|▋         | 11/150 [01:39<20:54,  9.02s/it]  8%|▊         | 12/150 [01:48<20:46,  9.03s/it]  9%|▊         | 13/150 [01:58<20:44,  9.08s/it]  9%|▉         | 14/150 [02:07<20:41,  9.13s/it] 10%|█         | 15/150 [02:16<20:18,  9.03s/it] 11%|█         | 16/150 [02:25<20:17,  9.09s/it] 11%|█▏        | 17/150 [02:34<19:56,  9.00s/it] 12%|█▏        | 18/150 [02:43<19:47,  9.00s/it] 13%|█▎        | 19/150 [02:52<19:34,  8.97s/it] 13%|█▎        | 20/150 [03:01<19:37,  9.06s/it]                                                {'loss': 0.2814, 'grad_norm': 2.4875433444976807, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [03:01<19:37,  9.06s/it] 14%|█▍        | 21/150 [03:10<19:32,  9.09s/it] 15%|█▍        | 22/150 [03:19<19:10,  8.99s/it] 15%|█▌        | 23/150 [03:28<19:06,  9.03s/it] 16%|█▌        | 24/150 [03:37<18:50,  8.97s/it] 17%|█▋        | 25/150 [03:46<18:43,  8.99s/it] 17%|█▋        | 26/150 [03:55<18:36,  9.01s/it] 18%|█▊        | 27/150 [04:04<18:25,  8.99s/it] 19%|█▊        | 28/150 [04:13<18:16,  8.99s/it] 19%|█▉        | 29/150 [04:22<18:04,  8.96s/it] 20%|██        | 30/150 [04:31<18:01,  9.02s/it]                                                {'loss': 0.2106, 'grad_norm': 1.5739929676055908, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [04:31<18:01,  9.02s/it] 21%|██        | 31/150 [04:40<17:45,  8.95s/it] 21%|██▏       | 32/150 [04:49<17:43,  9.02s/it] 22%|██▏       | 33/150 [04:58<17:41,  9.07s/it] 23%|██▎       | 34/150 [05:07<17:32,  9.08s/it] 23%|██▎       | 35/150 [05:16<17:28,  9.11s/it] 24%|██▍       | 36/150 [05:25<17:13,  9.07s/it] 25%|██▍       | 37/150 [05:34<17:02,  9.05s/it] 25%|██▌       | 38/150 [05:43<16:56,  9.08s/it] 26%|██▌       | 39/150 [05:53<16:52,  9.12s/it] 27%|██▋       | 40/150 [06:01<16:37,  9.06s/it]                                                {'loss': 0.1771, 'grad_norm': 1.4169811010360718, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [06:01<16:37,  9.06s/it] 27%|██▋       | 41/150 [06:10<16:18,  8.98s/it] 28%|██▊       | 42/150 [06:19<16:14,  9.02s/it] 29%|██▊       | 43/150 [06:28<16:06,  9.04s/it] 29%|██▉       | 44/150 [06:37<15:57,  9.04s/it] 30%|███       | 45/150 [06:46<15:49,  9.04s/it] 31%|███       | 46/150 [06:56<15:46,  9.10s/it] 31%|███▏      | 47/150 [07:05<15:33,  9.07s/it] 32%|███▏      | 48/150 [07:14<15:23,  9.05s/it] 33%|███▎      | 49/150 [07:23<15:12,  9.04s/it] 33%|███▎      | 50/150 [07:32<15:00,  9.00s/it]                                                {'loss': 0.1587, 'grad_norm': 1.0261136293411255, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [07:32<15:00,  9.00s/it] 34%|███▍      | 51/150 [07:41<14:56,  9.05s/it] 35%|███▍      | 52/150 [07:50<14:45,  9.03s/it] 35%|███▌      | 53/150 [07:59<14:39,  9.07s/it] 36%|███▌      | 54/150 [08:08<14:35,  9.12s/it] 37%|███▋      | 55/150 [08:17<14:22,  9.08s/it] 37%|███▋      | 56/150 [08:26<14:13,  9.08s/it] 38%|███▊      | 57/150 [08:35<14:02,  9.06s/it] 39%|███▊      | 58/150 [08:45<13:57,  9.11s/it] 39%|███▉      | 59/150 [08:54<13:53,  9.16s/it] 40%|████      | 60/150 [09:03<13:37,  9.08s/it]                                                {'loss': 0.126, 'grad_norm': 0.6709315180778503, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [09:03<13:37,  9.08s/it] 41%|████      | 61/150 [09:12<13:31,  9.11s/it] 41%|████▏     | 62/150 [09:21<13:13,  9.02s/it] 42%|████▏     | 63/150 [09:30<13:07,  9.05s/it] 43%|████▎     | 64/150 [09:39<12:58,  9.05s/it] 43%|████▎     | 65/150 [09:48<12:47,  9.02s/it] 44%|████▍     | 66/150 [09:57<12:35,  9.00s/it] 45%|████▍     | 67/150 [10:06<12:27,  9.01s/it] 45%|████▌     | 68/150 [10:15<12:16,  8.99s/it] 46%|████▌     | 69/150 [10:24<12:05,  8.96s/it] 47%|████▋     | 70/150 [10:33<11:59,  8.99s/it]                                                {'loss': 0.1251, 'grad_norm': 1.2762597799301147, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [10:33<11:59,  8.99s/it] 47%|████▋     | 71/150 [10:42<11:49,  8.98s/it] 48%|████▊     | 72/150 [10:51<11:44,  9.03s/it] 49%|████▊     | 73/150 [11:00<11:35,  9.04s/it] 49%|████▉     | 74/150 [11:09<11:27,  9.05s/it] 50%|█████     | 75/150 [11:18<11:17,  9.03s/it] 51%|█████     | 76/150 [11:27<11:01,  8.94s/it] 51%|█████▏    | 77/150 [11:36<10:53,  8.95s/it] 52%|█████▏    | 78/150 [11:45<10:52,  9.07s/it] 53%|█████▎    | 79/150 [11:54<10:39,  9.00s/it] 53%|█████▎    | 80/150 [12:03<10:35,  9.08s/it]                                                {'loss': 0.1123, 'grad_norm': 0.6650109887123108, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [12:03<10:35,  9.08s/it] 54%|█████▍    | 81/150 [12:12<10:20,  9.00s/it] 55%|█████▍    | 82/150 [12:21<10:13,  9.02s/it] 55%|█████▌    | 83/150 [12:30<10:02,  9.00s/it] 56%|█████▌    | 84/150 [12:39<09:58,  9.07s/it] 57%|█████▋    | 85/150 [12:48<09:45,  9.01s/it] 57%|█████▋    | 86/150 [12:57<09:33,  8.95s/it] 58%|█████▊    | 87/150 [13:06<09:26,  8.99s/it] 59%|█████▊    | 88/150 [13:15<09:16,  8.98s/it] 59%|█████▉    | 89/150 [13:24<09:07,  8.97s/it] 60%|██████    | 90/150 [13:33<09:01,  9.02s/it]                                                {'loss': 0.1126, 'grad_norm': 0.7020874619483948, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [13:33<09:01,  9.02s/it] 61%|██████    | 91/150 [13:42<08:52,  9.03s/it] 61%|██████▏   | 92/150 [13:51<08:41,  8.99s/it] 62%|██████▏   | 93/150 [14:00<08:35,  9.04s/it] 63%|██████▎   | 94/150 [14:09<08:28,  9.07s/it] 63%|██████▎   | 95/150 [14:18<08:17,  9.05s/it] 64%|██████▍   | 96/150 [14:27<08:09,  9.07s/it] 65%|██████▍   | 97/150 [14:36<07:58,  9.03s/it] 65%|██████▌   | 98/150 [14:45<07:52,  9.08s/it] 66%|██████▌   | 99/150 [14:55<07:47,  9.18s/it] 67%|██████▋   | 100/150 [15:04<07:34,  9.09s/it]                                                 {'loss': 0.1206, 'grad_norm': 0.5678475499153137, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [15:04<07:34,  9.09s/it] 67%|██████▋   | 101/150 [15:13<07:28,  9.15s/it] 68%|██████▊   | 102/150 [15:22<07:18,  9.13s/it] 69%|██████▊   | 103/150 [15:31<07:07,  9.10s/it] 69%|██████▉   | 104/150 [15:40<07:01,  9.15s/it] 70%|███████   | 105/150 [15:49<06:48,  9.08s/it] 71%|███████   | 106/150 [15:58<06:40,  9.10s/it] 71%|███████▏  | 107/150 [16:07<06:27,  9.00s/it] 72%|███████▏  | 108/150 [16:16<06:19,  9.03s/it] 73%|███████▎  | 109/150 [16:25<06:10,  9.04s/it] 73%|███████▎  | 110/150 [16:35<06:02,  9.07s/it]                                                 {'loss': 0.0936, 'grad_norm': 0.457126259803772, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [16:35<06:02,  9.07s/it] 74%|███████▍  | 111/150 [16:44<05:55,  9.12s/it] 75%|███████▍  | 112/150 [16:53<05:43,  9.04s/it] 75%|███████▌  | 113/150 [17:02<05:36,  9.10s/it] 76%|███████▌  | 114/150 [17:11<05:26,  9.08s/it] 77%|███████▋  | 115/150 [17:20<05:18,  9.09s/it] 77%|███████▋  | 116/150 [17:29<05:04,  8.97s/it] 78%|███████▊  | 117/150 [17:38<04:58,  9.04s/it] 79%|███████▊  | 118/150 [17:47<04:49,  9.05s/it] 79%|███████▉  | 119/150 [17:56<04:39,  9.03s/it] 80%|████████  | 120/150 [18:05<04:31,  9.06s/it]                                                 {'loss': 0.1171, 'grad_norm': 0.7358578443527222, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [18:05<04:31,  9.06s/it] 81%|████████  | 121/150 [18:14<04:22,  9.06s/it] 81%|████████▏ | 122/150 [18:23<04:14,  9.09s/it] 82%|████████▏ | 123/150 [18:32<04:05,  9.08s/it] 83%|████████▎ | 124/150 [18:41<03:54,  9.02s/it] 83%|████████▎ | 125/150 [18:50<03:45,  9.02s/it] 84%|████████▍ | 126/150 [18:59<03:35,  8.98s/it] 85%|████████▍ | 127/150 [19:08<03:28,  9.06s/it] 85%|████████▌ | 128/150 [19:17<03:18,  9.02s/it] 86%|████████▌ | 129/150 [19:27<03:11,  9.11s/it] 87%|████████▋ | 130/150 [19:36<03:01,  9.07s/it]                                                 {'loss': 0.0799, 'grad_norm': 0.6445026993751526, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [19:36<03:01,  9.07s/it] 87%|████████▋ | 131/150 [19:45<02:51,  9.05s/it] 88%|████████▊ | 132/150 [19:54<02:43,  9.10s/it] 89%|████████▊ | 133/150 [20:02<02:32,  8.97s/it] 89%|████████▉ | 134/150 [20:12<02:24,  9.02s/it] 90%|█████████ | 135/150 [20:21<02:15,  9.02s/it] 91%|█████████ | 136/150 [20:30<02:06,  9.03s/it] 91%|█████████▏| 137/150 [20:39<01:57,  9.01s/it] 92%|█████████▏| 138/150 [20:48<01:48,  9.06s/it] 93%|█████████▎| 139/150 [20:57<01:39,  9.08s/it] 93%|█████████▎| 140/150 [21:06<01:30,  9.04s/it]                                                 {'loss': 0.0812, 'grad_norm': 0.8352688550949097, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [21:06<01:30,  9.04s/it] 94%|█████████▍| 141/150 [21:15<01:21,  9.06s/it] 95%|█████████▍| 142/150 [21:24<01:12,  9.01s/it] 95%|█████████▌| 143/150 [21:33<01:03,  9.08s/it] 96%|█████████▌| 144/150 [21:42<00:54,  9.11s/it] 97%|█████████▋| 145/150 [21:51<00:45,  9.06s/it] 97%|█████████▋| 146/150 [22:01<00:36,  9.14s/it] 98%|█████████▊| 147/150 [22:10<00:27,  9.07s/it] 99%|█████████▊| 148/150 [22:19<00:18,  9.09s/it] 99%|█████████▉| 149/150 [22:28<00:09,  9.05s/it]100%|██████████| 150/150 [22:37<00:00,  9.03s/it]                                                 {'loss': 0.1053, 'grad_norm': 0.648970901966095, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [22:37<00:00,  9.03s/it][INFO|trainer.py:3705] 2024-11-18 23:56:31,756 >> Saving model checkpoint to saves/glm-4-9b-chat/my_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 23:56:31,791 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:56:31,792 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:56:31,975 >> tokenizer config file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:56:31,975 >> Special tokens file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:56:31,975 >> added tokens file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/checkpoint-150/added_tokens.json
[INFO|trainer.py:2505] 2024-11-18 23:56:32,407 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1358.6882, 'train_samples_per_second': 1.78, 'train_steps_per_second': 0.11, 'train_loss': 0.22614225069681804, 'epoch': 2.98}
100%|██████████| 150/150 [22:37<00:00,  9.03s/it]100%|██████████| 150/150 [22:37<00:00,  9.05s/it]
[INFO|trainer.py:3705] 2024-11-18 23:56:32,410 >> Saving model checkpoint to saves/glm-4-9b-chat/my_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 23:56:32,441 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:56:32,442 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:56:32,624 >> tokenizer config file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:56:32,624 >> Special tokens file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-18 23:56:32,624 >> added tokens file saved in saves/glm-4-9b-chat/my_prompt/lora/sft/added_tokens.json
***** train metrics *****
  epoch                    =      2.9777
  total_flos               = 114785607GF
  train_loss               =      0.2261
  train_runtime            =  0:22:38.68
  train_samples_per_second =        1.78
  train_steps_per_second   =        0.11
Figure saved at: saves/glm-4-9b-chat/my_prompt/lora/sft/training_loss.png
11/18/2024 23:56:32 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 23:56:32 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 23:56:32,812 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 23:56:32,812 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 23:56:32,812 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:07,  5.77it/s]  7%|▋         | 3/45 [00:00<00:10,  4.04it/s]  9%|▉         | 4/45 [00:01<00:11,  3.54it/s] 11%|█         | 5/45 [00:01<00:12,  3.31it/s] 13%|█▎        | 6/45 [00:01<00:12,  3.15it/s] 16%|█▌        | 7/45 [00:02<00:12,  3.05it/s] 18%|█▊        | 8/45 [00:02<00:12,  3.02it/s] 20%|██        | 9/45 [00:02<00:11,  3.04it/s] 22%|██▏       | 10/45 [00:03<00:11,  2.99it/s] 24%|██▍       | 11/45 [00:03<00:11,  3.01it/s] 27%|██▋       | 12/45 [00:03<00:11,  2.96it/s] 29%|██▉       | 13/45 [00:04<00:10,  2.96it/s] 31%|███       | 14/45 [00:04<00:10,  3.01it/s] 33%|███▎      | 15/45 [00:04<00:09,  3.05it/s] 36%|███▌      | 16/45 [00:05<00:09,  2.99it/s] 38%|███▊      | 17/45 [00:05<00:09,  2.95it/s] 40%|████      | 18/45 [00:05<00:09,  2.92it/s] 42%|████▏     | 19/45 [00:06<00:08,  2.90it/s] 44%|████▍     | 20/45 [00:06<00:08,  2.93it/s] 47%|████▋     | 21/45 [00:06<00:08,  2.90it/s] 49%|████▉     | 22/45 [00:07<00:07,  2.88it/s] 51%|█████     | 23/45 [00:07<00:07,  2.89it/s] 53%|█████▎    | 24/45 [00:07<00:07,  2.89it/s] 56%|█████▌    | 25/45 [00:08<00:06,  2.88it/s] 58%|█████▊    | 26/45 [00:08<00:06,  2.88it/s] 60%|██████    | 27/45 [00:08<00:06,  2.87it/s] 62%|██████▏   | 28/45 [00:09<00:05,  2.89it/s] 64%|██████▍   | 29/45 [00:09<00:05,  2.89it/s] 67%|██████▋   | 30/45 [00:09<00:05,  2.90it/s] 69%|██████▉   | 31/45 [00:10<00:04,  2.99it/s] 71%|███████   | 32/45 [00:10<00:04,  2.96it/s] 73%|███████▎  | 33/45 [00:10<00:04,  2.94it/s] 76%|███████▌  | 34/45 [00:11<00:03,  3.02it/s] 78%|███████▊  | 35/45 [00:11<00:03,  3.01it/s] 80%|████████  | 36/45 [00:11<00:02,  3.00it/s] 82%|████████▏ | 37/45 [00:12<00:02,  2.96it/s] 84%|████████▍ | 38/45 [00:12<00:02,  2.94it/s] 87%|████████▋ | 39/45 [00:12<00:02,  2.94it/s] 89%|████████▉ | 40/45 [00:13<00:01,  2.98it/s] 91%|█████████ | 41/45 [00:13<00:01,  3.01it/s] 93%|█████████▎| 42/45 [00:13<00:01,  2.97it/s] 96%|█████████▌| 43/45 [00:14<00:00,  2.95it/s] 98%|█████████▊| 44/45 [00:14<00:00,  2.99it/s]100%|██████████| 45/45 [00:14<00:00,  2.97it/s]100%|██████████| 45/45 [00:14<00:00,  3.01it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1328
  eval_runtime            = 0:00:15.30
  eval_samples_per_second =      5.879
  eval_steps_per_second   =      2.939
[INFO|modelcard.py:449] 2024-11-18 23:56:48,122 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:57:09,228] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:57:13 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23454
[2024-11-18 23:57:15,111] torch.distributed.run: [WARNING] 
[2024-11-18 23:57:15,111] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 23:57:15,111] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 23:57:15,111] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 23:57:22,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 23:57:22,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 23:57:23 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:57:23 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 23:57:23,895 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:57:23,897 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:57:23,898 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:23,903 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:23,904 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:23,904 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:23,904 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:23,904 >> loading file tokenizer.json
11/18/2024 23:57:23 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 23:57:23 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:57:24,470 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 23:57:24,471 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:57:24,472 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:57:24,473 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:24,475 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:24,475 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:24,475 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:24,475 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 23:57:24,475 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 23:57:25,041 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 23:57:25 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:57:25 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
11/18/2024 23:57:25 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
11/18/2024 23:57:25 - WARNING - llamafactory.model.loader - Processor was not found: 'ChatGLMConfig' object has no attribute 'vision_config'.
11/18/2024 23:57:25 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 419.99 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2766.96 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 23:57:29 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:02<00:34, 24.45 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:04<00:29, 26.97 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:06<00:25, 28.63 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:07<00:23, 29.10 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:09<00:20, 29.74 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:11<00:18, 29.97 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:13<00:16, 29.84 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:15<00:14, 30.27 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:17<00:12, 30.26 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:18<00:11, 30.46 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:20<00:09, 30.73 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:22<00:07, 30.74 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:24<00:05, 30.89 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:26<00:03, 30.97 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:27<00:01, 31.16 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 31.09 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:29<00:00, 30.08 examples/s]
training example:
input_ids:
[151331, 151333, 151336, 198, 113947, 102811, 98322, 99295, 99977, 99080, 98870, 510, 1502, 98323, 22526, 11, 103304, 2102, 1131, 2808, 369, 13318, 2763, 321, 51558, 1400, 4487, 13842, 2039, 148486, 304, 23892, 19226, 6, 98314, 104104, 198, 1502, 98323, 22526, 11, 103304, 2102, 1131, 2808, 369, 19832, 2039, 7443, 288, 304, 22384, 77357, 89, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 38, 383, 1775, 383, 12296, 2145, 62, 30055, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 38, 94194, 1668, 261, 1389, 62, 30055, 6, 98314, 104104, 198, 1502, 98323, 57409, 11, 103304, 606, 1131, 74054, 21982, 1557, 89075, 25891, 7660, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 9495, 1775, 2763, 321, 51558, 1400, 4487, 13842, 2039, 148486, 6, 98314, 104104, 198, 1502, 98323, 4578, 11, 103304, 606, 1131, 3608, 1139, 2032, 62, 84349, 20825, 6, 98314, 104104, 271, 98964, 98493, 99977, 99369, 100132, 102696, 98323, 109916, 102811, 98314, 56333, 27985, 102961, 510, 108820, 98346, 38744, 1400, 81, 1776, 292, 2039, 36927, 105306, 106422, 98314, 108384, 5373, 105358, 5373, 100132, 98327, 117260, 3837, 98512, 104559, 104172, 98314, 3144, 98327, 100695, 3837, 99312, 99067, 104172, 100695, 111279, 151337, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
inputs:
[gMASK] <sop> <|user|> 
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序 <|assistant|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
label_ids:
[151329, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 6347, 320, 77, 16, 25, 8984, 7287, 58, 81, 16, 25, 4648, 12718, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38744, 1400, 81, 1776, 292, 2039, 36927, 6, 470, 308, 17, 7315, 11, 308, 16, 9842, 11, 308, 17, 2644, 11, 308, 16, 72080, 11, 308, 16, 48034, 11, 308, 16, 31500, 1973, 553, 308, 17, 2644, 26, 151329]
labels:
<|endoftext|> 
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name; <|endoftext|>
[INFO|configuration_utils.py:673] 2024-11-18 23:58:02,024 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:673] 2024-11-18 23:58:02,025 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:58:02,027 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/work/liuytest/demo/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|modeling_utils.py:3729] 2024-11-18 23:58:02,091 >> loading weights file /home/work/liuytest/demo/glm-4-9b-chat/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 23:58:02,092 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 23:58:02,094 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:06,  1.44it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.58it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.60it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.61it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.62it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:00<00:05,  1.52it/s]Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.61it/s]Loading checkpoint shards:  20%|██        | 2/10 [00:01<00:05,  1.58it/s]Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.60it/s]Loading checkpoint shards:  30%|███       | 3/10 [00:01<00:04,  1.57it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:04<00:01,  1.61it/s]Loading checkpoint shards:  40%|████      | 4/10 [00:02<00:03,  1.57it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.60it/s]Loading checkpoint shards:  50%|█████     | 5/10 [00:03<00:03,  1.60it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 23:58:08,371 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:4582] 2024-11-18 23:58:08,372 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/work/liuytest/demo/glm-4-9b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 23:58:08,378 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 23:58:08,378 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "max_length": 128000,
  "pad_token_id": 151329,
  "temperature": 0.8,
  "top_p": 0.8
}

11/18/2024 23:58:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:58:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:58:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:58:08 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:58:08 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense_4h_to_h,query_key_value,dense_h_to_4h,dense
Loading checkpoint shards:  60%|██████    | 6/10 [00:03<00:02,  1.59it/s]11/18/2024 23:58:09 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 23:58:09,083 >> Using auto half precision backend
Loading checkpoint shards:  70%|███████   | 7/10 [00:04<00:01,  1.57it/s]Loading checkpoint shards:  80%|████████  | 8/10 [00:05<00:01,  1.58it/s]Loading checkpoint shards:  90%|█████████ | 9/10 [00:05<00:00,  1.57it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]
11/18/2024 23:58:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 23:58:11 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 23:58:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 23:58:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 23:58:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense,query_key_value,dense_4h_to_h,dense_h_to_4h
11/18/2024 23:58:11 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
[INFO|trainer.py:2243] 2024-11-18 23:58:12,192 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 23:58:12,193 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 23:58:12,193 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 23:58:12,193 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 23:58:12,193 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 23:58:12,193 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 23:58:12,193 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 23:58:12,199 >>   Number of trainable parameters = 21,176,320
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:05<14:31,  5.85s/it]  1%|▏         | 2/150 [00:11<14:00,  5.68s/it]  2%|▏         | 3/150 [00:16<13:30,  5.51s/it]  3%|▎         | 4/150 [00:22<13:24,  5.51s/it]  3%|▎         | 5/150 [00:27<13:07,  5.43s/it]  4%|▍         | 6/150 [00:33<13:08,  5.48s/it]  5%|▍         | 7/150 [00:38<13:05,  5.49s/it]  5%|▌         | 8/150 [00:43<12:48,  5.41s/it]  6%|▌         | 9/150 [00:49<12:45,  5.43s/it]  7%|▋         | 10/150 [00:54<12:31,  5.36s/it]                                                {'loss': 1.6917, 'grad_norm': 5.200274467468262, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:54<12:31,  5.36s/it]  7%|▋         | 11/150 [01:00<13:06,  5.66s/it]  8%|▊         | 12/150 [01:06<12:50,  5.58s/it]  9%|▊         | 13/150 [01:11<12:41,  5.56s/it]  9%|▉         | 14/150 [01:17<12:34,  5.55s/it] 10%|█         | 15/150 [01:22<12:23,  5.50s/it] 11%|█         | 16/150 [01:28<12:12,  5.47s/it] 11%|█▏        | 17/150 [01:33<11:56,  5.39s/it] 12%|█▏        | 18/150 [01:38<11:52,  5.40s/it] 13%|█▎        | 19/150 [01:44<11:57,  5.48s/it] 13%|█▎        | 20/150 [01:50<12:08,  5.60s/it]                                                {'loss': 0.2844, 'grad_norm': 2.9265635013580322, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:50<12:08,  5.60s/it] 14%|█▍        | 21/150 [01:55<11:58,  5.57s/it] 15%|█▍        | 22/150 [02:01<12:15,  5.75s/it] 15%|█▌        | 23/150 [02:08<12:25,  5.87s/it] 16%|█▌        | 24/150 [02:13<12:03,  5.74s/it] 17%|█▋        | 25/150 [02:19<12:00,  5.76s/it] 17%|█▋        | 26/150 [02:24<11:50,  5.73s/it] 18%|█▊        | 27/150 [02:30<11:49,  5.77s/it] 19%|█▊        | 28/150 [02:37<12:02,  5.92s/it] 19%|█▉        | 29/150 [02:42<11:51,  5.88s/it] 20%|██        | 30/150 [02:48<11:32,  5.77s/it]                                                {'loss': 0.11, 'grad_norm': 1.361992597579956, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:48<11:32,  5.77s/it] 21%|██        | 31/150 [02:54<11:40,  5.88s/it] 21%|██▏       | 32/150 [03:00<11:33,  5.87s/it] 22%|██▏       | 33/150 [03:06<11:27,  5.88s/it] 23%|██▎       | 34/150 [03:11<11:07,  5.75s/it] 23%|██▎       | 35/150 [03:17<11:03,  5.77s/it] 24%|██▍       | 36/150 [03:22<10:45,  5.66s/it] 25%|██▍       | 37/150 [03:29<10:57,  5.82s/it] 25%|██▌       | 38/150 [03:34<10:46,  5.77s/it] 26%|██▌       | 39/150 [03:40<10:45,  5.81s/it] 27%|██▋       | 40/150 [03:46<10:42,  5.84s/it]                                                {'loss': 0.0823, 'grad_norm': 1.0429681539535522, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [03:46<10:42,  5.84s/it] 27%|██▋       | 41/150 [03:52<10:28,  5.77s/it] 28%|██▊       | 42/150 [03:58<10:29,  5.83s/it] 29%|██▊       | 43/150 [04:04<10:31,  5.90s/it] 29%|██▉       | 44/150 [04:10<10:35,  6.00s/it] 30%|███       | 45/150 [04:16<10:25,  5.96s/it] 31%|███       | 46/150 [04:22<10:18,  5.95s/it] 31%|███▏      | 47/150 [04:27<10:01,  5.84s/it] 32%|███▏      | 48/150 [04:34<10:08,  5.97s/it] 33%|███▎      | 49/150 [04:39<09:54,  5.89s/it] 33%|███▎      | 50/150 [04:45<09:50,  5.91s/it]                                                {'loss': 0.0729, 'grad_norm': 0.8029654622077942, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [04:45<09:50,  5.91s/it] 34%|███▍      | 51/150 [04:51<09:37,  5.83s/it] 35%|███▍      | 52/150 [04:56<09:19,  5.71s/it] 35%|███▌      | 53/150 [05:03<09:26,  5.84s/it] 36%|███▌      | 54/150 [05:08<09:19,  5.83s/it] 37%|███▋      | 55/150 [05:14<08:58,  5.67s/it] 37%|███▋      | 56/150 [05:19<08:53,  5.67s/it] 38%|███▊      | 57/150 [05:25<08:48,  5.68s/it] 39%|███▊      | 58/150 [05:32<09:12,  6.01s/it] 39%|███▉      | 59/150 [05:38<09:21,  6.17s/it] 40%|████      | 60/150 [05:43<08:47,  5.86s/it]                                                {'loss': 0.0511, 'grad_norm': 0.4461100101470947, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [05:43<08:47,  5.86s/it] 41%|████      | 61/150 [05:49<08:40,  5.85s/it] 41%|████▏     | 62/150 [05:55<08:37,  5.88s/it] 42%|████▏     | 63/150 [06:01<08:22,  5.77s/it] 43%|████▎     | 64/150 [06:06<08:11,  5.71s/it] 43%|████▎     | 65/150 [06:12<08:11,  5.78s/it] 44%|████▍     | 66/150 [06:18<08:10,  5.83s/it] 45%|████▍     | 67/150 [06:23<07:49,  5.65s/it] 45%|████▌     | 68/150 [06:30<07:54,  5.78s/it] 46%|████▌     | 69/150 [06:35<07:35,  5.62s/it] 47%|████▋     | 70/150 [06:41<07:33,  5.67s/it]                                                {'loss': 0.0443, 'grad_norm': 0.6234487295150757, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [06:41<07:33,  5.67s/it] 47%|████▋     | 71/150 [06:46<07:27,  5.66s/it] 48%|████▊     | 72/150 [06:52<07:25,  5.71s/it] 49%|████▊     | 73/150 [06:58<07:19,  5.71s/it] 49%|████▉     | 74/150 [07:03<07:11,  5.67s/it] 50%|█████     | 75/150 [07:09<07:08,  5.71s/it] 51%|█████     | 76/150 [07:15<07:07,  5.77s/it] 51%|█████▏    | 77/150 [07:21<06:59,  5.75s/it] 52%|█████▏    | 78/150 [07:26<06:48,  5.67s/it] 53%|█████▎    | 79/150 [07:31<06:33,  5.54s/it] 53%|█████▎    | 80/150 [07:37<06:34,  5.63s/it]                                                {'loss': 0.0508, 'grad_norm': 0.4743824601173401, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [07:37<06:34,  5.63s/it] 54%|█████▍    | 81/150 [07:43<06:34,  5.72s/it] 55%|█████▍    | 82/150 [07:49<06:24,  5.66s/it] 55%|█████▌    | 83/150 [07:54<06:10,  5.53s/it] 56%|█████▌    | 84/150 [07:59<06:04,  5.52s/it] 57%|█████▋    | 85/150 [08:06<06:10,  5.70s/it] 57%|█████▋    | 86/150 [08:12<06:12,  5.82s/it] 58%|█████▊    | 87/150 [08:18<06:20,  6.04s/it] 59%|█████▊    | 88/150 [08:24<06:00,  5.81s/it] 59%|█████▉    | 89/150 [08:30<05:57,  5.86s/it] 60%|██████    | 90/150 [08:35<05:47,  5.79s/it]                                                {'loss': 0.0528, 'grad_norm': 0.5783360600471497, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [08:35<05:47,  5.79s/it] 61%|██████    | 91/150 [08:41<05:42,  5.81s/it] 61%|██████▏   | 92/150 [08:47<05:35,  5.78s/it] 62%|██████▏   | 93/150 [08:52<05:28,  5.76s/it] 63%|██████▎   | 94/150 [08:59<05:28,  5.87s/it] 63%|██████▎   | 95/150 [09:04<05:16,  5.75s/it] 64%|██████▍   | 96/150 [09:10<05:15,  5.84s/it] 65%|██████▍   | 97/150 [09:16<05:18,  6.01s/it] 65%|██████▌   | 98/150 [09:22<05:04,  5.86s/it] 66%|██████▌   | 99/150 [09:28<05:00,  5.89s/it] 67%|██████▋   | 100/150 [09:33<04:45,  5.70s/it]                                                 {'loss': 0.0515, 'grad_norm': 0.5492401123046875, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [09:33<04:45,  5.70s/it] 67%|██████▋   | 101/150 [09:39<04:46,  5.85s/it] 68%|██████▊   | 102/150 [09:45<04:32,  5.68s/it] 69%|██████▊   | 103/150 [09:51<04:30,  5.75s/it] 69%|██████▉   | 104/150 [09:57<04:31,  5.90s/it] 70%|███████   | 105/150 [10:02<04:16,  5.70s/it] 71%|███████   | 106/150 [10:08<04:11,  5.71s/it] 71%|███████▏  | 107/150 [10:13<03:59,  5.57s/it] 72%|███████▏  | 108/150 [10:19<03:57,  5.67s/it] 73%|███████▎  | 109/150 [10:24<03:49,  5.59s/it] 73%|███████▎  | 110/150 [10:30<03:42,  5.57s/it]                                                 {'loss': 0.0383, 'grad_norm': 0.552619457244873, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [10:30<03:42,  5.57s/it] 74%|███████▍  | 111/150 [10:36<03:41,  5.69s/it] 75%|███████▍  | 112/150 [10:41<03:31,  5.56s/it] 75%|███████▌  | 113/150 [10:47<03:29,  5.67s/it] 76%|███████▌  | 114/150 [10:52<03:20,  5.57s/it] 77%|███████▋  | 115/150 [10:58<03:17,  5.65s/it] 77%|███████▋  | 116/150 [11:04<03:10,  5.59s/it] 78%|███████▊  | 117/150 [11:09<03:05,  5.63s/it] 79%|███████▊  | 118/150 [11:15<03:01,  5.68s/it] 79%|███████▉  | 119/150 [11:21<02:54,  5.62s/it] 80%|████████  | 120/150 [11:26<02:50,  5.67s/it]                                                 {'loss': 0.0394, 'grad_norm': 0.5389404296875, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [11:26<02:50,  5.67s/it] 81%|████████  | 121/150 [11:32<02:40,  5.53s/it] 81%|████████▏ | 122/150 [11:37<02:34,  5.52s/it] 82%|████████▏ | 123/150 [11:43<02:35,  5.75s/it] 83%|████████▎ | 124/150 [11:49<02:27,  5.66s/it] 83%|████████▎ | 125/150 [11:56<02:28,  5.95s/it] 84%|████████▍ | 126/150 [12:01<02:17,  5.72s/it] 85%|████████▍ | 127/150 [12:06<02:09,  5.64s/it] 85%|████████▌ | 128/150 [12:11<02:01,  5.53s/it] 86%|████████▌ | 129/150 [12:17<01:58,  5.62s/it] 87%|████████▋ | 130/150 [12:23<01:54,  5.75s/it]                                                 {'loss': 0.0271, 'grad_norm': 0.23865212500095367, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [12:23<01:54,  5.75s/it] 87%|████████▋ | 131/150 [12:29<01:48,  5.69s/it] 88%|████████▊ | 132/150 [12:34<01:41,  5.64s/it] 89%|████████▊ | 133/150 [12:40<01:35,  5.62s/it] 89%|████████▉ | 134/150 [12:46<01:30,  5.65s/it] 90%|█████████ | 135/150 [12:51<01:24,  5.60s/it] 91%|█████████ | 136/150 [12:57<01:19,  5.65s/it] 91%|█████████▏| 137/150 [13:03<01:13,  5.68s/it] 92%|█████████▏| 138/150 [13:09<01:10,  5.86s/it] 93%|█████████▎| 139/150 [13:15<01:04,  5.86s/it] 93%|█████████▎| 140/150 [13:20<00:57,  5.78s/it]                                                 {'loss': 0.0292, 'grad_norm': 0.6328083276748657, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [13:20<00:57,  5.78s/it] 94%|█████████▍| 141/150 [13:27<00:52,  5.88s/it] 95%|█████████▍| 142/150 [13:33<00:47,  5.92s/it] 95%|█████████▌| 143/150 [13:39<00:42,  6.01s/it] 96%|█████████▌| 144/150 [13:45<00:35,  5.99s/it] 97%|█████████▋| 145/150 [13:50<00:29,  5.84s/it] 97%|█████████▋| 146/150 [13:56<00:23,  5.86s/it] 98%|█████████▊| 147/150 [14:01<00:17,  5.71s/it] 99%|█████████▊| 148/150 [14:07<00:11,  5.81s/it] 99%|█████████▉| 149/150 [14:13<00:05,  5.87s/it]100%|██████████| 150/150 [14:19<00:00,  5.85s/it]                                                 {'loss': 0.0388, 'grad_norm': 0.2674480974674225, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [14:19<00:00,  5.85s/it][INFO|trainer.py:3705] 2024-11-19 00:12:32,930 >> Saving model checkpoint to saves/glm-4-9b-chat/vector_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:12:32,965 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:12:32,966 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:12:33,159 >> tokenizer config file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:12:33,159 >> Special tokens file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-19 00:12:33,159 >> added tokens file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/checkpoint-150/added_tokens.json
[INFO|trainer.py:2505] 2024-11-19 00:12:33,595 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 861.3961, 'train_samples_per_second': 2.807, 'train_steps_per_second': 0.174, 'train_loss': 0.17763918042182922, 'epoch': 2.98}
100%|██████████| 150/150 [14:20<00:00,  5.85s/it]100%|██████████| 150/150 [14:20<00:00,  5.74s/it]
[INFO|trainer.py:3705] 2024-11-19 00:12:33,598 >> Saving model checkpoint to saves/glm-4-9b-chat/vector_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:12:33,629 >> loading configuration file /home/work/liuytest/demo/glm-4-9b-chat/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:12:33,630 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:12:33,816 >> tokenizer config file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:12:33,816 >> Special tokens file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/special_tokens_map.json
[INFO|tokenization_utils_base.py:2701] 2024-11-19 00:12:33,817 >> added tokens file saved in saves/glm-4-9b-chat/vector_prompt/lora/sft/added_tokens.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 30983966GF
  train_loss               =     0.1776
  train_runtime            = 0:14:21.39
  train_samples_per_second =      2.807
  train_steps_per_second   =      0.174
Figure saved at: saves/glm-4-9b-chat/vector_prompt/lora/sft/training_loss.png
11/19/2024 00:12:34 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:12:34 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:12:34,006 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:12:34,006 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:12:34,006 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:03, 14.04it/s]  9%|▉         | 4/45 [00:00<00:04,  8.73it/s] 11%|█         | 5/45 [00:00<00:04,  8.41it/s] 13%|█▎        | 6/45 [00:00<00:05,  7.79it/s] 16%|█▌        | 7/45 [00:00<00:05,  7.47it/s] 18%|█▊        | 8/45 [00:00<00:04,  7.59it/s] 20%|██        | 9/45 [00:01<00:04,  7.79it/s] 22%|██▏       | 10/45 [00:01<00:04,  7.49it/s] 24%|██▍       | 11/45 [00:01<00:04,  7.60it/s] 27%|██▋       | 12/45 [00:01<00:04,  7.46it/s] 29%|██▉       | 13/45 [00:01<00:04,  7.59it/s] 31%|███       | 14/45 [00:01<00:03,  7.83it/s] 33%|███▎      | 15/45 [00:01<00:03,  7.99it/s] 36%|███▌      | 16/45 [00:02<00:03,  7.44it/s] 38%|███▊      | 17/45 [00:02<00:03,  7.26it/s] 40%|████      | 18/45 [00:02<00:03,  7.11it/s] 42%|████▏     | 19/45 [00:02<00:03,  6.80it/s] 44%|████▍     | 20/45 [00:02<00:03,  7.05it/s] 47%|████▋     | 21/45 [00:02<00:03,  7.06it/s] 49%|████▉     | 22/45 [00:02<00:03,  7.07it/s] 51%|█████     | 23/45 [00:03<00:03,  7.10it/s] 53%|█████▎    | 24/45 [00:03<00:03,  6.93it/s] 56%|█████▌    | 25/45 [00:03<00:02,  6.84it/s] 58%|█████▊    | 26/45 [00:03<00:02,  6.74it/s] 60%|██████    | 27/45 [00:03<00:02,  6.66it/s] 62%|██████▏   | 28/45 [00:03<00:02,  6.82it/s] 64%|██████▍   | 29/45 [00:03<00:02,  6.65it/s] 67%|██████▋   | 30/45 [00:04<00:02,  6.70it/s] 69%|██████▉   | 31/45 [00:04<00:01,  7.08it/s] 71%|███████   | 32/45 [00:04<00:01,  7.07it/s] 73%|███████▎  | 33/45 [00:04<00:01,  6.96it/s] 76%|███████▌  | 34/45 [00:04<00:01,  7.33it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.49it/s] 80%|████████  | 36/45 [00:04<00:01,  7.60it/s] 82%|████████▏ | 37/45 [00:05<00:01,  7.30it/s] 84%|████████▍ | 38/45 [00:05<00:00,  7.09it/s] 87%|████████▋ | 39/45 [00:05<00:00,  7.21it/s] 89%|████████▉ | 40/45 [00:05<00:00,  7.47it/s] 91%|█████████ | 41/45 [00:05<00:00,  7.70it/s] 93%|█████████▎| 42/45 [00:05<00:00,  7.41it/s] 96%|█████████▌| 43/45 [00:05<00:00,  7.25it/s] 98%|█████████▊| 44/45 [00:05<00:00,  7.49it/s]100%|██████████| 45/45 [00:06<00:00,  6.83it/s]100%|██████████| 45/45 [00:06<00:00,  7.31it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0655
  eval_runtime            = 0:00:06.29
  eval_samples_per_second =     14.286
  eval_steps_per_second   =      7.143
[INFO|modelcard.py:449] 2024-11-19 00:12:40,306 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:12:58,343] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:13:02 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:28050
[2024-11-19 00:13:04,177] torch.distributed.run: [WARNING] 
[2024-11-19 00:13:04,177] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:13:04,177] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:13:04,177] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:13:11,354] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:13:11,719] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:13:12 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:13:12 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:13:12,495 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:13:12,496 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:12,498 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:12,498 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:12,498 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:12,498 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:12,498 >> loading file tokenizer_config.json
11/19/2024 00:13:12 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:13:12 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:13:13,141 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:13:13,143 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:13:13,144 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:13,145 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:13,145 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:13,145 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:13,145 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:13:13,145 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:13:13,791 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:13:13 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:13:13 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:13:13 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
11/19/2024 00:13:13 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
11/19/2024 00:13:14 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:13:14 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:13:14 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 436.23 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3036.09 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:13:17 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:25, 33.27 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:11, 65.87 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:02<00:07, 95.70 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:02<00:05, 122.70 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:04, 142.88 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:03<00:03, 161.54 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:03<00:03, 135.87 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:04<00:02, 163.10 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:04<00:01, 183.65 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:04<00:00, 219.68 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:05<00:00, 208.02 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:05<00:00, 169.01 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:06<00:00, 184.34 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:06<00:00, 146.10 examples/s]
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 15225, 45163, 88852, 109683, 120074, 47770, 72234, 18184, 29129, 75493, 9554, 57815, 29182, 52254, 512, 33976, 93233, 33764, 40052, 1426, 81, 1814, 292, 2083, 38221, 99750, 106236, 110999, 104123, 43323, 9554, 94668, 5486, 34171, 64022, 5486, 120074, 34208, 116122, 105514, 91495, 32626, 31944, 62855, 9554, 3222, 34208, 31091, 3922, 60251, 60979, 31944, 62855, 31091, 75761, 128009, 128006, 78191, 128007, 271, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|eot_id|><|start_header_id|>assistant<|end_header_id|>

match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
[INFO|configuration_utils.py:673] 2024-11-19 00:13:24,828 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:13:24,829 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:13:24,880 >> loading weights file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:13:24,880 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:13:24,882 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.06it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.00it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
[INFO|modeling_utils.py:4574] 2024-11-19 00:13:28,514 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:13:28,514 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 00:13:28,519 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 00:13:28,519 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:13:28 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:13:28 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,o_proj,q_proj,up_proj,down_proj,k_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:13:28 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:13:28 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:13:28 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,q_proj,gate_proj,up_proj,o_proj,v_proj
11/19/2024 00:13:29 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:13:29,279 >> Using auto half precision backend
11/19/2024 00:13:29 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
[INFO|trainer.py:2243] 2024-11-19 00:13:29,926 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:13:29,927 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:13:29,927 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:13:29,927 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:13:29,927 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:13:29,927 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:13:29,927 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:13:29,934 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:51,  4.37s/it]  1%|▏         | 2/150 [00:08<10:07,  4.11s/it]  2%|▏         | 3/150 [00:12<09:46,  3.99s/it]  3%|▎         | 4/150 [00:16<09:34,  3.94s/it]  3%|▎         | 5/150 [00:19<09:23,  3.89s/it]  4%|▍         | 6/150 [00:23<09:10,  3.82s/it]  5%|▍         | 7/150 [00:27<09:05,  3.82s/it]  5%|▌         | 8/150 [00:31<09:03,  3.83s/it]  6%|▌         | 9/150 [00:34<08:59,  3.83s/it]  7%|▋         | 10/150 [00:38<08:56,  3.83s/it]                                                {'loss': 1.4932, 'grad_norm': 2.1582279205322266, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:38<08:56,  3.83s/it]  7%|▋         | 11/150 [00:42<08:51,  3.83s/it]  8%|▊         | 12/150 [00:46<08:46,  3.82s/it]  9%|▊         | 13/150 [00:50<08:42,  3.82s/it]  9%|▉         | 14/150 [00:54<08:39,  3.82s/it] 10%|█         | 15/150 [00:57<08:34,  3.81s/it] 11%|█         | 16/150 [01:01<08:30,  3.81s/it] 11%|█▏        | 17/150 [01:05<08:26,  3.81s/it] 12%|█▏        | 18/150 [01:09<08:24,  3.82s/it] 13%|█▎        | 19/150 [01:13<08:22,  3.83s/it] 13%|█▎        | 20/150 [01:17<08:18,  3.84s/it]                                                {'loss': 0.5234, 'grad_norm': 1.6072402000427246, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:17<08:18,  3.84s/it] 14%|█▍        | 21/150 [01:20<08:15,  3.84s/it] 15%|█▍        | 22/150 [01:24<08:12,  3.85s/it] 15%|█▌        | 23/150 [01:28<08:06,  3.83s/it] 16%|█▌        | 24/150 [01:32<08:01,  3.82s/it] 17%|█▋        | 25/150 [01:36<07:56,  3.81s/it] 17%|█▋        | 26/150 [01:39<07:53,  3.82s/it] 18%|█▊        | 27/150 [01:43<07:46,  3.79s/it] 19%|█▊        | 28/150 [01:47<07:35,  3.73s/it] 19%|█▉        | 29/150 [01:50<07:26,  3.69s/it] 20%|██        | 30/150 [01:54<07:19,  3.66s/it]                                                {'loss': 0.1928, 'grad_norm': 1.0294513702392578, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:54<07:19,  3.66s/it] 21%|██        | 31/150 [01:58<07:14,  3.65s/it] 21%|██▏       | 32/150 [02:01<07:10,  3.65s/it] 22%|██▏       | 33/150 [02:05<07:06,  3.65s/it] 23%|██▎       | 34/150 [02:09<07:02,  3.64s/it] 23%|██▎       | 35/150 [02:12<06:58,  3.64s/it] 24%|██▍       | 36/150 [02:16<06:54,  3.63s/it] 25%|██▍       | 37/150 [02:19<06:51,  3.64s/it] 25%|██▌       | 38/150 [02:23<06:45,  3.62s/it] 26%|██▌       | 39/150 [02:27<06:40,  3.61s/it] 27%|██▋       | 40/150 [02:30<06:35,  3.59s/it]                                                {'loss': 0.1169, 'grad_norm': 0.8127015829086304, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:30<06:35,  3.59s/it] 27%|██▋       | 41/150 [02:34<06:31,  3.59s/it] 28%|██▊       | 42/150 [02:37<06:26,  3.58s/it] 29%|██▊       | 43/150 [02:41<06:23,  3.58s/it] 29%|██▉       | 44/150 [02:44<06:17,  3.57s/it] 30%|███       | 45/150 [02:48<06:12,  3.55s/it] 31%|███       | 46/150 [02:51<06:10,  3.56s/it] 31%|███▏      | 47/150 [02:55<06:04,  3.54s/it] 32%|███▏      | 48/150 [02:59<06:00,  3.54s/it] 33%|███▎      | 49/150 [03:02<05:56,  3.53s/it] 33%|███▎      | 50/150 [03:06<05:53,  3.53s/it]                                                {'loss': 0.0872, 'grad_norm': 0.4073559045791626, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:06<05:53,  3.53s/it] 34%|███▍      | 51/150 [03:09<05:50,  3.54s/it] 35%|███▍      | 52/150 [03:13<05:48,  3.56s/it] 35%|███▌      | 53/150 [03:16<05:46,  3.57s/it] 36%|███▌      | 54/150 [03:20<05:44,  3.59s/it] 37%|███▋      | 55/150 [03:24<05:40,  3.58s/it] 37%|███▋      | 56/150 [03:27<05:36,  3.57s/it] 38%|███▊      | 57/150 [03:31<05:32,  3.57s/it] 39%|███▊      | 58/150 [03:34<05:27,  3.57s/it] 39%|███▉      | 59/150 [03:38<05:23,  3.56s/it] 40%|████      | 60/150 [03:41<05:20,  3.56s/it]                                                {'loss': 0.0676, 'grad_norm': 0.3596411645412445, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:41<05:20,  3.56s/it] 41%|████      | 61/150 [03:45<05:16,  3.55s/it] 41%|████▏     | 62/150 [03:48<05:13,  3.56s/it] 42%|████▏     | 63/150 [03:52<05:10,  3.57s/it] 43%|████▎     | 64/150 [03:56<05:08,  3.59s/it] 43%|████▎     | 65/150 [03:59<05:05,  3.60s/it] 44%|████▍     | 66/150 [04:03<05:02,  3.60s/it] 45%|████▍     | 67/150 [04:06<04:58,  3.60s/it] 45%|████▌     | 68/150 [04:10<04:55,  3.61s/it] 46%|████▌     | 69/150 [04:14<04:52,  3.61s/it] 47%|████▋     | 70/150 [04:17<04:49,  3.62s/it]                                                {'loss': 0.0559, 'grad_norm': 0.35015231370925903, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:17<04:49,  3.62s/it] 47%|████▋     | 71/150 [04:21<04:46,  3.62s/it] 48%|████▊     | 72/150 [04:25<04:41,  3.61s/it] 49%|████▊     | 73/150 [04:28<04:37,  3.60s/it] 49%|████▉     | 74/150 [04:32<04:33,  3.60s/it] 50%|█████     | 75/150 [04:35<04:30,  3.60s/it] 51%|█████     | 76/150 [04:39<04:26,  3.60s/it] 51%|█████▏    | 77/150 [04:43<04:22,  3.60s/it] 52%|█████▏    | 78/150 [04:46<04:18,  3.59s/it] 53%|█████▎    | 79/150 [04:50<04:14,  3.58s/it] 53%|█████▎    | 80/150 [04:53<04:10,  3.58s/it]                                                {'loss': 0.0551, 'grad_norm': 0.3890810012817383, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:53<04:10,  3.58s/it] 54%|█████▍    | 81/150 [04:57<04:07,  3.58s/it] 55%|█████▍    | 82/150 [05:00<04:03,  3.59s/it] 55%|█████▌    | 83/150 [05:04<04:00,  3.59s/it] 56%|█████▌    | 84/150 [05:08<03:55,  3.57s/it] 57%|█████▋    | 85/150 [05:11<03:51,  3.57s/it] 57%|█████▋    | 86/150 [05:15<03:48,  3.57s/it] 58%|█████▊    | 87/150 [05:18<03:45,  3.58s/it] 59%|█████▊    | 88/150 [05:22<03:42,  3.58s/it] 59%|█████▉    | 89/150 [05:25<03:39,  3.59s/it] 60%|██████    | 90/150 [05:29<03:35,  3.60s/it]                                                {'loss': 0.062, 'grad_norm': 0.39091166853904724, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:29<03:35,  3.60s/it] 61%|██████    | 91/150 [05:33<03:32,  3.60s/it] 61%|██████▏   | 92/150 [05:36<03:28,  3.60s/it] 62%|██████▏   | 93/150 [05:40<03:25,  3.60s/it] 63%|██████▎   | 94/150 [05:43<03:21,  3.60s/it] 63%|██████▎   | 95/150 [05:47<03:18,  3.60s/it] 64%|██████▍   | 96/150 [05:51<03:14,  3.61s/it] 65%|██████▍   | 97/150 [05:54<03:11,  3.61s/it] 65%|██████▌   | 98/150 [05:58<03:07,  3.61s/it] 66%|██████▌   | 99/150 [06:02<03:04,  3.62s/it] 67%|██████▋   | 100/150 [06:05<03:00,  3.61s/it]                                                 {'loss': 0.0589, 'grad_norm': 0.3413178026676178, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:05<03:00,  3.61s/it] 67%|██████▋   | 101/150 [06:09<02:56,  3.61s/it] 68%|██████▊   | 102/150 [06:12<02:52,  3.59s/it] 69%|██████▊   | 103/150 [06:16<02:48,  3.58s/it] 69%|██████▉   | 104/150 [06:19<02:43,  3.56s/it] 70%|███████   | 105/150 [06:23<02:40,  3.56s/it] 71%|███████   | 106/150 [06:27<02:36,  3.56s/it] 71%|███████▏  | 107/150 [06:30<02:32,  3.55s/it] 72%|███████▏  | 108/150 [06:34<02:29,  3.56s/it] 73%|███████▎  | 109/150 [06:37<02:26,  3.56s/it] 73%|███████▎  | 110/150 [06:41<02:22,  3.56s/it]                                                 {'loss': 0.0481, 'grad_norm': 0.3069192171096802, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:41<02:22,  3.56s/it] 74%|███████▍  | 111/150 [06:44<02:19,  3.56s/it] 75%|███████▍  | 112/150 [06:48<02:15,  3.57s/it] 75%|███████▌  | 113/150 [06:51<02:11,  3.57s/it] 76%|███████▌  | 114/150 [06:55<02:08,  3.57s/it] 77%|███████▋  | 115/150 [06:59<02:04,  3.56s/it] 77%|███████▋  | 116/150 [07:02<02:00,  3.55s/it] 78%|███████▊  | 117/150 [07:06<01:57,  3.56s/it] 79%|███████▊  | 118/150 [07:09<01:54,  3.56s/it] 79%|███████▉  | 119/150 [07:13<01:50,  3.58s/it] 80%|████████  | 120/150 [07:16<01:47,  3.57s/it]                                                 {'loss': 0.0543, 'grad_norm': 0.37332621216773987, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:16<01:47,  3.57s/it] 81%|████████  | 121/150 [07:20<01:43,  3.58s/it] 81%|████████▏ | 122/150 [07:24<01:40,  3.59s/it] 82%|████████▏ | 123/150 [07:27<01:37,  3.60s/it] 83%|████████▎ | 124/150 [07:31<01:33,  3.60s/it] 83%|████████▎ | 125/150 [07:34<01:30,  3.61s/it] 84%|████████▍ | 126/150 [07:38<01:26,  3.61s/it] 85%|████████▍ | 127/150 [07:42<01:22,  3.61s/it] 85%|████████▌ | 128/150 [07:45<01:19,  3.60s/it] 86%|████████▌ | 129/150 [07:49<01:15,  3.60s/it] 87%|████████▋ | 130/150 [07:52<01:11,  3.59s/it]                                                 {'loss': 0.037, 'grad_norm': 0.1809934824705124, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [07:52<01:11,  3.59s/it] 87%|████████▋ | 131/150 [07:56<01:08,  3.59s/it] 88%|████████▊ | 132/150 [08:00<01:04,  3.59s/it] 89%|████████▊ | 133/150 [08:03<01:01,  3.61s/it] 89%|████████▉ | 134/150 [08:07<00:57,  3.61s/it] 90%|█████████ | 135/150 [08:11<00:54,  3.61s/it] 91%|█████████ | 136/150 [08:14<00:50,  3.62s/it] 91%|█████████▏| 137/150 [08:18<00:46,  3.61s/it] 92%|█████████▏| 138/150 [08:21<00:43,  3.62s/it] 93%|█████████▎| 139/150 [08:25<00:39,  3.62s/it] 93%|█████████▎| 140/150 [08:29<00:36,  3.62s/it]                                                 {'loss': 0.0367, 'grad_norm': 0.4468074142932892, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:29<00:36,  3.62s/it] 94%|█████████▍| 141/150 [08:32<00:32,  3.61s/it] 95%|█████████▍| 142/150 [08:36<00:28,  3.59s/it] 95%|█████████▌| 143/150 [08:39<00:25,  3.58s/it] 96%|█████████▌| 144/150 [08:43<00:21,  3.58s/it] 97%|█████████▋| 145/150 [08:46<00:17,  3.58s/it] 97%|█████████▋| 146/150 [08:50<00:14,  3.57s/it] 98%|█████████▊| 147/150 [08:54<00:10,  3.57s/it] 99%|█████████▊| 148/150 [08:57<00:07,  3.57s/it] 99%|█████████▉| 149/150 [09:01<00:03,  3.56s/it]100%|██████████| 150/150 [09:04<00:00,  3.57s/it]                                                 {'loss': 0.0508, 'grad_norm': 0.19510769844055176, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [09:04<00:00,  3.57s/it][INFO|trainer.py:3705] 2024-11-19 00:22:35,509 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:22:35,548 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:22:35,549 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:22:35,720 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:22:35,720 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 00:22:36,470 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 546.5359, 'train_samples_per_second': 4.424, 'train_steps_per_second': 0.274, 'train_loss': 0.195987046957016, 'epoch': 2.98}
100%|██████████| 150/150 [09:05<00:00,  3.57s/it]100%|██████████| 150/150 [09:05<00:00,  3.64s/it]
[INFO|trainer.py:3705] 2024-11-19 00:22:36,473 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:22:36,506 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:22:36,508 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:22:36,671 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:22:36,672 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 12730990GF
  train_loss               =      0.196
  train_runtime            = 0:09:06.53
  train_samples_per_second =      4.424
  train_steps_per_second   =      0.274
Figure saved at: saves/Meta-Llama-3.1-8B-Instruct/normal_prompt/lora/sft/training_loss.png
11/19/2024 00:22:37 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:22:37 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:22:37,129 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:22:37,129 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:22:37,129 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 19.42it/s]  9%|▉         | 4/45 [00:00<00:03, 12.08it/s] 13%|█▎        | 6/45 [00:00<00:03, 10.99it/s] 18%|█▊        | 8/45 [00:00<00:03, 10.52it/s] 22%|██▏       | 10/45 [00:00<00:03, 10.27it/s] 27%|██▋       | 12/45 [00:01<00:03, 10.13it/s] 31%|███       | 14/45 [00:01<00:03, 10.03it/s] 36%|███▌      | 16/45 [00:01<00:02,  9.96it/s] 40%|████      | 18/45 [00:01<00:02,  9.87it/s] 42%|████▏     | 19/45 [00:01<00:02,  9.86it/s] 44%|████▍     | 20/45 [00:01<00:02,  9.83it/s] 47%|████▋     | 21/45 [00:02<00:02,  9.83it/s] 49%|████▉     | 22/45 [00:02<00:02,  9.81it/s] 51%|█████     | 23/45 [00:02<00:02,  9.80it/s] 53%|█████▎    | 24/45 [00:02<00:02,  9.79it/s] 56%|█████▌    | 25/45 [00:02<00:02,  9.79it/s] 58%|█████▊    | 26/45 [00:02<00:01,  9.79it/s] 60%|██████    | 27/45 [00:02<00:01,  9.78it/s] 62%|██████▏   | 28/45 [00:02<00:01,  9.77it/s] 64%|██████▍   | 29/45 [00:02<00:01,  9.77it/s] 67%|██████▋   | 30/45 [00:02<00:01,  9.77it/s] 69%|██████▉   | 31/45 [00:03<00:01,  9.76it/s] 71%|███████   | 32/45 [00:03<00:01,  9.78it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.75it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.75it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.76it/s] 80%|████████  | 36/45 [00:03<00:00,  9.77it/s] 82%|████████▏ | 37/45 [00:03<00:00,  9.77it/s] 84%|████████▍ | 38/45 [00:03<00:00,  9.75it/s] 87%|████████▋ | 39/45 [00:03<00:00,  9.77it/s] 89%|████████▉ | 40/45 [00:03<00:00,  9.76it/s] 91%|█████████ | 41/45 [00:04<00:00,  9.76it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.76it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.74it/s] 98%|█████████▊| 44/45 [00:04<00:00,  9.74it/s]100%|██████████| 45/45 [00:04<00:00,  9.77it/s]100%|██████████| 45/45 [00:04<00:00, 10.00it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0683
  eval_runtime            = 0:00:04.62
  eval_samples_per_second =     19.477
  eval_steps_per_second   =      9.739
[INFO|modelcard.py:449] 2024-11-19 00:22:41,750 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:23:01,107] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:23:05 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26870
[2024-11-19 00:23:07,229] torch.distributed.run: [WARNING] 
[2024-11-19 00:23:07,229] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:23:07,229] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:23:07,229] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:23:14,527] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:23:14,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:23:15 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:23:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:23:15,774 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:23:15,775 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:15,776 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:15,777 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:15,777 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:15,777 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:15,777 >> loading file tokenizer_config.json
11/19/2024 00:23:15 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:23:15 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:23:16,434 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:23:16,435 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:23:16,436 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:16,438 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:16,438 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:16,438 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:16,438 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:23:16,438 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:23:17,074 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:23:17 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:23:17 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:23:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:23:17 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
11/19/2024 00:23:17 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
11/19/2024 00:23:17 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:23:17 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 446.98 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3302.58 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:23:21 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:26, 31.59 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:02<00:12, 62.56 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:02<00:08, 90.85 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:03<00:08, 77.86 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:03<00:02, 170.05 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:04<00:03, 142.17 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:04<00:02, 151.10 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:04<00:01, 201.29 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:04<00:00, 230.29 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:05<00:00, 214.88 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:05<00:00, 198.12 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:05<00:00, 219.21 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:06<00:00, 194.59 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:06<00:00, 145.68 examples/s]
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 37767, 53283, 75493, 9554, 17801, 28469, 121589, 29411, 5018, 17327, 794, 4482, 103377, 9554, 33005, 18184, 5232, 3670, 2173, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 21710, 1073, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 26094, 2252, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 21710, 1073, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 6313, 498, 330, 103377, 9554, 33005, 18184, 5232, 26094, 6313, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 6313, 498, 330, 103377, 9554, 33005, 18184, 5232, 56065, 266, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 58904, 498, 330, 103377, 9554, 33005, 18184, 5232, 1816, 266, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 58904, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 9792, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 2658, 40622, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 33498, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 33498, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 33134, 82, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 8629, 11, 72718, 28542, 33005, 18184, 25, 58904, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 9164, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 4581, 1073, 11, 72718, 28542, 33005, 18184, 25, 2050, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 23405, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 13014, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 22502, 11, 72718, 28542, 33005, 18184, 25, 4681, 11, 105866, 28542, 33005, 18184, 25, 4681, 1058, 498, 330, 103377, 9554, 33005, 18184, 5232, 1056, 392, 1058, 1073, 11, 72718, 28542, 33005, 18184, 25, 4681, 1058, 11, 105866, 28542, 33005, 18184, 25, 4681, 1058, 8073, 330, 20606, 794, 4482, 93474, 33005, 25, 6313, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 4222, 518, 364, 1834, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 2050, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 518, 364, 1337, 4181, 11844, 330, 93474, 33005, 25, 9164, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 2386, 518, 364, 13265, 518, 364, 52365, 518, 364, 11789, 518, 364, 30306, 518, 364, 29525, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 58904, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 518, 364, 1337, 4181, 11844, 330, 93474, 33005, 25, 23405, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 2150, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 4681, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 4181, 11844, 330, 93474, 33005, 25, 2252, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 4222, 518, 364, 1834, 518, 364, 11789, 518, 364, 1843, 1213, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 4681, 1058, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 4181, 8, 93110, 15225, 45163, 88852, 109683, 120074, 47770, 72234, 18184, 29129, 75493, 9554, 57815, 29182, 52254, 512, 33976, 93233, 33764, 40052, 1426, 81, 1814, 292, 2083, 38221, 99750, 106236, 110999, 104123, 43323, 9554, 94668, 5486, 34171, 64022, 5486, 120074, 34208, 116122, 105514, 91495, 32626, 31944, 62855, 9554, 3222, 34208, 31091, 3922, 60251, 60979, 31944, 62855, 31091, 75761, 128009, 128006, 78191, 128007, 271, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|eot_id|><|start_header_id|>assistant<|end_header_id|>

match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
[INFO|configuration_utils.py:673] 2024-11-19 00:23:28,296 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:23:28,297 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:23:28,341 >> loading weights file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:23:28,341 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:23:28,343 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
[INFO|modeling_utils.py:4574] 2024-11-19 00:23:32,014 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:23:32,014 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 00:23:32,019 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 00:23:32,019 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

11/19/2024 00:23:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:23:32 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:23:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:23:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:23:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,q_proj,v_proj,up_proj,o_proj,gate_proj
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/it]11/19/2024 00:23:32 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:23:32,791 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
11/19/2024 00:23:33 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:23:33 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:23:33 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:23:33 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:23:33 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,up_proj,k_proj,gate_proj,o_proj,q_proj
11/19/2024 00:23:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
[INFO|trainer.py:2243] 2024-11-19 00:23:34,498 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:23:34,499 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:23:34,499 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:23:34,499 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:23:34,499 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:23:34,499 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:23:34,499 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:23:34,506 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:55,  4.40s/it]  1%|▏         | 2/150 [00:08<10:17,  4.17s/it]  2%|▏         | 3/150 [00:12<10:03,  4.11s/it]  3%|▎         | 4/150 [00:16<09:52,  4.06s/it]  3%|▎         | 5/150 [00:20<09:42,  4.02s/it]  4%|▍         | 6/150 [00:24<09:40,  4.03s/it]  5%|▍         | 7/150 [00:28<09:35,  4.02s/it]  5%|▌         | 8/150 [00:32<09:28,  4.00s/it]  6%|▌         | 9/150 [00:36<09:20,  3.98s/it]  7%|▋         | 10/150 [00:40<09:14,  3.96s/it]                                                {'loss': 0.8208, 'grad_norm': 1.5800347328186035, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:40<09:14,  3.96s/it]  7%|▋         | 11/150 [00:44<09:14,  3.99s/it]  8%|▊         | 12/150 [00:48<09:10,  3.99s/it]  9%|▊         | 13/150 [00:52<09:09,  4.01s/it]  9%|▉         | 14/150 [00:56<09:05,  4.01s/it] 10%|█         | 15/150 [01:00<09:00,  4.00s/it] 11%|█         | 16/150 [01:04<08:54,  3.99s/it] 11%|█▏        | 17/150 [01:08<08:50,  3.99s/it] 12%|█▏        | 18/150 [01:12<08:44,  3.97s/it] 13%|█▎        | 19/150 [01:16<08:40,  3.97s/it] 13%|█▎        | 20/150 [01:20<08:38,  3.99s/it]                                                {'loss': 0.1888, 'grad_norm': 1.0966943502426147, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:20<08:38,  3.99s/it] 14%|█▍        | 21/150 [01:24<08:34,  3.99s/it] 15%|█▍        | 22/150 [01:28<08:30,  3.98s/it] 15%|█▌        | 23/150 [01:32<08:26,  3.99s/it] 16%|█▌        | 24/150 [01:36<08:21,  3.98s/it] 17%|█▋        | 25/150 [01:40<08:16,  3.97s/it] 17%|█▋        | 26/150 [01:44<08:13,  3.98s/it] 18%|█▊        | 27/150 [01:48<08:09,  3.98s/it] 19%|█▊        | 28/150 [01:52<08:05,  3.98s/it] 19%|█▉        | 29/150 [01:56<08:01,  3.98s/it] 20%|██        | 30/150 [02:00<07:58,  3.99s/it]                                                {'loss': 0.1052, 'grad_norm': 0.76889967918396, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:00<07:58,  3.99s/it] 21%|██        | 31/150 [02:03<07:54,  3.98s/it] 21%|██▏       | 32/150 [02:07<07:49,  3.98s/it] 22%|██▏       | 33/150 [02:11<07:45,  3.98s/it] 23%|██▎       | 34/150 [02:15<07:42,  3.99s/it] 23%|██▎       | 35/150 [02:19<07:38,  3.99s/it] 24%|██▍       | 36/150 [02:23<07:34,  3.98s/it] 25%|██▍       | 37/150 [02:27<07:28,  3.97s/it] 25%|██▌       | 38/150 [02:31<07:26,  3.98s/it] 26%|██▌       | 39/150 [02:35<07:21,  3.97s/it] 27%|██▋       | 40/150 [02:39<07:17,  3.98s/it]                                                {'loss': 0.0864, 'grad_norm': 0.3916124403476715, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:39<07:17,  3.98s/it] 27%|██▋       | 41/150 [02:43<07:12,  3.97s/it] 28%|██▊       | 42/150 [02:47<07:08,  3.97s/it] 29%|██▊       | 43/150 [02:51<07:05,  3.98s/it] 29%|██▉       | 44/150 [02:55<07:01,  3.98s/it] 30%|███       | 45/150 [02:59<06:58,  3.98s/it] 31%|███       | 46/150 [03:03<06:55,  4.00s/it] 31%|███▏      | 47/150 [03:07<06:50,  3.99s/it] 32%|███▏      | 48/150 [03:11<06:45,  3.98s/it] 33%|███▎      | 49/150 [03:15<06:40,  3.96s/it] 33%|███▎      | 50/150 [03:19<06:35,  3.96s/it]                                                {'loss': 0.074, 'grad_norm': 0.43601033091545105, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:19<06:35,  3.96s/it] 34%|███▍      | 51/150 [03:23<06:31,  3.95s/it] 35%|███▍      | 52/150 [03:27<06:28,  3.97s/it] 35%|███▌      | 53/150 [03:31<06:25,  3.97s/it] 36%|███▌      | 54/150 [03:35<06:21,  3.98s/it] 37%|███▋      | 55/150 [03:39<06:17,  3.98s/it] 37%|███▋      | 56/150 [03:43<06:13,  3.97s/it] 38%|███▊      | 57/150 [03:47<06:09,  3.97s/it] 39%|███▊      | 58/150 [03:51<06:04,  3.96s/it] 39%|███▉      | 59/150 [03:55<06:01,  3.97s/it] 40%|████      | 60/150 [03:59<05:57,  3.97s/it]                                                {'loss': 0.0583, 'grad_norm': 0.28595978021621704, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:59<05:57,  3.97s/it] 41%|████      | 61/150 [04:03<05:54,  3.98s/it] 41%|████▏     | 62/150 [04:07<05:49,  3.97s/it] 42%|████▏     | 63/150 [04:11<05:45,  3.97s/it] 43%|████▎     | 64/150 [04:15<05:41,  3.97s/it] 43%|████▎     | 65/150 [04:19<05:38,  3.99s/it] 44%|████▍     | 66/150 [04:23<05:33,  3.97s/it] 45%|████▍     | 67/150 [04:27<05:29,  3.97s/it] 45%|████▌     | 68/150 [04:31<05:25,  3.96s/it] 46%|████▌     | 69/150 [04:35<05:21,  3.97s/it] 47%|████▋     | 70/150 [04:39<05:18,  3.98s/it]                                                {'loss': 0.0502, 'grad_norm': 0.26137062907218933, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:39<05:18,  3.98s/it] 47%|████▋     | 71/150 [04:42<05:14,  3.98s/it] 48%|████▊     | 72/150 [04:46<05:09,  3.97s/it] 49%|████▊     | 73/150 [04:50<05:05,  3.97s/it] 49%|████▉     | 74/150 [04:54<05:03,  4.00s/it] 50%|█████     | 75/150 [04:59<05:01,  4.02s/it] 51%|█████     | 76/150 [05:03<04:59,  4.04s/it] 51%|█████▏    | 77/150 [05:07<04:57,  4.07s/it] 52%|█████▏    | 78/150 [05:11<04:56,  4.12s/it] 53%|█████▎    | 79/150 [05:15<04:54,  4.15s/it] 53%|█████▎    | 80/150 [05:19<04:51,  4.16s/it]                                                {'loss': 0.0518, 'grad_norm': 0.32620808482170105, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [05:19<04:51,  4.16s/it] 54%|█████▍    | 81/150 [05:24<04:46,  4.15s/it] 55%|█████▍    | 82/150 [05:28<04:42,  4.16s/it] 55%|█████▌    | 83/150 [05:32<04:38,  4.16s/it] 56%|█████▌    | 84/150 [05:36<04:31,  4.12s/it] 57%|█████▋    | 85/150 [05:40<04:24,  4.07s/it] 57%|█████▋    | 86/150 [05:44<04:19,  4.05s/it] 58%|█████▊    | 87/150 [05:48<04:13,  4.02s/it] 59%|█████▊    | 88/150 [05:52<04:08,  4.01s/it] 59%|█████▉    | 89/150 [05:56<04:03,  3.99s/it] 60%|██████    | 90/150 [06:00<04:00,  4.00s/it]                                                {'loss': 0.059, 'grad_norm': 0.3741208016872406, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [06:00<04:00,  4.00s/it] 61%|██████    | 91/150 [06:04<03:55,  3.99s/it] 61%|██████▏   | 92/150 [06:08<03:49,  3.96s/it] 62%|██████▏   | 93/150 [06:12<03:46,  3.97s/it] 63%|██████▎   | 94/150 [06:16<03:42,  3.97s/it] 63%|██████▎   | 95/150 [06:20<03:38,  3.97s/it] 64%|██████▍   | 96/150 [06:24<03:34,  3.97s/it] 65%|██████▍   | 97/150 [06:28<03:31,  3.99s/it] 65%|██████▌   | 98/150 [06:32<03:27,  3.98s/it] 66%|██████▌   | 99/150 [06:36<03:23,  3.99s/it] 67%|██████▋   | 100/150 [06:40<03:19,  3.99s/it]                                                 {'loss': 0.0548, 'grad_norm': 0.2783927619457245, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:40<03:19,  3.99s/it] 67%|██████▋   | 101/150 [06:44<03:16,  4.01s/it] 68%|██████▊   | 102/150 [06:48<03:11,  4.00s/it] 69%|██████▊   | 103/150 [06:52<03:07,  3.99s/it] 69%|██████▉   | 104/150 [06:55<03:03,  3.99s/it] 70%|███████   | 105/150 [06:59<02:59,  3.98s/it] 71%|███████   | 106/150 [07:03<02:55,  3.99s/it] 71%|███████▏  | 107/150 [07:07<02:51,  3.99s/it] 72%|███████▏  | 108/150 [07:11<02:47,  4.00s/it] 73%|███████▎  | 109/150 [07:15<02:43,  3.99s/it] 73%|███████▎  | 110/150 [07:19<02:39,  3.98s/it]                                                 {'loss': 0.0453, 'grad_norm': 0.2583763003349304, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [07:19<02:39,  3.98s/it] 74%|███████▍  | 111/150 [07:24<02:36,  4.02s/it] 75%|███████▍  | 112/150 [07:28<02:33,  4.05s/it] 75%|███████▌  | 113/150 [07:32<02:30,  4.06s/it] 76%|███████▌  | 114/150 [07:36<02:26,  4.08s/it] 77%|███████▋  | 115/150 [07:40<02:22,  4.08s/it] 77%|███████▋  | 116/150 [07:44<02:19,  4.10s/it] 78%|███████▊  | 117/150 [07:48<02:15,  4.11s/it] 79%|███████▊  | 118/150 [07:52<02:09,  4.06s/it] 79%|███████▉  | 119/150 [07:56<02:05,  4.04s/it] 80%|████████  | 120/150 [08:00<02:00,  4.03s/it]                                                 {'loss': 0.0522, 'grad_norm': 0.3768836259841919, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [08:00<02:00,  4.03s/it] 81%|████████  | 121/150 [08:04<01:56,  4.01s/it] 81%|████████▏ | 122/150 [08:08<01:52,  4.01s/it] 82%|████████▏ | 123/150 [08:12<01:47,  3.98s/it] 83%|████████▎ | 124/150 [08:16<01:43,  3.97s/it] 83%|████████▎ | 125/150 [08:20<01:38,  3.96s/it] 84%|████████▍ | 126/150 [08:24<01:35,  3.97s/it] 85%|████████▍ | 127/150 [08:28<01:31,  3.96s/it] 85%|████████▌ | 128/150 [08:32<01:27,  3.96s/it] 86%|████████▌ | 129/150 [08:36<01:23,  3.97s/it] 87%|████████▋ | 130/150 [08:40<01:19,  3.96s/it]                                                 {'loss': 0.0328, 'grad_norm': 0.1773710697889328, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:40<01:19,  3.96s/it] 87%|████████▋ | 131/150 [08:44<01:15,  3.97s/it] 88%|████████▊ | 132/150 [08:48<01:11,  3.98s/it] 89%|████████▊ | 133/150 [08:52<01:07,  3.97s/it] 89%|████████▉ | 134/150 [08:56<01:03,  3.98s/it] 90%|█████████ | 135/150 [09:00<00:59,  3.98s/it] 91%|█████████ | 136/150 [09:04<00:55,  3.96s/it] 91%|█████████▏| 137/150 [09:08<00:51,  3.96s/it] 92%|█████████▏| 138/150 [09:12<00:47,  3.98s/it] 93%|█████████▎| 139/150 [09:16<00:43,  3.98s/it] 93%|█████████▎| 140/150 [09:20<00:39,  3.97s/it]                                                 {'loss': 0.0393, 'grad_norm': 0.4683811068534851, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [09:20<00:39,  3.97s/it] 94%|█████████▍| 141/150 [09:23<00:35,  3.96s/it] 95%|█████████▍| 142/150 [09:27<00:31,  3.96s/it] 95%|█████████▌| 143/150 [09:31<00:27,  3.96s/it] 96%|█████████▌| 144/150 [09:35<00:23,  3.96s/it] 97%|█████████▋| 145/150 [09:39<00:19,  3.97s/it] 97%|█████████▋| 146/150 [09:43<00:15,  3.98s/it] 98%|█████████▊| 147/150 [09:47<00:11,  3.99s/it] 99%|█████████▊| 148/150 [09:51<00:07,  3.98s/it] 99%|█████████▉| 149/150 [09:55<00:03,  3.98s/it]100%|██████████| 150/150 [09:59<00:00,  3.97s/it]                                                 {'loss': 0.0483, 'grad_norm': 0.11460845172405243, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [09:59<00:00,  3.97s/it][INFO|trainer.py:3705] 2024-11-19 00:33:35,024 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:33:35,063 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:33:35,064 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:33:35,237 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:33:35,238 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 00:33:35,943 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 601.4368, 'train_samples_per_second': 4.02, 'train_steps_per_second': 0.249, 'train_loss': 0.1178185886144638, 'epoch': 2.98}
100%|██████████| 150/150 [10:00<00:00,  3.97s/it]100%|██████████| 150/150 [10:00<00:00,  4.00s/it]
[INFO|trainer.py:3705] 2024-11-19 00:33:35,945 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:33:35,979 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:33:35,980 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:33:36,145 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:33:36,145 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 92656551GF
  train_loss               =     0.1178
  train_runtime            = 0:10:01.43
  train_samples_per_second =       4.02
  train_steps_per_second   =      0.249
Figure saved at: saves/Meta-Llama-3.1-8B-Instruct/schema_prompt/lora/sft/training_loss.png
11/19/2024 00:33:36 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:33:36 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:33:36,603 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:33:36,603 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:33:36,603 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 14.78it/s]  9%|▉         | 4/45 [00:00<00:04,  9.31it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.36it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.06it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.00it/s] 20%|██        | 9/45 [00:01<00:04,  7.91it/s] 22%|██▏       | 10/45 [00:01<00:04,  7.87it/s] 24%|██▍       | 11/45 [00:01<00:04,  7.89it/s] 27%|██▋       | 12/45 [00:01<00:04,  7.83it/s] 29%|██▉       | 13/45 [00:01<00:04,  7.80it/s] 31%|███       | 14/45 [00:01<00:04,  7.75it/s] 33%|███▎      | 15/45 [00:01<00:03,  7.72it/s] 36%|███▌      | 16/45 [00:01<00:03,  7.71it/s] 38%|███▊      | 17/45 [00:02<00:03,  7.76it/s] 40%|████      | 18/45 [00:02<00:03,  7.71it/s] 42%|████▏     | 19/45 [00:02<00:03,  7.77it/s] 44%|████▍     | 20/45 [00:02<00:03,  7.74it/s] 47%|████▋     | 21/45 [00:02<00:03,  7.61it/s] 49%|████▉     | 22/45 [00:02<00:03,  7.61it/s] 51%|█████     | 23/45 [00:02<00:02,  7.67it/s] 53%|█████▎    | 24/45 [00:03<00:02,  7.66it/s] 56%|█████▌    | 25/45 [00:03<00:02,  7.66it/s] 58%|█████▊    | 26/45 [00:03<00:02,  7.68it/s] 60%|██████    | 27/45 [00:03<00:02,  7.70it/s] 62%|██████▏   | 28/45 [00:03<00:02,  7.64it/s] 64%|██████▍   | 29/45 [00:03<00:02,  7.52it/s] 67%|██████▋   | 30/45 [00:03<00:02,  7.44it/s] 69%|██████▉   | 31/45 [00:03<00:01,  7.56it/s] 71%|███████   | 32/45 [00:04<00:01,  7.57it/s] 73%|███████▎  | 33/45 [00:04<00:01,  7.58it/s] 76%|███████▌  | 34/45 [00:04<00:01,  7.74it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.70it/s] 80%|████████  | 36/45 [00:04<00:01,  7.68it/s] 82%|████████▏ | 37/45 [00:04<00:01,  7.74it/s] 84%|████████▍ | 38/45 [00:04<00:00,  7.72it/s] 87%|████████▋ | 39/45 [00:04<00:00,  7.77it/s] 89%|████████▉ | 40/45 [00:05<00:00,  7.80it/s] 91%|█████████ | 41/45 [00:05<00:00,  7.82it/s] 93%|█████████▎| 42/45 [00:05<00:00,  7.75it/s] 96%|█████████▌| 43/45 [00:05<00:00,  7.68it/s] 98%|█████████▊| 44/45 [00:05<00:00,  7.73it/s]100%|██████████| 45/45 [00:05<00:00,  7.70it/s]100%|██████████| 45/45 [00:05<00:00,  7.82it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0638
  eval_runtime            = 0:00:05.89
  eval_samples_per_second =     15.265
  eval_steps_per_second   =      7.633
[INFO|modelcard.py:449] 2024-11-19 00:33:42,499 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:33:59,889] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:34:03 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:24687
[2024-11-19 00:34:06,058] torch.distributed.run: [WARNING] 
[2024-11-19 00:34:06,058] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:34:06,058] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:34:06,058] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:34:13,328] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:34:13,581] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:34:14 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:34:14 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:34:14,731 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:34:14,732 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:14,734 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:14,734 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:14,734 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:14,734 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:14,734 >> loading file tokenizer_config.json
11/19/2024 00:34:14 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:34:14 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:34:15,401 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:34:15,403 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:34:15,404 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:15,405 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:15,406 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:15,406 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:15,406 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:34:15,406 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:34:16,040 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:34:16 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:34:16 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:34:16 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
11/19/2024 00:34:16 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/19/2024 00:34:16 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:34:16 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:34:16 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 389.14 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2985.22 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:34:20 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:27, 31.08 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:02<00:17, 45.67 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:03<00:07, 95.78 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:03<00:05, 105.31 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:03<00:04, 113.55 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:04<00:04, 118.92 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:04<00:03, 123.98 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:05<00:03, 127.94 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:05<00:02, 137.57 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:05<00:01, 144.77 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:06<00:01, 148.83 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:06<00:01, 145.34 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:06<00:00, 165.45 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:07<00:00, 166.42 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:07<00:00, 142.27 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:07<00:00, 116.30 examples/s]
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 37767, 53283, 75493, 9554, 17801, 28469, 121589, 29411, 5018, 17327, 794, 4482, 103377, 9554, 33005, 18184, 5232, 3670, 2173, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 21710, 1073, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 26094, 2252, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 2252, 498, 330, 103377, 9554, 33005, 18184, 5232, 21710, 1073, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 6313, 498, 330, 103377, 9554, 33005, 18184, 5232, 26094, 6313, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 6313, 498, 330, 103377, 9554, 33005, 18184, 5232, 56065, 266, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 58904, 498, 330, 103377, 9554, 33005, 18184, 5232, 1816, 266, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 58904, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 9792, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 2658, 40622, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 33498, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 33498, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 33134, 82, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 9164, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 8629, 11, 72718, 28542, 33005, 18184, 25, 58904, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 40563, 258, 9164, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 285, 4581, 1073, 11, 72718, 28542, 33005, 18184, 25, 2050, 11, 105866, 28542, 33005, 18184, 25, 2050, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 23405, 11, 72718, 28542, 33005, 18184, 25, 23405, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 2252, 11, 72718, 28542, 33005, 18184, 25, 2252, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 561, 351, 6313, 11, 72718, 28542, 33005, 18184, 25, 6313, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 4752, 13014, 11, 72718, 28542, 33005, 18184, 25, 9164, 11, 105866, 28542, 33005, 18184, 25, 4681, 498, 330, 103377, 9554, 33005, 18184, 5232, 71, 22502, 11, 72718, 28542, 33005, 18184, 25, 4681, 11, 105866, 28542, 33005, 18184, 25, 4681, 1058, 498, 330, 103377, 9554, 33005, 18184, 5232, 1056, 392, 1058, 1073, 11, 72718, 28542, 33005, 18184, 25, 4681, 1058, 11, 105866, 28542, 33005, 18184, 25, 4681, 1058, 8073, 330, 20606, 794, 4482, 93474, 33005, 25, 6313, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 4222, 518, 364, 1834, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 2050, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 518, 364, 1337, 4181, 11844, 330, 93474, 33005, 25, 9164, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 2386, 518, 364, 13265, 518, 364, 52365, 518, 364, 11789, 518, 364, 30306, 518, 364, 29525, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 58904, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 518, 364, 1337, 4181, 11844, 330, 93474, 33005, 25, 23405, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 2150, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 4681, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 4181, 11844, 330, 93474, 33005, 25, 2252, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 4222, 518, 364, 1834, 518, 364, 11789, 518, 364, 1843, 1213, 518, 364, 2588, 575, 518, 364, 23279, 2656, 518, 364, 38475, 1045, 4181, 11844, 330, 93474, 33005, 25, 4681, 1058, 11, 120610, 80356, 28469, 3349, 681, 307, 518, 364, 1103, 518, 364, 609, 4181, 8, 93110, 37767, 53283, 75493, 16325, 48706, 88852, 20675, 28469, 512, 1530, 18184, 23405, 11, 80356, 2150, 1151, 2878, 369, 13629, 2832, 321, 52999, 1426, 4588, 14172, 2083, 797, 301, 304, 24912, 19818, 6, 9554, 93474, 198, 1530, 18184, 23405, 11, 80356, 2150, 1151, 2878, 369, 20462, 2083, 7595, 288, 304, 23251, 79083, 89, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 38, 383, 1813, 383, 12582, 2194, 62, 31272, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 38, 96091, 1702, 261, 1414, 62, 31272, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 75739, 22811, 1586, 90917, 27062, 7815, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 9688, 1813, 2832, 321, 52999, 1426, 4588, 14172, 2083, 797, 301, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 3692, 1159, 2076, 62, 86147, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
[INFO|configuration_utils.py:673] 2024-11-19 00:34:28,526 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:34:28,528 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:34:28,579 >> loading weights file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:34:28,579 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:34:28,581 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
[INFO|modeling_utils.py:4574] 2024-11-19 00:34:32,679 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:34:32,679 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 00:34:32,684 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 00:34:32,684 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:34:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:34:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,k_proj,gate_proj,down_proj,up_proj,v_proj,q_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:34:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:34:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:34:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,down_proj,gate_proj,o_proj,q_proj,v_proj
11/19/2024 00:34:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:34:33,444 >> Using auto half precision backend
11/19/2024 00:34:33 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
[INFO|trainer.py:2243] 2024-11-19 00:34:34,029 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:34:34,029 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:34:34,029 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:34:34,029 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:34:34,029 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:34:34,029 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:34:34,029 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:34:34,036 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<11:13,  4.52s/it]  1%|▏         | 2/150 [00:08<10:28,  4.24s/it]  2%|▏         | 3/150 [00:12<10:12,  4.17s/it]  3%|▎         | 4/150 [00:16<10:00,  4.12s/it]  3%|▎         | 5/150 [00:20<09:54,  4.10s/it]  4%|▍         | 6/150 [00:24<09:48,  4.09s/it]  5%|▍         | 7/150 [00:28<09:43,  4.08s/it]  5%|▌         | 8/150 [00:32<09:38,  4.08s/it]  6%|▌         | 9/150 [00:37<09:34,  4.08s/it]  7%|▋         | 10/150 [00:41<09:29,  4.07s/it]                                                {'loss': 1.5118, 'grad_norm': 2.9781548976898193, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:41<09:29,  4.07s/it]  7%|▋         | 11/150 [00:45<09:25,  4.07s/it]  8%|▊         | 12/150 [00:49<09:19,  4.06s/it]  9%|▊         | 13/150 [00:53<09:15,  4.05s/it]  9%|▉         | 14/150 [00:57<09:09,  4.04s/it] 10%|█         | 15/150 [01:01<09:06,  4.05s/it] 11%|█         | 16/150 [01:05<09:02,  4.05s/it] 11%|█▏        | 17/150 [01:09<08:59,  4.06s/it] 12%|█▏        | 18/150 [01:13<08:55,  4.05s/it] 13%|█▎        | 19/150 [01:17<08:51,  4.05s/it] 13%|█▎        | 20/150 [01:21<08:46,  4.05s/it]                                                {'loss': 0.6905, 'grad_norm': 2.0850179195404053, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:21<08:46,  4.05s/it] 14%|█▍        | 21/150 [01:25<08:41,  4.04s/it] 15%|█▍        | 22/150 [01:29<08:37,  4.04s/it] 15%|█▌        | 23/150 [01:33<08:33,  4.04s/it] 16%|█▌        | 24/150 [01:37<08:30,  4.05s/it] 17%|█▋        | 25/150 [01:41<08:26,  4.06s/it] 17%|█▋        | 26/150 [01:45<08:22,  4.06s/it] 18%|█▊        | 27/150 [01:49<08:18,  4.05s/it] 19%|█▊        | 28/150 [01:53<08:14,  4.06s/it] 19%|█▉        | 29/150 [01:58<08:10,  4.05s/it] 20%|██        | 30/150 [02:02<08:07,  4.06s/it]                                                {'loss': 0.3846, 'grad_norm': 0.8506960868835449, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:02<08:07,  4.06s/it] 21%|██        | 31/150 [02:06<08:03,  4.06s/it] 21%|██▏       | 32/150 [02:10<07:58,  4.06s/it] 22%|██▏       | 33/150 [02:14<07:55,  4.06s/it] 23%|██▎       | 34/150 [02:18<07:50,  4.06s/it] 23%|██▎       | 35/150 [02:22<07:45,  4.05s/it] 24%|██▍       | 36/150 [02:26<07:41,  4.05s/it] 25%|██▍       | 37/150 [02:30<07:38,  4.05s/it] 25%|██▌       | 38/150 [02:34<07:33,  4.05s/it] 26%|██▌       | 39/150 [02:38<07:28,  4.04s/it] 27%|██▋       | 40/150 [02:42<07:25,  4.05s/it]                                                {'loss': 0.3566, 'grad_norm': 0.7399797439575195, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:42<07:25,  4.05s/it] 27%|██▋       | 41/150 [02:46<07:21,  4.05s/it] 28%|██▊       | 42/150 [02:50<07:16,  4.05s/it] 29%|██▊       | 43/150 [02:54<07:12,  4.04s/it] 29%|██▉       | 44/150 [02:58<07:08,  4.04s/it] 30%|███       | 45/150 [03:02<07:04,  4.05s/it] 31%|███       | 46/150 [03:06<07:00,  4.05s/it] 31%|███▏      | 47/150 [03:10<06:56,  4.04s/it] 32%|███▏      | 48/150 [03:14<06:52,  4.04s/it] 33%|███▎      | 49/150 [03:19<06:48,  4.05s/it] 33%|███▎      | 50/150 [03:23<06:45,  4.05s/it]                                                {'loss': 0.3329, 'grad_norm': 0.6819377541542053, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:23<06:45,  4.05s/it] 34%|███▍      | 51/150 [03:27<06:40,  4.05s/it] 35%|███▍      | 52/150 [03:31<06:36,  4.05s/it] 35%|███▌      | 53/150 [03:35<06:33,  4.06s/it] 36%|███▌      | 54/150 [03:39<06:29,  4.05s/it] 37%|███▋      | 55/150 [03:43<06:25,  4.05s/it] 37%|███▋      | 56/150 [03:47<06:20,  4.05s/it] 38%|███▊      | 57/150 [03:51<06:16,  4.05s/it] 39%|███▊      | 58/150 [03:55<06:12,  4.05s/it] 39%|███▉      | 59/150 [03:59<06:07,  4.04s/it] 40%|████      | 60/150 [04:03<06:04,  4.05s/it]                                                {'loss': 0.2864, 'grad_norm': 0.5508837699890137, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [04:03<06:04,  4.05s/it] 41%|████      | 61/150 [04:07<06:00,  4.05s/it] 41%|████▏     | 62/150 [04:11<05:57,  4.06s/it] 42%|████▏     | 63/150 [04:15<05:53,  4.07s/it] 43%|████▎     | 64/150 [04:19<05:48,  4.06s/it] 43%|████▎     | 65/150 [04:23<05:45,  4.06s/it] 44%|████▍     | 66/150 [04:27<05:41,  4.07s/it] 45%|████▍     | 67/150 [04:32<05:38,  4.08s/it] 45%|████▌     | 68/150 [04:36<05:34,  4.08s/it] 46%|████▌     | 69/150 [04:40<05:29,  4.06s/it] 47%|████▋     | 70/150 [04:44<05:24,  4.06s/it]                                                {'loss': 0.2513, 'grad_norm': 0.6012725234031677, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:44<05:24,  4.06s/it] 47%|████▋     | 71/150 [04:48<05:20,  4.06s/it] 48%|████▊     | 72/150 [04:52<05:17,  4.07s/it] 49%|████▊     | 73/150 [04:56<05:13,  4.07s/it] 49%|████▉     | 74/150 [05:00<05:08,  4.06s/it] 50%|█████     | 75/150 [05:04<05:04,  4.06s/it] 51%|█████     | 76/150 [05:08<05:00,  4.06s/it] 51%|█████▏    | 77/150 [05:12<04:56,  4.06s/it] 52%|█████▏    | 78/150 [05:16<04:51,  4.05s/it] 53%|█████▎    | 79/150 [05:20<04:47,  4.05s/it] 53%|█████▎    | 80/150 [05:24<04:43,  4.05s/it]                                                {'loss': 0.2773, 'grad_norm': 0.7623250484466553, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [05:24<04:43,  4.05s/it] 54%|█████▍    | 81/150 [05:28<04:39,  4.04s/it] 55%|█████▍    | 82/150 [05:32<04:34,  4.04s/it] 55%|█████▌    | 83/150 [05:36<04:31,  4.05s/it] 56%|█████▌    | 84/150 [05:40<04:27,  4.05s/it] 57%|█████▋    | 85/150 [05:45<04:23,  4.06s/it] 57%|█████▋    | 86/150 [05:49<04:19,  4.06s/it] 58%|█████▊    | 87/150 [05:53<04:16,  4.07s/it] 59%|█████▊    | 88/150 [05:57<04:11,  4.06s/it] 59%|█████▉    | 89/150 [06:01<04:07,  4.06s/it] 60%|██████    | 90/150 [06:05<04:02,  4.05s/it]                                                {'loss': 0.2653, 'grad_norm': 0.5671867728233337, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [06:05<04:02,  4.05s/it] 61%|██████    | 91/150 [06:09<03:59,  4.06s/it] 61%|██████▏   | 92/150 [06:13<03:55,  4.05s/it] 62%|██████▏   | 93/150 [06:17<03:50,  4.05s/it] 63%|██████▎   | 94/150 [06:21<03:46,  4.05s/it] 63%|██████▎   | 95/150 [06:25<03:43,  4.07s/it] 64%|██████▍   | 96/150 [06:29<03:40,  4.08s/it] 65%|██████▍   | 97/150 [06:33<03:35,  4.07s/it] 65%|██████▌   | 98/150 [06:37<03:31,  4.07s/it] 66%|██████▌   | 99/150 [06:41<03:27,  4.06s/it] 67%|██████▋   | 100/150 [06:45<03:22,  4.06s/it]                                                 {'loss': 0.2774, 'grad_norm': 0.5719400644302368, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:45<03:22,  4.06s/it] 67%|██████▋   | 101/150 [06:50<03:18,  4.05s/it] 68%|██████▊   | 102/150 [06:54<03:14,  4.05s/it] 69%|██████▊   | 103/150 [06:58<03:10,  4.05s/it] 69%|██████▉   | 104/150 [07:02<03:06,  4.05s/it] 70%|███████   | 105/150 [07:06<03:02,  4.05s/it] 71%|███████   | 106/150 [07:10<02:58,  4.05s/it] 71%|███████▏  | 107/150 [07:14<02:54,  4.05s/it] 72%|███████▏  | 108/150 [07:18<02:50,  4.05s/it] 73%|███████▎  | 109/150 [07:22<02:46,  4.05s/it] 73%|███████▎  | 110/150 [07:26<02:41,  4.05s/it]                                                 {'loss': 0.2506, 'grad_norm': 0.8434846997261047, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [07:26<02:41,  4.05s/it] 74%|███████▍  | 111/150 [07:30<02:37,  4.05s/it] 75%|███████▍  | 112/150 [07:34<02:33,  4.05s/it] 75%|███████▌  | 113/150 [07:38<02:29,  4.05s/it] 76%|███████▌  | 114/150 [07:42<02:25,  4.05s/it] 77%|███████▋  | 115/150 [07:46<02:21,  4.05s/it] 77%|███████▋  | 116/150 [07:50<02:17,  4.05s/it] 78%|███████▊  | 117/150 [07:54<02:14,  4.06s/it] 79%|███████▊  | 118/150 [07:58<02:09,  4.06s/it] 79%|███████▉  | 119/150 [08:02<02:05,  4.06s/it] 80%|████████  | 120/150 [08:06<02:01,  4.05s/it]                                                 {'loss': 0.2598, 'grad_norm': 0.5134290456771851, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [08:06<02:01,  4.05s/it] 81%|████████  | 121/150 [08:11<01:58,  4.07s/it] 81%|████████▏ | 122/150 [08:15<01:53,  4.07s/it] 82%|████████▏ | 123/150 [08:19<01:49,  4.06s/it] 83%|████████▎ | 124/150 [08:23<01:45,  4.06s/it] 83%|████████▎ | 125/150 [08:27<01:41,  4.06s/it] 84%|████████▍ | 126/150 [08:31<01:37,  4.05s/it] 85%|████████▍ | 127/150 [08:35<01:33,  4.06s/it] 85%|████████▌ | 128/150 [08:39<01:29,  4.05s/it] 86%|████████▌ | 129/150 [08:43<01:24,  4.05s/it] 87%|████████▋ | 130/150 [08:47<01:20,  4.04s/it]                                                 {'loss': 0.2171, 'grad_norm': 0.4558512568473816, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:47<01:20,  4.04s/it] 87%|████████▋ | 131/150 [08:51<01:16,  4.05s/it] 88%|████████▊ | 132/150 [08:55<01:12,  4.05s/it] 89%|████████▊ | 133/150 [08:59<01:08,  4.05s/it] 89%|████████▉ | 134/150 [09:03<01:04,  4.05s/it] 90%|█████████ | 135/150 [09:07<01:00,  4.05s/it] 91%|█████████ | 136/150 [09:11<00:56,  4.04s/it] 91%|█████████▏| 137/150 [09:15<00:52,  4.04s/it] 92%|█████████▏| 138/150 [09:19<00:48,  4.04s/it] 93%|█████████▎| 139/150 [09:23<00:44,  4.04s/it] 93%|█████████▎| 140/150 [09:28<00:40,  4.05s/it]                                                 {'loss': 0.2433, 'grad_norm': 0.69736248254776, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [09:28<00:40,  4.05s/it] 94%|█████████▍| 141/150 [09:32<00:36,  4.04s/it] 95%|█████████▍| 142/150 [09:36<00:32,  4.05s/it] 95%|█████████▌| 143/150 [09:40<00:28,  4.04s/it] 96%|█████████▌| 144/150 [09:44<00:24,  4.04s/it] 97%|█████████▋| 145/150 [09:48<00:20,  4.05s/it] 97%|█████████▋| 146/150 [09:52<00:16,  4.05s/it] 98%|█████████▊| 147/150 [09:56<00:12,  4.06s/it] 99%|█████████▊| 148/150 [10:00<00:08,  4.05s/it] 99%|█████████▉| 149/150 [10:04<00:04,  4.04s/it]100%|██████████| 150/150 [10:08<00:00,  4.05s/it]                                                 {'loss': 0.2311, 'grad_norm': 0.6337286233901978, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [10:08<00:00,  4.05s/it][INFO|trainer.py:3705] 2024-11-19 00:44:43,349 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:44:43,390 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:44:43,391 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:44:43,580 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:44:43,580 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 00:44:44,366 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 610.3293, 'train_samples_per_second': 3.962, 'train_steps_per_second': 0.246, 'train_loss': 0.38906673590342206, 'epoch': 2.98}
100%|██████████| 150/150 [10:09<00:00,  4.05s/it]100%|██████████| 150/150 [10:09<00:00,  4.06s/it]
[INFO|trainer.py:3705] 2024-11-19 00:44:44,369 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:44:44,408 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:44:44,409 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:44:44,586 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:44:44,586 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9777
  total_flos               = 102174938GF
  train_loss               =      0.3891
  train_runtime            =  0:10:10.32
  train_samples_per_second =       3.962
  train_steps_per_second   =       0.246
Figure saved at: saves/Meta-Llama-3.1-8B-Instruct/my_prompt/lora/sft/training_loss.png
11/19/2024 00:44:45 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:44:45 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:44:45,077 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:44:45,078 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:44:45,078 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 15.27it/s]  9%|▉         | 4/45 [00:00<00:04,  9.25it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.40it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.19it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.04it/s] 20%|██        | 9/45 [00:01<00:04,  7.78it/s] 22%|██▏       | 10/45 [00:01<00:04,  7.70it/s] 24%|██▍       | 11/45 [00:01<00:04,  7.62it/s] 27%|██▋       | 12/45 [00:01<00:04,  7.62it/s] 29%|██▉       | 13/45 [00:01<00:04,  7.63it/s] 31%|███       | 14/45 [00:01<00:04,  7.54it/s] 33%|███▎      | 15/45 [00:01<00:04,  7.46it/s] 36%|███▌      | 16/45 [00:02<00:03,  7.51it/s] 38%|███▊      | 17/45 [00:02<00:03,  7.55it/s] 40%|████      | 18/45 [00:02<00:03,  7.57it/s] 42%|████▏     | 19/45 [00:02<00:03,  7.48it/s] 44%|████▍     | 20/45 [00:02<00:03,  7.39it/s] 47%|████▋     | 21/45 [00:02<00:03,  7.44it/s] 49%|████▉     | 22/45 [00:02<00:03,  7.46it/s] 51%|█████     | 23/45 [00:02<00:02,  7.52it/s] 53%|█████▎    | 24/45 [00:03<00:02,  7.55it/s] 56%|█████▌    | 25/45 [00:03<00:02,  7.57it/s] 58%|█████▊    | 26/45 [00:03<00:02,  7.59it/s] 60%|██████    | 27/45 [00:03<00:02,  7.60it/s] 62%|██████▏   | 28/45 [00:03<00:02,  7.61it/s] 64%|██████▍   | 29/45 [00:03<00:02,  7.61it/s] 67%|██████▋   | 30/45 [00:03<00:01,  7.57it/s] 69%|██████▉   | 31/45 [00:04<00:01,  7.48it/s] 71%|███████   | 32/45 [00:04<00:01,  7.42it/s] 73%|███████▎  | 33/45 [00:04<00:01,  7.43it/s] 76%|███████▌  | 34/45 [00:04<00:01,  7.41it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.48it/s] 80%|████████  | 36/45 [00:04<00:01,  7.54it/s] 82%|████████▏ | 37/45 [00:04<00:01,  7.49it/s] 84%|████████▍ | 38/45 [00:04<00:00,  7.53it/s] 87%|████████▋ | 39/45 [00:05<00:00,  7.48it/s] 89%|████████▉ | 40/45 [00:05<00:00,  7.46it/s] 91%|█████████ | 41/45 [00:05<00:00,  7.40it/s] 93%|█████████▎| 42/45 [00:05<00:00,  7.47it/s] 96%|█████████▌| 43/45 [00:05<00:00,  7.52it/s] 98%|█████████▊| 44/45 [00:05<00:00,  7.55it/s]100%|██████████| 45/45 [00:05<00:00,  7.58it/s]100%|██████████| 45/45 [00:05<00:00,  7.67it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.2229
  eval_runtime            = 0:00:06.02
  eval_samples_per_second =     14.944
  eval_steps_per_second   =      7.472
[INFO|modelcard.py:449] 2024-11-19 00:44:51,101 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:45:08,333] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:45:12 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22505
[2024-11-19 00:45:14,474] torch.distributed.run: [WARNING] 
[2024-11-19 00:45:14,474] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:45:14,474] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:45:14,474] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:45:21,867] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:45:22,102] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:45:23 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:45:23 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/19/2024 00:45:23 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:45:23 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:45:23,343 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:45:23,345 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:23,346 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:23,346 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:23,346 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:23,347 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:23,347 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:45:23,999 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:45:24,005 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:45:24,007 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:24,009 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:24,009 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:24,009 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:24,009 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:45:24,009 >> loading file tokenizer_config.json
11/19/2024 00:45:24 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:45:24 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:45:24 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:45:24,631 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:45:24 - WARNING - llamafactory.model.loader - Processor was not found: 'LlamaConfig' object has no attribute 'vision_config'.
11/19/2024 00:45:24 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
11/19/2024 00:45:24 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
11/19/2024 00:45:24 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 373.97 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2960.59 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:45:28 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:24, 34.29 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:11, 66.09 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:02<00:07, 93.34 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:02<00:05, 115.93 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:04, 133.59 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:03<00:03, 148.38 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:03<00:03, 160.74 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:03<00:02, 183.72 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:04<00:01, 191.28 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:04<00:01, 198.62 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:04<00:00, 224.68 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:04<00:00, 216.07 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:05<00:00, 219.26 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:05<00:00, 229.05 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:05<00:00, 243.53 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:05<00:00, 158.26 examples/s]
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 37767, 53283, 75493, 16325, 48706, 88852, 20675, 28469, 512, 1530, 18184, 23405, 11, 80356, 2150, 1151, 2878, 369, 13629, 2832, 321, 52999, 1426, 4588, 14172, 2083, 797, 301, 304, 24912, 19818, 6, 9554, 93474, 198, 1530, 18184, 23405, 11, 80356, 2150, 1151, 2878, 369, 20462, 2083, 7595, 288, 304, 23251, 79083, 89, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 38, 383, 1813, 383, 12582, 2194, 62, 31272, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 38, 96091, 1702, 261, 1414, 62, 31272, 6, 9554, 93474, 198, 1530, 18184, 58904, 11, 80356, 609, 1151, 75739, 22811, 1586, 90917, 27062, 7815, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 9688, 1813, 2832, 321, 52999, 1426, 4588, 14172, 2083, 797, 301, 6, 9554, 93474, 198, 1530, 18184, 4681, 11, 80356, 609, 1151, 3692, 1159, 2076, 62, 86147, 21541, 6, 9554, 93474, 271, 15225, 45163, 88852, 109683, 120074, 111602, 115605, 18184, 33764, 76982, 75493, 9554, 57815, 29182, 52254, 512, 33976, 93233, 33764, 40052, 1426, 81, 1814, 292, 2083, 38221, 99750, 106236, 110999, 104123, 43323, 9554, 94668, 5486, 34171, 64022, 5486, 120074, 34208, 116122, 105514, 91495, 32626, 31944, 62855, 9554, 3222, 34208, 31091, 3922, 60251, 60979, 31944, 62855, 31091, 75761, 128009, 128006, 78191, 128007, 271, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|eot_id|><|start_header_id|>assistant<|end_header_id|>

match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6481, 320, 77, 16, 25, 9164, 7435, 58, 81, 16, 25, 4752, 13014, 6428, 7, 77, 17, 25, 4681, 8, 220, 1405, 308, 17, 2710, 1151, 40052, 1426, 81, 1814, 292, 2083, 38221, 6, 471, 308, 17, 7464, 11, 308, 16, 10048, 11, 308, 17, 2710, 11, 308, 16, 73737, 11, 308, 16, 49437, 11, 308, 16, 32733, 2015, 555, 308, 17, 2710, 26, 128009]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|eot_id|>
[INFO|configuration_utils.py:673] 2024-11-19 00:45:35,133 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:45:35,135 >> Model config LlamaConfig {
  "_name_or_path": "/home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:45:35,190 >> loading weights file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:45:35,191 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:45:35,195 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:45:39 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:45:39 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,k_proj,v_proj,down_proj,gate_proj,up_proj,q_proj
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/it]11/19/2024 00:45:39 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
[INFO|modeling_utils.py:4574] 2024-11-19 00:45:39,940 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:45:39,940 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 00:45:39,944 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 00:45:39,945 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:45:39 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:45:39 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:45:39 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,up_proj,down_proj,q_proj,k_proj,gate_proj,o_proj
11/19/2024 00:45:40 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:45:40,728 >> Using auto half precision backend
[INFO|trainer.py:2243] 2024-11-19 00:45:41,260 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:45:41,260 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:45:41,261 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:45:41,261 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:45:41,261 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:45:41,261 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:45:41,261 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:45:41,268 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:06,  4.07s/it]  1%|▏         | 2/150 [00:07<09:29,  3.85s/it]  2%|▏         | 3/150 [00:11<09:10,  3.75s/it]  3%|▎         | 4/150 [00:15<08:59,  3.70s/it]  3%|▎         | 5/150 [00:18<08:50,  3.66s/it]  4%|▍         | 6/150 [00:22<08:45,  3.65s/it]  5%|▍         | 7/150 [00:25<08:40,  3.64s/it]  5%|▌         | 8/150 [00:29<08:34,  3.62s/it]  6%|▌         | 9/150 [00:33<08:28,  3.61s/it]  7%|▋         | 10/150 [00:36<08:20,  3.58s/it]                                                {'loss': 1.2855, 'grad_norm': 2.004826068878174, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:36<08:20,  3.58s/it]  7%|▋         | 11/150 [00:40<08:14,  3.56s/it]  8%|▊         | 12/150 [00:43<08:08,  3.54s/it]  9%|▊         | 13/150 [00:47<08:04,  3.54s/it]  9%|▉         | 14/150 [00:50<08:01,  3.54s/it] 10%|█         | 15/150 [00:54<07:58,  3.54s/it] 11%|█         | 16/150 [00:57<07:54,  3.54s/it] 11%|█▏        | 17/150 [01:01<07:51,  3.55s/it] 12%|█▏        | 18/150 [01:04<07:47,  3.54s/it] 13%|█▎        | 19/150 [01:08<07:44,  3.55s/it] 13%|█▎        | 20/150 [01:11<07:41,  3.55s/it]                                                {'loss': 0.4008, 'grad_norm': 1.6665712594985962, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:11<07:41,  3.55s/it] 14%|█▍        | 21/150 [01:15<07:37,  3.55s/it] 15%|█▍        | 22/150 [01:18<07:34,  3.55s/it] 15%|█▌        | 23/150 [01:22<07:29,  3.54s/it] 16%|█▌        | 24/150 [01:26<07:26,  3.54s/it] 17%|█▋        | 25/150 [01:29<07:22,  3.54s/it] 17%|█▋        | 26/150 [01:33<07:19,  3.54s/it] 18%|█▊        | 27/150 [01:36<07:16,  3.55s/it] 19%|█▊        | 28/150 [01:40<07:14,  3.56s/it] 19%|█▉        | 29/150 [01:43<07:10,  3.56s/it] 20%|██        | 30/150 [01:47<07:05,  3.55s/it]                                                {'loss': 0.1608, 'grad_norm': 1.02173912525177, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:47<07:05,  3.55s/it] 21%|██        | 31/150 [01:50<07:02,  3.55s/it] 21%|██▏       | 32/150 [01:54<06:58,  3.54s/it] 22%|██▏       | 33/150 [01:57<06:54,  3.54s/it] 23%|██▎       | 34/150 [02:01<06:51,  3.55s/it] 23%|██▎       | 35/150 [02:05<06:47,  3.54s/it] 24%|██▍       | 36/150 [02:08<06:43,  3.54s/it] 25%|██▍       | 37/150 [02:12<06:40,  3.54s/it] 25%|██▌       | 38/150 [02:15<06:36,  3.54s/it] 26%|██▌       | 39/150 [02:19<06:33,  3.55s/it] 27%|██▋       | 40/150 [02:22<06:28,  3.53s/it]                                                {'loss': 0.1064, 'grad_norm': 0.6039837002754211, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:22<06:28,  3.53s/it] 27%|██▋       | 41/150 [02:26<06:25,  3.54s/it] 28%|██▊       | 42/150 [02:29<06:22,  3.54s/it] 29%|██▊       | 43/150 [02:33<06:18,  3.54s/it] 29%|██▉       | 44/150 [02:36<06:15,  3.54s/it] 30%|███       | 45/150 [02:40<06:11,  3.54s/it] 31%|███       | 46/150 [02:44<06:08,  3.55s/it] 31%|███▏      | 47/150 [02:47<06:05,  3.55s/it] 32%|███▏      | 48/150 [02:51<06:02,  3.55s/it] 33%|███▎      | 49/150 [02:54<05:58,  3.55s/it] 33%|███▎      | 50/150 [02:58<05:55,  3.56s/it]                                                {'loss': 0.0784, 'grad_norm': 0.39704209566116333, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [02:58<05:55,  3.56s/it] 34%|███▍      | 51/150 [03:01<05:53,  3.57s/it] 35%|███▍      | 52/150 [03:05<05:51,  3.59s/it] 35%|███▌      | 53/150 [03:09<05:50,  3.61s/it] 36%|███▌      | 54/150 [03:12<05:45,  3.59s/it] 37%|███▋      | 55/150 [03:16<05:40,  3.58s/it] 37%|███▋      | 56/150 [03:19<05:35,  3.57s/it] 38%|███▊      | 57/150 [03:23<05:32,  3.58s/it] 39%|███▊      | 58/150 [03:27<05:29,  3.58s/it] 39%|███▉      | 59/150 [03:30<05:25,  3.57s/it] 40%|████      | 60/150 [03:34<05:21,  3.57s/it]                                                {'loss': 0.0628, 'grad_norm': 0.3736039400100708, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:34<05:21,  3.57s/it] 41%|████      | 61/150 [03:37<05:17,  3.57s/it] 41%|████▏     | 62/150 [03:41<05:14,  3.57s/it] 42%|████▏     | 63/150 [03:44<05:10,  3.57s/it] 43%|████▎     | 64/150 [03:48<05:07,  3.58s/it] 43%|████▎     | 65/150 [03:52<05:04,  3.58s/it] 44%|████▍     | 66/150 [03:55<05:00,  3.58s/it] 45%|████▍     | 67/150 [03:59<04:57,  3.58s/it] 45%|████▌     | 68/150 [04:02<04:53,  3.58s/it] 46%|████▌     | 69/150 [04:06<04:50,  3.59s/it] 47%|████▋     | 70/150 [04:09<04:46,  3.58s/it]                                                {'loss': 0.0545, 'grad_norm': 0.29710906744003296, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:09<04:46,  3.58s/it] 47%|████▋     | 71/150 [04:13<04:42,  3.57s/it] 48%|████▊     | 72/150 [04:17<04:37,  3.56s/it] 49%|████▊     | 73/150 [04:20<04:34,  3.56s/it] 49%|████▉     | 74/150 [04:24<04:30,  3.56s/it] 50%|█████     | 75/150 [04:27<04:27,  3.57s/it] 51%|█████     | 76/150 [04:31<04:24,  3.58s/it] 51%|█████▏    | 77/150 [04:34<04:21,  3.58s/it] 52%|█████▏    | 78/150 [04:38<04:18,  3.59s/it] 53%|█████▎    | 79/150 [04:42<04:14,  3.59s/it] 53%|█████▎    | 80/150 [04:45<04:11,  3.59s/it]                                                {'loss': 0.0538, 'grad_norm': 0.35832008719444275, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:45<04:11,  3.59s/it] 54%|█████▍    | 81/150 [04:49<04:08,  3.60s/it] 55%|█████▍    | 82/150 [04:52<04:04,  3.59s/it] 55%|█████▌    | 83/150 [04:56<04:00,  3.59s/it] 56%|█████▌    | 84/150 [05:00<03:56,  3.58s/it] 57%|█████▋    | 85/150 [05:03<03:52,  3.58s/it] 57%|█████▋    | 86/150 [05:07<03:48,  3.58s/it] 58%|█████▊    | 87/150 [05:10<03:45,  3.57s/it] 59%|█████▊    | 88/150 [05:14<03:42,  3.58s/it] 59%|█████▉    | 89/150 [05:17<03:38,  3.59s/it] 60%|██████    | 90/150 [05:21<03:34,  3.58s/it]                                                {'loss': 0.0637, 'grad_norm': 0.3171551525592804, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:21<03:34,  3.58s/it] 61%|██████    | 91/150 [05:25<03:31,  3.58s/it] 61%|██████▏   | 92/150 [05:28<03:27,  3.58s/it] 62%|██████▏   | 93/150 [05:32<03:23,  3.57s/it] 63%|██████▎   | 94/150 [05:35<03:19,  3.56s/it] 63%|██████▎   | 95/150 [05:39<03:14,  3.54s/it] 64%|██████▍   | 96/150 [05:42<03:10,  3.52s/it] 65%|██████▍   | 97/150 [05:46<03:06,  3.52s/it] 65%|██████▌   | 98/150 [05:49<03:02,  3.52s/it] 66%|██████▌   | 99/150 [05:53<02:59,  3.53s/it] 67%|██████▋   | 100/150 [05:56<02:56,  3.54s/it]                                                 {'loss': 0.0593, 'grad_norm': 0.3060782849788666, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [05:56<02:56,  3.54s/it] 67%|██████▋   | 101/150 [06:00<02:53,  3.55s/it] 68%|██████▊   | 102/150 [06:04<02:50,  3.55s/it] 69%|██████▊   | 103/150 [06:07<02:46,  3.55s/it] 69%|██████▉   | 104/150 [06:11<02:43,  3.55s/it] 70%|███████   | 105/150 [06:14<02:39,  3.54s/it] 71%|███████   | 106/150 [06:18<02:35,  3.54s/it] 71%|███████▏  | 107/150 [06:21<02:32,  3.54s/it] 72%|███████▏  | 108/150 [06:25<02:28,  3.55s/it] 73%|███████▎  | 109/150 [06:28<02:25,  3.56s/it] 73%|███████▎  | 110/150 [06:32<02:22,  3.56s/it]                                                 {'loss': 0.05, 'grad_norm': 0.3421141803264618, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:32<02:22,  3.56s/it] 74%|███████▍  | 111/150 [06:36<02:19,  3.57s/it] 75%|███████▍  | 112/150 [06:39<02:15,  3.58s/it] 75%|███████▌  | 113/150 [06:43<02:12,  3.58s/it] 76%|███████▌  | 114/150 [06:46<02:08,  3.58s/it] 77%|███████▋  | 115/150 [06:50<02:05,  3.58s/it] 77%|███████▋  | 116/150 [06:53<02:01,  3.58s/it] 78%|███████▊  | 117/150 [06:57<01:58,  3.58s/it] 79%|███████▊  | 118/150 [07:01<01:54,  3.58s/it] 79%|███████▉  | 119/150 [07:04<01:50,  3.58s/it] 80%|████████  | 120/150 [07:08<01:47,  3.58s/it]                                                 {'loss': 0.0543, 'grad_norm': 0.3551461100578308, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:08<01:47,  3.58s/it] 81%|████████  | 121/150 [07:11<01:43,  3.58s/it] 81%|████████▏ | 122/150 [07:15<01:40,  3.57s/it] 82%|████████▏ | 123/150 [07:18<01:36,  3.56s/it] 83%|████████▎ | 124/150 [07:22<01:32,  3.57s/it] 83%|████████▎ | 125/150 [07:26<01:29,  3.57s/it] 84%|████████▍ | 126/150 [07:29<01:25,  3.56s/it] 85%|████████▍ | 127/150 [07:33<01:22,  3.57s/it] 85%|████████▌ | 128/150 [07:36<01:18,  3.58s/it] 86%|████████▌ | 129/150 [07:40<01:15,  3.59s/it] 87%|████████▋ | 130/150 [07:43<01:11,  3.58s/it]                                                 {'loss': 0.0363, 'grad_norm': 0.18873973190784454, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [07:43<01:11,  3.58s/it] 87%|████████▋ | 131/150 [07:47<01:07,  3.57s/it] 88%|████████▊ | 132/150 [07:51<01:04,  3.56s/it] 89%|████████▊ | 133/150 [07:54<01:00,  3.57s/it] 89%|████████▉ | 134/150 [07:58<00:57,  3.57s/it] 90%|█████████ | 135/150 [08:01<00:53,  3.58s/it] 91%|█████████ | 136/150 [08:05<00:50,  3.58s/it] 91%|█████████▏| 137/150 [08:09<00:46,  3.58s/it] 92%|█████████▏| 138/150 [08:12<00:43,  3.58s/it] 93%|█████████▎| 139/150 [08:16<00:39,  3.58s/it] 93%|█████████▎| 140/150 [08:19<00:35,  3.58s/it]                                                 {'loss': 0.038, 'grad_norm': 0.47406286001205444, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:19<00:35,  3.58s/it] 94%|█████████▍| 141/150 [08:23<00:32,  3.58s/it] 95%|█████████▍| 142/150 [08:26<00:28,  3.57s/it] 95%|█████████▌| 143/150 [08:30<00:24,  3.57s/it] 96%|█████████▌| 144/150 [08:34<00:21,  3.57s/it] 97%|█████████▋| 145/150 [08:37<00:17,  3.56s/it] 97%|█████████▋| 146/150 [08:41<00:14,  3.56s/it] 98%|█████████▊| 147/150 [08:44<00:10,  3.56s/it] 99%|█████████▊| 148/150 [08:48<00:07,  3.56s/it] 99%|█████████▉| 149/150 [08:51<00:03,  3.56s/it]100%|██████████| 150/150 [08:55<00:00,  3.56s/it]                                                 {'loss': 0.0519, 'grad_norm': 0.2052914798259735, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [08:55<00:00,  3.56s/it][INFO|trainer.py:3705] 2024-11-19 00:54:37,409 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:54:37,447 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:54:37,448 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:54:37,634 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:54:37,635 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 00:54:38,376 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 537.1086, 'train_samples_per_second': 4.502, 'train_steps_per_second': 0.279, 'train_loss': 0.1704471190770467, 'epoch': 2.98}
100%|██████████| 150/150 [08:56<00:00,  3.56s/it]100%|██████████| 150/150 [08:56<00:00,  3.58s/it]
[INFO|trainer.py:3705] 2024-11-19 00:54:38,379 >> Saving model checkpoint to saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:54:38,413 >> loading configuration file /home/work/liuytest/demo/LLama3.1/Meta-Llama-3.1-8B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:54:38,414 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:54:38,584 >> tokenizer config file saved in saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:54:38,585 >> Special tokens file saved in saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 27835857GF
  train_loss               =     0.1704
  train_runtime            = 0:08:57.10
  train_samples_per_second =      4.502
  train_steps_per_second   =      0.279
Figure saved at: saves/Meta-Llama-3.1-8B-Instruct/vector_prompt/lora/sft/training_loss.png
11/19/2024 00:54:39 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:54:39 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:54:39,042 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:54:39,042 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:54:39,043 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 18.18it/s]  9%|▉         | 4/45 [00:00<00:03, 11.91it/s] 13%|█▎        | 6/45 [00:00<00:03, 10.88it/s] 18%|█▊        | 8/45 [00:00<00:03, 10.49it/s] 22%|██▏       | 10/45 [00:00<00:03, 10.27it/s] 27%|██▋       | 12/45 [00:01<00:03, 10.17it/s] 31%|███       | 14/45 [00:01<00:03, 10.13it/s] 36%|███▌      | 16/45 [00:01<00:02, 10.09it/s] 40%|████      | 18/45 [00:01<00:02, 10.03it/s] 44%|████▍     | 20/45 [00:01<00:02, 10.00it/s] 49%|████▉     | 22/45 [00:02<00:02,  9.98it/s] 53%|█████▎    | 24/45 [00:02<00:02,  9.93it/s] 56%|█████▌    | 25/45 [00:02<00:02,  9.90it/s] 58%|█████▊    | 26/45 [00:02<00:01,  9.88it/s] 60%|██████    | 27/45 [00:02<00:01,  9.88it/s] 62%|██████▏   | 28/45 [00:02<00:01,  9.87it/s] 64%|██████▍   | 29/45 [00:02<00:01,  9.87it/s] 67%|██████▋   | 30/45 [00:02<00:01,  9.88it/s] 69%|██████▉   | 31/45 [00:03<00:01,  9.81it/s] 71%|███████   | 32/45 [00:03<00:01,  9.83it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.85it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.89it/s] 80%|████████  | 36/45 [00:03<00:00,  9.87it/s] 82%|████████▏ | 37/45 [00:03<00:00,  9.85it/s] 84%|████████▍ | 38/45 [00:03<00:00,  9.84it/s] 87%|████████▋ | 39/45 [00:03<00:00,  9.84it/s] 91%|█████████ | 41/45 [00:04<00:00,  9.91it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.89it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.88it/s]100%|██████████| 45/45 [00:04<00:00,  9.96it/s]100%|██████████| 45/45 [00:04<00:00, 10.09it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0662
  eval_runtime            = 0:00:04.58
  eval_samples_per_second =     19.635
  eval_steps_per_second   =      9.817
[INFO|modelcard.py:449] 2024-11-19 00:54:43,627 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:55:01,418] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:55:05 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29390
[2024-11-19 00:55:07,409] torch.distributed.run: [WARNING] 
[2024-11-19 00:55:07,409] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:55:07,409] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:55:07,409] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:55:14,863] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:55:14,964] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:55:16 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:55:16 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/19/2024 00:55:16 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:55:16 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:55:16,140 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:55:16,142 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,144 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:55:16,616 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:55:16,617 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:55:16,618 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:55:16,619 >> loading file tokenizer_config.json
11/19/2024 00:55:17 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:55:17,100 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:55:17 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 00:55:17 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/19/2024 00:55:17 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/19/2024 00:55:17 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 395.58 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2977.88 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:55:21 - INFO - llamafactory.data.loader - Loading dataset ldbc_normal_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:16, 49.48 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 96.53 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 139.53 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 170.55 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:03, 182.02 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:01, 271.70 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 255.99 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 239.61 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:03<00:01, 189.67 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:03<00:01, 208.68 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 237.28 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 273.28 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 299.84 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 322.25 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 310.45 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 209.92 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14880, 44063, 87752, 99795, 102064, 105359, 17714, 28029, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 101395, 104057, 90395, 31526, 105151, 9370, 3144, 33108, 29991, 3837, 59151, 59879, 105151, 29991, 74661, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|im_end|>
<|im_start|>assistant
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-19 00:55:26,284 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:55:26,286 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:55:26,339 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:55:26,340 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:55:26,343 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
[INFO|modeling_utils.py:4574] 2024-11-19 00:55:31,210 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:55:31,210 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:55:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
[INFO|configuration_utils.py:1052] 2024-11-19 00:55:31,226 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
11/19/2024 00:55:31 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
[INFO|configuration_utils.py:1099] 2024-11-19 00:55:31,227 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,down_proj,v_proj,o_proj,k_proj,gate_proj,up_proj
11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:55:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:55:31 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:55:31 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,k_proj,gate_proj,o_proj,down_proj,up_proj
11/19/2024 00:55:31 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
11/19/2024 00:55:31 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:55:31,929 >> Using auto half precision backend
[INFO|trainer.py:2243] 2024-11-19 00:55:32,443 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:55:32,443 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:55:32,443 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:55:32,443 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:55:32,443 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:55:32,444 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:55:32,444 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:55:32,450 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:03<08:59,  3.62s/it]  1%|▏         | 2/150 [00:06<08:25,  3.42s/it]  2%|▏         | 3/150 [00:10<08:04,  3.30s/it]  3%|▎         | 4/150 [00:13<07:52,  3.24s/it]  3%|▎         | 5/150 [00:16<07:44,  3.21s/it]  4%|▍         | 6/150 [00:19<07:39,  3.19s/it]  5%|▍         | 7/150 [00:22<07:34,  3.18s/it]  5%|▌         | 8/150 [00:25<07:30,  3.17s/it]  6%|▌         | 9/150 [00:28<07:26,  3.17s/it]  7%|▋         | 10/150 [00:32<07:22,  3.16s/it]                                                {'loss': 2.3884, 'grad_norm': 3.391977310180664, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:32<07:22,  3.16s/it]  7%|▋         | 11/150 [00:35<07:19,  3.16s/it]  8%|▊         | 12/150 [00:38<07:15,  3.15s/it][2024-11-19 00:56:12,965] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGHUP death signal, shutting down workers
[2024-11-19 00:56:12,966] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3877959 closing signal SIGHUP
[2024-11-19 00:56:12,966] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3877960 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/liuyang/miniconda3/envs/pytorch21/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 877, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3877737 got signal: 1
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:56:25,517] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:56:29 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22492
[2024-11-19 00:56:31,976] torch.distributed.run: [WARNING] 
[2024-11-19 00:56:31,976] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 00:56:31,976] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 00:56:31,976] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 00:56:39,041] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 00:56:39,044] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 00:56:40 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:56:40 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 00:56:40,116 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:56:40,117 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,119 >> loading file tokenizer_config.json
11/19/2024 00:56:40 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 00:56:40 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:56:40,589 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 00:56:40,590 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:56:40,591 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 00:56:40,592 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 00:56:41,067 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 00:56:41 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 00:56:41 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/19/2024 00:56:41 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
11/19/2024 00:56:41 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 00:56:41 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 420.09 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3083.77 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 00:56:44 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:19, 43.14 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:09, 86.25 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 185.02 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:02, 210.19 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 230.08 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 194.35 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 235.71 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:03<00:01, 248.18 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 318.03 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 285.43 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 337.53 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 328.27 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 224.31 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 14880, 44063, 87752, 99795, 102064, 105359, 17714, 28029, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 101395, 104057, 90395, 31526, 105151, 9370, 3144, 33108, 29991, 3837, 59151, 59879, 105151, 29991, 74661, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
请将以下自然语言转换为图数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|im_end|>
<|im_start|>assistant
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-19 00:56:49,619 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:56:49,621 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-19 00:56:49,671 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 00:56:49,671 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 00:56:49,673 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
[INFO|modeling_utils.py:4574] 2024-11-19 00:56:54,149 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 00:56:54,149 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 00:56:54,153 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 00:56:54,154 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:56:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:56:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,up_proj,down_proj,gate_proj,k_proj,q_proj,v_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 00:56:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 00:56:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 00:56:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,up_proj,gate_proj,k_proj,down_proj,v_proj,o_proj
11/19/2024 00:56:54 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 00:56:54,881 >> Using auto half precision backend
11/19/2024 00:56:55 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-19 00:56:55,580 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 00:56:55,580 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 00:56:55,580 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 00:56:55,580 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 00:56:55,580 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 00:56:55,580 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 00:56:55,580 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 00:56:55,587 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:03<09:53,  3.98s/it]  1%|▏         | 2/150 [00:07<09:15,  3.76s/it]  2%|▏         | 3/150 [00:11<09:01,  3.69s/it]  3%|▎         | 4/150 [00:14<08:50,  3.63s/it]  3%|▎         | 5/150 [00:18<08:43,  3.61s/it]  4%|▍         | 6/150 [00:21<08:41,  3.62s/it]  5%|▍         | 7/150 [00:25<08:36,  3.61s/it]  5%|▌         | 8/150 [00:29<08:29,  3.59s/it]  6%|▌         | 9/150 [00:32<08:25,  3.58s/it]  7%|▋         | 10/150 [00:36<08:21,  3.58s/it]                                                {'loss': 1.5389, 'grad_norm': 2.5251011848449707, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:36<08:21,  3.58s/it]  7%|▋         | 11/150 [00:39<08:20,  3.60s/it]  8%|▊         | 12/150 [00:43<08:16,  3.60s/it]  9%|▊         | 13/150 [00:47<08:14,  3.61s/it]  9%|▉         | 14/150 [00:50<08:09,  3.60s/it] 10%|█         | 15/150 [00:54<08:04,  3.59s/it] 11%|█         | 16/150 [00:57<07:59,  3.58s/it] 11%|█▏        | 17/150 [01:01<07:54,  3.57s/it] 12%|█▏        | 18/150 [01:04<07:51,  3.57s/it] 13%|█▎        | 19/150 [01:08<07:46,  3.56s/it] 13%|█▎        | 20/150 [01:12<07:44,  3.57s/it]                                                {'loss': 0.4595, 'grad_norm': 0.7296004891395569, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:12<07:44,  3.57s/it] 14%|█▍        | 21/150 [01:15<07:42,  3.59s/it] 15%|█▍        | 22/150 [01:19<07:38,  3.58s/it] 15%|█▌        | 23/150 [01:22<07:35,  3.58s/it] 16%|█▌        | 24/150 [01:26<07:29,  3.57s/it] 17%|█▋        | 25/150 [01:29<07:25,  3.57s/it] 17%|█▋        | 26/150 [01:33<07:21,  3.56s/it] 18%|█▊        | 27/150 [01:37<07:17,  3.56s/it] 19%|█▊        | 28/150 [01:40<07:14,  3.56s/it] 19%|█▉        | 29/150 [01:44<07:12,  3.58s/it] 20%|██        | 30/150 [01:47<07:10,  3.59s/it]                                                {'loss': 0.1561, 'grad_norm': 0.46732866764068604, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:47<07:10,  3.59s/it] 21%|██        | 31/150 [01:51<07:06,  3.58s/it] 21%|██▏       | 32/150 [01:54<07:01,  3.57s/it] 22%|██▏       | 33/150 [01:58<06:57,  3.57s/it] 23%|██▎       | 34/150 [02:02<06:55,  3.58s/it] 23%|██▎       | 35/150 [02:05<06:51,  3.58s/it] 24%|██▍       | 36/150 [02:09<06:47,  3.57s/it] 25%|██▍       | 37/150 [02:12<06:43,  3.57s/it] 25%|██▌       | 38/150 [02:16<06:40,  3.57s/it] 26%|██▌       | 39/150 [02:19<06:36,  3.57s/it] 27%|██▋       | 40/150 [02:23<06:31,  3.56s/it]                                                {'loss': 0.1154, 'grad_norm': 0.501385509967804, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:23<06:31,  3.56s/it] 27%|██▋       | 41/150 [02:27<06:27,  3.56s/it] 28%|██▊       | 42/150 [02:30<06:23,  3.55s/it] 29%|██▊       | 43/150 [02:34<06:21,  3.57s/it] 29%|██▉       | 44/150 [02:37<06:16,  3.55s/it] 30%|███       | 45/150 [02:41<06:15,  3.58s/it] 31%|███       | 46/150 [02:44<06:12,  3.59s/it] 31%|███▏      | 47/150 [02:48<06:09,  3.59s/it] 32%|███▏      | 48/150 [02:52<06:04,  3.57s/it] 33%|███▎      | 49/150 [02:55<05:58,  3.55s/it] 33%|███▎      | 50/150 [02:59<05:56,  3.56s/it]                                                {'loss': 0.0834, 'grad_norm': 0.37336814403533936, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [02:59<05:56,  3.56s/it] 34%|███▍      | 51/150 [03:02<05:51,  3.55s/it] 35%|███▍      | 52/150 [03:06<05:49,  3.56s/it] 35%|███▌      | 53/150 [03:09<05:47,  3.59s/it] 36%|███▌      | 54/150 [03:13<05:44,  3.59s/it] 37%|███▋      | 55/150 [03:17<05:42,  3.60s/it] 37%|███▋      | 56/150 [03:20<05:38,  3.60s/it] 38%|███▊      | 57/150 [03:24<05:33,  3.59s/it] 39%|███▊      | 58/150 [03:27<05:29,  3.58s/it] 39%|███▉      | 59/150 [03:31<05:26,  3.58s/it] 40%|████      | 60/150 [03:34<05:20,  3.57s/it]                                                {'loss': 0.0723, 'grad_norm': 0.3531387746334076, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:34<05:20,  3.57s/it] 41%|████      | 61/150 [03:38<05:18,  3.58s/it] 41%|████▏     | 62/150 [03:42<05:14,  3.58s/it] 42%|████▏     | 63/150 [03:45<05:11,  3.58s/it] 43%|████▎     | 64/150 [03:49<05:06,  3.57s/it] 43%|████▎     | 65/150 [03:52<05:03,  3.57s/it] 44%|████▍     | 66/150 [03:56<04:58,  3.56s/it] 45%|████▍     | 67/150 [03:59<04:53,  3.54s/it] 45%|████▌     | 68/150 [04:03<04:49,  3.53s/it] 46%|████▌     | 69/150 [04:06<04:46,  3.53s/it] 47%|████▋     | 70/150 [04:10<04:44,  3.56s/it]                                                {'loss': 0.0587, 'grad_norm': 0.2607802450656891, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:10<04:44,  3.56s/it] 47%|████▋     | 71/150 [04:14<04:40,  3.55s/it] 48%|████▊     | 72/150 [04:17<04:38,  3.57s/it] 49%|████▊     | 73/150 [04:21<04:35,  3.58s/it] 49%|████▉     | 74/150 [04:24<04:31,  3.57s/it] 50%|█████     | 75/150 [04:28<04:27,  3.57s/it] 51%|█████     | 76/150 [04:31<04:23,  3.57s/it] 51%|█████▏    | 77/150 [04:35<04:19,  3.55s/it] 52%|█████▏    | 78/150 [04:39<04:17,  3.58s/it] 53%|█████▎    | 79/150 [04:42<04:14,  3.59s/it] 53%|█████▎    | 80/150 [04:46<04:11,  3.59s/it]                                                {'loss': 0.0588, 'grad_norm': 0.2537110447883606, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:46<04:11,  3.59s/it] 54%|█████▍    | 81/150 [04:49<04:06,  3.57s/it] 55%|█████▍    | 82/150 [04:53<04:02,  3.56s/it] 55%|█████▌    | 83/150 [04:57<04:00,  3.59s/it] 56%|█████▌    | 84/150 [05:00<03:57,  3.59s/it] 57%|█████▋    | 85/150 [05:04<03:52,  3.58s/it] 57%|█████▋    | 86/150 [05:07<03:48,  3.57s/it] 58%|█████▊    | 87/150 [05:11<03:44,  3.56s/it] 59%|█████▊    | 88/150 [05:14<03:40,  3.56s/it] 59%|█████▉    | 89/150 [05:18<03:36,  3.56s/it] 60%|██████    | 90/150 [05:22<03:34,  3.57s/it]                                                {'loss': 0.0692, 'grad_norm': 0.30295395851135254, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:22<03:34,  3.57s/it] 61%|██████    | 91/150 [05:25<03:29,  3.56s/it] 61%|██████▏   | 92/150 [05:29<03:25,  3.55s/it] 62%|██████▏   | 93/150 [05:32<03:23,  3.57s/it] 63%|██████▎   | 94/150 [05:36<03:19,  3.57s/it] 63%|██████▎   | 95/150 [05:39<03:15,  3.56s/it] 64%|██████▍   | 96/150 [05:43<03:12,  3.57s/it] 65%|██████▍   | 97/150 [05:46<03:09,  3.58s/it] 65%|██████▌   | 98/150 [05:50<03:05,  3.56s/it] 66%|██████▌   | 99/150 [05:54<03:02,  3.57s/it] 67%|██████▋   | 100/150 [05:57<02:59,  3.58s/it]                                                 {'loss': 0.0653, 'grad_norm': 0.5932579636573792, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [05:57<02:59,  3.58s/it] 67%|██████▋   | 101/150 [06:01<02:55,  3.58s/it] 68%|██████▊   | 102/150 [06:04<02:51,  3.58s/it] 69%|██████▊   | 103/150 [06:08<02:47,  3.57s/it] 69%|██████▉   | 104/150 [06:11<02:43,  3.56s/it] 70%|███████   | 105/150 [06:15<02:39,  3.55s/it] 71%|███████   | 106/150 [06:19<02:37,  3.57s/it] 71%|███████▏  | 107/150 [06:22<02:34,  3.59s/it] 72%|███████▏  | 108/150 [06:26<02:30,  3.58s/it] 73%|███████▎  | 109/150 [06:29<02:27,  3.59s/it] 73%|███████▎  | 110/150 [06:33<02:23,  3.58s/it]                                                 {'loss': 0.0523, 'grad_norm': 0.24122583866119385, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:33<02:23,  3.58s/it] 74%|███████▍  | 111/150 [06:37<02:20,  3.60s/it] 75%|███████▍  | 112/150 [06:40<02:16,  3.59s/it] 75%|███████▌  | 113/150 [06:44<02:13,  3.61s/it] 76%|███████▌  | 114/150 [06:47<02:09,  3.60s/it] 77%|███████▋  | 115/150 [06:51<02:05,  3.60s/it] 77%|███████▋  | 116/150 [06:55<02:02,  3.61s/it] 78%|███████▊  | 117/150 [06:58<01:59,  3.62s/it] 79%|███████▊  | 118/150 [07:02<01:55,  3.61s/it] 79%|███████▉  | 119/150 [07:05<01:51,  3.61s/it] 80%|████████  | 120/150 [07:09<01:48,  3.62s/it]                                                 {'loss': 0.059, 'grad_norm': 0.2792021334171295, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:09<01:48,  3.62s/it] 81%|████████  | 121/150 [07:13<01:44,  3.60s/it] 81%|████████▏ | 122/150 [07:16<01:40,  3.59s/it] 82%|████████▏ | 123/150 [07:20<01:36,  3.57s/it] 83%|████████▎ | 124/150 [07:23<01:32,  3.55s/it] 83%|████████▎ | 125/150 [07:27<01:28,  3.54s/it] 84%|████████▍ | 126/150 [07:30<01:25,  3.55s/it] 85%|████████▍ | 127/150 [07:34<01:21,  3.54s/it] 85%|████████▌ | 128/150 [07:37<01:18,  3.55s/it] 86%|████████▌ | 129/150 [07:41<01:14,  3.56s/it] 87%|████████▋ | 130/150 [07:45<01:11,  3.55s/it]                                                 {'loss': 0.0378, 'grad_norm': 0.17127226293087006, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [07:45<01:11,  3.55s/it] 87%|████████▋ | 131/150 [07:48<01:07,  3.57s/it] 88%|████████▊ | 132/150 [07:52<01:04,  3.58s/it] 89%|████████▊ | 133/150 [07:55<01:00,  3.56s/it] 89%|████████▉ | 134/150 [07:59<00:56,  3.56s/it] 90%|█████████ | 135/150 [08:02<00:53,  3.56s/it] 91%|█████████ | 136/150 [08:06<00:49,  3.55s/it] 91%|█████████▏| 137/150 [08:09<00:46,  3.56s/it] 92%|█████████▏| 138/150 [08:13<00:42,  3.58s/it] 93%|█████████▎| 139/150 [08:17<00:39,  3.57s/it] 93%|█████████▎| 140/150 [08:20<00:35,  3.55s/it]                                                 {'loss': 0.0424, 'grad_norm': 0.38496413826942444, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:20<00:35,  3.55s/it] 94%|█████████▍| 141/150 [08:24<00:31,  3.55s/it] 95%|█████████▍| 142/150 [08:27<00:28,  3.55s/it] 95%|█████████▌| 143/150 [08:31<00:24,  3.56s/it] 96%|█████████▌| 144/150 [08:34<00:21,  3.56s/it] 97%|█████████▋| 145/150 [08:38<00:17,  3.58s/it] 97%|█████████▋| 146/150 [08:42<00:14,  3.57s/it] 98%|█████████▊| 147/150 [08:45<00:10,  3.58s/it] 99%|█████████▊| 148/150 [08:49<00:07,  3.57s/it] 99%|█████████▉| 149/150 [08:52<00:03,  3.58s/it]100%|██████████| 150/150 [08:56<00:00,  3.56s/it]                                                 {'loss': 0.0583, 'grad_norm': 0.12868057191371918, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [08:56<00:00,  3.56s/it][INFO|trainer.py:3705] 2024-11-19 01:05:52,690 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 01:05:52,727 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:05:52,728 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:05:52,890 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:05:52,891 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 01:05:53,607 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 538.0203, 'train_samples_per_second': 4.494, 'train_steps_per_second': 0.279, 'train_loss': 0.19515025913715361, 'epoch': 2.98}
100%|██████████| 150/150 [08:57<00:00,  3.56s/it]100%|██████████| 150/150 [08:57<00:00,  3.58s/it]
[INFO|trainer.py:3705] 2024-11-19 01:05:53,610 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 01:05:53,642 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:05:53,643 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:05:53,794 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:05:53,794 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 83226592GF
  train_loss               =     0.1952
  train_runtime            = 0:08:58.02
  train_samples_per_second =      4.494
  train_steps_per_second   =      0.279
Figure saved at: saves/Qwen2.5-7B-Instruct/schema_prompt/lora/sft/training_loss.png
11/19/2024 01:05:54 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 01:05:54 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 01:05:54,255 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 01:05:54,255 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 01:05:54,255 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 15.52it/s]  9%|▉         | 4/45 [00:00<00:04, 10.01it/s] 13%|█▎        | 6/45 [00:00<00:04,  9.03it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.80it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.67it/s] 20%|██        | 9/45 [00:00<00:04,  8.51it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.36it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.49it/s] 27%|██▋       | 12/45 [00:01<00:03,  8.37it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.30it/s] 31%|███       | 14/45 [00:01<00:03,  8.42it/s] 33%|███▎      | 15/45 [00:01<00:03,  8.59it/s] 36%|███▌      | 16/45 [00:01<00:03,  8.38it/s] 38%|███▊      | 17/45 [00:01<00:03,  8.29it/s] 40%|████      | 18/45 [00:02<00:03,  8.26it/s] 42%|████▏     | 19/45 [00:02<00:03,  8.48it/s] 44%|████▍     | 20/45 [00:02<00:02,  8.39it/s] 47%|████▋     | 21/45 [00:02<00:02,  8.22it/s] 49%|████▉     | 22/45 [00:02<00:02,  8.15it/s] 51%|█████     | 23/45 [00:02<00:02,  8.27it/s] 53%|█████▎    | 24/45 [00:02<00:02,  8.24it/s] 56%|█████▌    | 25/45 [00:02<00:02,  8.21it/s] 58%|█████▊    | 26/45 [00:03<00:02,  8.17it/s] 60%|██████    | 27/45 [00:03<00:02,  8.26it/s] 62%|██████▏   | 28/45 [00:03<00:02,  8.16it/s] 64%|██████▍   | 29/45 [00:03<00:01,  8.07it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.12it/s] 69%|██████▉   | 31/45 [00:03<00:01,  8.38it/s] 71%|███████   | 32/45 [00:03<00:01,  8.47it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.37it/s] 76%|███████▌  | 34/45 [00:03<00:01,  8.56it/s] 78%|███████▊  | 35/45 [00:04<00:01,  8.41it/s] 80%|████████  | 36/45 [00:04<00:01,  8.41it/s] 82%|████████▏ | 37/45 [00:04<00:00,  8.31it/s] 84%|████████▍ | 38/45 [00:04<00:00,  8.31it/s] 87%|████████▋ | 39/45 [00:04<00:00,  8.17it/s] 89%|████████▉ | 40/45 [00:04<00:00,  8.27it/s] 91%|█████████ | 41/45 [00:04<00:00,  8.47it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.58it/s] 96%|█████████▌| 43/45 [00:05<00:00,  8.44it/s] 98%|█████████▊| 44/45 [00:05<00:00,  8.33it/s]100%|██████████| 45/45 [00:05<00:00,  8.32it/s]100%|██████████| 45/45 [00:05<00:00,  8.46it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.0668
  eval_runtime            = 0:00:05.45
  eval_samples_per_second =     16.513
  eval_steps_per_second   =      8.257
[INFO|modelcard.py:449] 2024-11-19 01:05:59,705 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 01:06:24,754] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 01:06:28 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:21536
[2024-11-19 01:06:30,729] torch.distributed.run: [WARNING] 
[2024-11-19 01:06:30,729] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 01:06:30,729] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 01:06:30,729] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 01:06:38,014] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 01:06:38,187] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 01:06:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 01:06:39 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 01:06:39,133 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:06:39,134 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,136 >> loading file tokenizer_config.json
11/19/2024 01:06:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 01:06:39 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 01:06:39,595 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 01:06:39,596 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:06:39,597 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:06:39,598 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 01:06:40,058 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 01:06:40 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 01:06:40 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/19/2024 01:06:40 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/19/2024 01:06:40 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 01:06:40 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 450.35 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3389.51 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 01:06:44 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:19, 43.15 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:09, 85.83 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 125.48 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:04, 161.37 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:03, 191.76 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 214.11 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 232.49 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 256.01 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:03<00:01, 264.65 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 345.54 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 339.94 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 321.77 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 301.81 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 297.91 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 215.75 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-19 01:06:49,400 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:06:49,401 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-19 01:06:49,450 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 01:06:49,451 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 01:06:49,452 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
[INFO|modeling_utils.py:4574] 2024-11-19 01:06:53,898 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 01:06:53,898 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 01:06:53,902 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 01:06:53,902 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/19/2024 01:06:53 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 01:06:53 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 01:06:53 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 01:06:53 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 01:06:53 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,q_proj,v_proj,o_proj,up_proj,down_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
11/19/2024 01:06:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 01:06:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 01:06:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 01:06:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 01:06:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,down_proj,v_proj,k_proj,gate_proj,q_proj,up_proj
11/19/2024 01:06:54 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 01:06:54,622 >> Using auto half precision backend
11/19/2024 01:06:54 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-19 01:06:55,390 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 01:06:55,390 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 01:06:55,390 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 01:06:55,390 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 01:06:55,390 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 01:06:55,390 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 01:06:55,390 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 01:06:55,397 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:16,  4.14s/it]  1%|▏         | 2/150 [00:07<09:34,  3.88s/it]  2%|▏         | 3/150 [00:11<09:19,  3.81s/it]  3%|▎         | 4/150 [00:15<09:09,  3.76s/it]  3%|▎         | 5/150 [00:18<09:03,  3.75s/it]  4%|▍         | 6/150 [00:22<08:59,  3.74s/it]  5%|▍         | 7/150 [00:26<08:53,  3.73s/it]  5%|▌         | 8/150 [00:30<08:47,  3.72s/it]  6%|▌         | 9/150 [00:33<08:44,  3.72s/it]  7%|▋         | 10/150 [00:37<08:39,  3.71s/it]                                                {'loss': 1.7328, 'grad_norm': 2.0818839073181152, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:37<08:39,  3.71s/it]  7%|▋         | 11/150 [00:41<08:37,  3.72s/it]  8%|▊         | 12/150 [00:44<08:32,  3.71s/it]  9%|▊         | 13/150 [00:48<08:28,  3.71s/it]  9%|▉         | 14/150 [00:52<08:23,  3.70s/it] 10%|█         | 15/150 [00:56<08:19,  3.70s/it] 11%|█         | 16/150 [00:59<08:15,  3.70s/it] 11%|█▏        | 17/150 [01:03<08:13,  3.71s/it] 12%|█▏        | 18/150 [01:07<08:08,  3.70s/it] 13%|█▎        | 19/150 [01:10<08:05,  3.71s/it] 13%|█▎        | 20/150 [01:14<08:00,  3.70s/it]                                                {'loss': 0.8777, 'grad_norm': 0.9223300218582153, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:14<08:00,  3.70s/it] 14%|█▍        | 21/150 [01:18<07:57,  3.70s/it] 15%|█▍        | 22/150 [01:21<07:52,  3.69s/it] 15%|█▌        | 23/150 [01:25<07:50,  3.70s/it] 16%|█▌        | 24/150 [01:29<07:46,  3.70s/it] 17%|█▋        | 25/150 [01:33<07:42,  3.70s/it] 17%|█▋        | 26/150 [01:36<07:38,  3.70s/it] 18%|█▊        | 27/150 [01:40<07:35,  3.70s/it] 19%|█▊        | 28/150 [01:44<07:30,  3.70s/it] 19%|█▉        | 29/150 [01:47<07:26,  3.69s/it] 20%|██        | 30/150 [01:51<07:23,  3.70s/it]                                                {'loss': 0.4202, 'grad_norm': 0.6713889837265015, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:51<07:23,  3.70s/it] 21%|██        | 31/150 [01:55<07:19,  3.69s/it] 21%|██▏       | 32/150 [01:58<07:15,  3.69s/it] 22%|██▏       | 33/150 [02:02<07:12,  3.70s/it] 23%|██▎       | 34/150 [02:06<07:08,  3.70s/it] 23%|██▎       | 35/150 [02:10<07:05,  3.70s/it] 24%|██▍       | 36/150 [02:13<07:01,  3.70s/it] 25%|██▍       | 37/150 [02:17<06:58,  3.70s/it] 25%|██▌       | 38/150 [02:21<06:54,  3.70s/it] 26%|██▌       | 39/150 [02:24<06:51,  3.71s/it] 27%|██▋       | 40/150 [02:28<06:47,  3.70s/it]                                                {'loss': 0.3293, 'grad_norm': 0.6168792843818665, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:28<06:47,  3.70s/it] 27%|██▋       | 41/150 [02:32<06:42,  3.69s/it] 28%|██▊       | 42/150 [02:35<06:38,  3.69s/it] 29%|██▊       | 43/150 [02:39<06:35,  3.70s/it] 29%|██▉       | 44/150 [02:43<06:31,  3.69s/it] 30%|███       | 45/150 [02:46<06:28,  3.70s/it] 31%|███       | 46/150 [02:50<06:23,  3.69s/it] 31%|███▏      | 47/150 [02:54<06:20,  3.69s/it] 32%|███▏      | 48/150 [02:58<06:16,  3.69s/it] 33%|███▎      | 49/150 [03:01<06:13,  3.70s/it] 33%|███▎      | 50/150 [03:05<06:09,  3.70s/it]                                                {'loss': 0.2626, 'grad_norm': 0.6144203543663025, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:05<06:09,  3.70s/it] 34%|███▍      | 51/150 [03:09<06:05,  3.69s/it] 35%|███▍      | 52/150 [03:12<06:01,  3.69s/it] 35%|███▌      | 53/150 [03:16<05:57,  3.69s/it] 36%|███▌      | 54/150 [03:20<05:53,  3.68s/it] 37%|███▋      | 55/150 [03:23<05:50,  3.69s/it] 37%|███▋      | 56/150 [03:27<05:47,  3.69s/it] 38%|███▊      | 57/150 [03:31<05:43,  3.69s/it] 39%|███▊      | 58/150 [03:34<05:39,  3.69s/it] 39%|███▉      | 59/150 [03:38<05:35,  3.69s/it] 40%|████      | 60/150 [03:42<05:32,  3.69s/it]                                                {'loss': 0.2254, 'grad_norm': 0.4768203794956207, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:42<05:32,  3.69s/it] 41%|████      | 61/150 [03:46<05:28,  3.69s/it] 41%|████▏     | 62/150 [03:49<05:24,  3.69s/it] 42%|████▏     | 63/150 [03:53<05:21,  3.69s/it] 43%|████▎     | 64/150 [03:57<05:17,  3.69s/it] 43%|████▎     | 65/150 [04:00<05:13,  3.69s/it] 44%|████▍     | 66/150 [04:04<05:10,  3.69s/it] 45%|████▍     | 67/150 [04:08<05:07,  3.70s/it] 45%|████▌     | 68/150 [04:11<05:03,  3.70s/it] 46%|████▌     | 69/150 [04:15<04:59,  3.70s/it] 47%|████▋     | 70/150 [04:19<04:55,  3.70s/it]                                                {'loss': 0.2067, 'grad_norm': 0.8136812448501587, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:19<04:55,  3.70s/it] 47%|████▋     | 71/150 [04:23<04:52,  3.70s/it] 48%|████▊     | 72/150 [04:26<04:48,  3.70s/it] 49%|████▊     | 73/150 [04:30<04:44,  3.69s/it] 49%|████▉     | 74/150 [04:34<04:41,  3.70s/it] 50%|█████     | 75/150 [04:37<04:37,  3.70s/it] 51%|█████     | 76/150 [04:41<04:33,  3.70s/it] 51%|█████▏    | 77/150 [04:45<04:30,  3.70s/it] 52%|█████▏    | 78/150 [04:48<04:26,  3.70s/it] 53%|█████▎    | 79/150 [04:52<04:23,  3.72s/it] 53%|█████▎    | 80/150 [04:56<04:19,  3.71s/it]                                                {'loss': 0.2103, 'grad_norm': 0.4438735842704773, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:56<04:19,  3.71s/it] 54%|█████▍    | 81/150 [05:00<04:15,  3.70s/it] 55%|█████▍    | 82/150 [05:03<04:11,  3.70s/it] 55%|█████▌    | 83/150 [05:07<04:07,  3.70s/it] 56%|█████▌    | 84/150 [05:11<04:04,  3.70s/it] 57%|█████▋    | 85/150 [05:14<04:00,  3.70s/it] 57%|█████▋    | 86/150 [05:18<03:56,  3.70s/it] 58%|█████▊    | 87/150 [05:22<03:53,  3.70s/it] 59%|█████▊    | 88/150 [05:25<03:49,  3.70s/it] 59%|█████▉    | 89/150 [05:29<03:45,  3.69s/it] 60%|██████    | 90/150 [05:33<03:41,  3.69s/it]                                                {'loss': 0.218, 'grad_norm': 0.5538231730461121, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:33<03:41,  3.69s/it] 61%|██████    | 91/150 [05:36<03:37,  3.69s/it] 61%|██████▏   | 92/150 [05:40<03:33,  3.69s/it] 62%|██████▏   | 93/150 [05:44<03:30,  3.69s/it] 63%|██████▎   | 94/150 [05:48<03:27,  3.70s/it] 63%|██████▎   | 95/150 [05:51<03:23,  3.70s/it] 64%|██████▍   | 96/150 [05:55<03:20,  3.71s/it] 65%|██████▍   | 97/150 [05:59<03:16,  3.70s/it] 65%|██████▌   | 98/150 [06:02<03:13,  3.71s/it] 66%|██████▌   | 99/150 [06:06<03:09,  3.71s/it] 67%|██████▋   | 100/150 [06:10<03:04,  3.70s/it]                                                 {'loss': 0.2259, 'grad_norm': 0.6441213488578796, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:10<03:04,  3.70s/it] 67%|██████▋   | 101/150 [06:13<03:01,  3.70s/it] 68%|██████▊   | 102/150 [06:17<02:57,  3.70s/it] 69%|██████▊   | 103/150 [06:21<02:53,  3.68s/it] 69%|██████▉   | 104/150 [06:25<02:49,  3.68s/it] 70%|███████   | 105/150 [06:28<02:45,  3.68s/it] 71%|███████   | 106/150 [06:32<02:42,  3.69s/it] 71%|███████▏  | 107/150 [06:36<02:38,  3.69s/it] 72%|███████▏  | 108/150 [06:39<02:35,  3.69s/it] 73%|███████▎  | 109/150 [06:43<02:31,  3.70s/it] 73%|███████▎  | 110/150 [06:47<02:28,  3.70s/it]                                                 {'loss': 0.1929, 'grad_norm': 0.8204667568206787, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:47<02:28,  3.70s/it] 74%|███████▍  | 111/150 [06:50<02:24,  3.70s/it] 75%|███████▍  | 112/150 [06:54<02:20,  3.70s/it] 75%|███████▌  | 113/150 [06:58<02:16,  3.70s/it] 76%|███████▌  | 114/150 [07:02<02:13,  3.70s/it] 77%|███████▋  | 115/150 [07:05<02:09,  3.70s/it] 77%|███████▋  | 116/150 [07:09<02:05,  3.70s/it] 78%|███████▊  | 117/150 [07:13<02:02,  3.70s/it] 79%|███████▊  | 118/150 [07:16<01:58,  3.70s/it] 79%|███████▉  | 119/150 [07:20<01:54,  3.69s/it] 80%|████████  | 120/150 [07:24<01:50,  3.69s/it]                                                 {'loss': 0.2244, 'grad_norm': 0.5788815021514893, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:24<01:50,  3.69s/it] 81%|████████  | 121/150 [07:27<01:47,  3.70s/it] 81%|████████▏ | 122/150 [07:31<01:43,  3.70s/it] 82%|████████▏ | 123/150 [07:35<01:39,  3.70s/it] 83%|████████▎ | 124/150 [07:38<01:36,  3.70s/it] 83%|████████▎ | 125/150 [07:42<01:32,  3.70s/it] 84%|████████▍ | 126/150 [07:46<01:28,  3.70s/it] 85%|████████▍ | 127/150 [07:50<01:24,  3.69s/it] 85%|████████▌ | 128/150 [07:53<01:21,  3.69s/it] 86%|████████▌ | 129/150 [07:57<01:17,  3.69s/it] 87%|████████▋ | 130/150 [08:01<01:13,  3.69s/it]                                                 {'loss': 0.1622, 'grad_norm': 0.336868017911911, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:01<01:13,  3.69s/it] 87%|████████▋ | 131/150 [08:04<01:10,  3.69s/it] 88%|████████▊ | 132/150 [08:08<01:06,  3.70s/it] 89%|████████▊ | 133/150 [08:12<01:02,  3.69s/it] 89%|████████▉ | 134/150 [08:15<00:59,  3.69s/it] 90%|█████████ | 135/150 [08:19<00:55,  3.70s/it] 91%|█████████ | 136/150 [08:23<00:51,  3.70s/it] 91%|█████████▏| 137/150 [08:27<00:48,  3.70s/it] 92%|█████████▏| 138/150 [08:30<00:44,  3.70s/it] 93%|█████████▎| 139/150 [08:34<00:40,  3.70s/it] 93%|█████████▎| 140/150 [08:38<00:36,  3.69s/it]                                                 {'loss': 0.1978, 'grad_norm': 0.6244106888771057, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:38<00:36,  3.69s/it] 94%|█████████▍| 141/150 [08:41<00:33,  3.69s/it] 95%|█████████▍| 142/150 [08:45<00:29,  3.69s/it] 95%|█████████▌| 143/150 [08:49<00:25,  3.70s/it] 96%|█████████▌| 144/150 [08:52<00:22,  3.71s/it] 97%|█████████▋| 145/150 [08:56<00:18,  3.71s/it] 97%|█████████▋| 146/150 [09:00<00:14,  3.70s/it] 98%|█████████▊| 147/150 [09:04<00:11,  3.70s/it] 99%|█████████▊| 148/150 [09:07<00:07,  3.70s/it] 99%|█████████▉| 149/150 [09:11<00:03,  3.69s/it]100%|██████████| 150/150 [09:15<00:00,  3.70s/it]                                                 {'loss': 0.1628, 'grad_norm': 0.4576830565929413, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [09:15<00:00,  3.70s/it][INFO|trainer.py:3705] 2024-11-19 01:16:11,245 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 01:16:11,283 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:16:11,284 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:16:11,449 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:16:11,449 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 01:16:12,171 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 556.7743, 'train_samples_per_second': 4.343, 'train_steps_per_second': 0.269, 'train_loss': 0.37659364541371665, 'epoch': 2.98}
100%|██████████| 150/150 [09:16<00:00,  3.70s/it]100%|██████████| 150/150 [09:16<00:00,  3.71s/it]
[INFO|trainer.py:3705] 2024-11-19 01:16:12,174 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 01:16:12,206 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:16:12,207 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:16:12,361 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:16:12,362 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 94723273GF
  train_loss               =     0.3766
  train_runtime            = 0:09:16.77
  train_samples_per_second =      4.343
  train_steps_per_second   =      0.269
Figure saved at: saves/Qwen2.5-7B-Instruct/my_prompt/lora/sft/training_loss.png
11/19/2024 01:16:12 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 01:16:12 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 01:16:12,841 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 01:16:12,841 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 01:16:12,841 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 15.81it/s]  9%|▉         | 4/45 [00:00<00:04, 10.05it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.93it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.73it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.47it/s] 20%|██        | 9/45 [00:01<00:04,  8.34it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.30it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.16it/s] 27%|██▋       | 12/45 [00:01<00:04,  8.14it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.05it/s] 31%|███       | 14/45 [00:01<00:03,  8.04it/s] 33%|███▎      | 15/45 [00:01<00:03,  7.93it/s] 36%|███▌      | 16/45 [00:01<00:03,  7.93it/s] 38%|███▊      | 17/45 [00:02<00:03,  8.00it/s] 40%|████      | 18/45 [00:02<00:03,  8.06it/s] 42%|████▏     | 19/45 [00:02<00:03,  8.08it/s] 44%|████▍     | 20/45 [00:02<00:03,  8.01it/s] 47%|████▋     | 21/45 [00:02<00:02,  8.02it/s] 49%|████▉     | 22/45 [00:02<00:02,  8.01it/s] 51%|█████     | 23/45 [00:02<00:02,  8.06it/s] 53%|█████▎    | 24/45 [00:02<00:02,  7.99it/s] 56%|█████▌    | 25/45 [00:03<00:02,  8.04it/s] 58%|█████▊    | 26/45 [00:03<00:02,  8.09it/s] 60%|██████    | 27/45 [00:03<00:02,  8.11it/s] 62%|██████▏   | 28/45 [00:03<00:02,  8.03it/s] 64%|██████▍   | 29/45 [00:03<00:01,  8.08it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.10it/s] 69%|██████▉   | 31/45 [00:03<00:01,  7.98it/s] 71%|███████   | 32/45 [00:03<00:01,  8.03it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.01it/s] 76%|███████▌  | 34/45 [00:04<00:01,  7.97it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.97it/s] 80%|████████  | 36/45 [00:04<00:01,  7.92it/s] 82%|████████▏ | 37/45 [00:04<00:01,  7.99it/s] 84%|████████▍ | 38/45 [00:04<00:00,  8.04it/s] 87%|████████▋ | 39/45 [00:04<00:00,  8.08it/s] 89%|████████▉ | 40/45 [00:04<00:00,  8.05it/s] 91%|█████████ | 41/45 [00:05<00:00,  7.97it/s] 93%|█████████▎| 42/45 [00:05<00:00,  8.03it/s] 96%|█████████▌| 43/45 [00:05<00:00,  8.02it/s] 98%|█████████▊| 44/45 [00:05<00:00,  8.03it/s]100%|██████████| 45/45 [00:05<00:00,  8.11it/s]100%|██████████| 45/45 [00:05<00:00,  8.19it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1867
  eval_runtime            = 0:00:05.64
  eval_samples_per_second =     15.952
  eval_steps_per_second   =      7.976
[INFO|modelcard.py:449] 2024-11-19 01:16:18,484 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 01:16:39,356] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 01:16:43 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22331
[2024-11-19 01:16:45,500] torch.distributed.run: [WARNING] 
[2024-11-19 01:16:45,500] torch.distributed.run: [WARNING] *****************************************
[2024-11-19 01:16:45,500] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-19 01:16:45,500] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-19 01:16:52,988] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-19 01:16:53,053] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/19/2024 01:16:54 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 01:16:54 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-19 01:16:54,216 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:16:54,217 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,219 >> loading file tokenizer_config.json
11/19/2024 01:16:54 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/19/2024 01:16:54 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-19 01:16:54,708 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-19 01:16:54,708 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:16:54,709 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,710 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,710 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,710 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,710 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,710 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-19 01:16:54,711 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-19 01:16:55,203 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/19/2024 01:16:55 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 01:16:55 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/19/2024 01:16:55 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
11/19/2024 01:16:55 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/19/2024 01:16:55 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 413.54 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2903.81 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/19/2024 01:16:59 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:16, 50.10 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 97.35 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 141.68 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 179.79 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:02, 212.35 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 237.46 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 201.24 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 302.22 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:02<00:01, 307.92 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:02<00:00, 312.51 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 321.64 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 341.60 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 351.85 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 328.62 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 344.01 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 236.03 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 101395, 104057, 90395, 31526, 105151, 9370, 3144, 33108, 29991, 3837, 59151, 59879, 105151, 29991, 74661, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和姓氏，并返回标签的URL和名称，结果按标签名称排序<|im_end|>
<|im_start|>assistant
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-19 01:17:04,108 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:17:04,109 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-19 01:17:04,163 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-19 01:17:04,164 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-19 01:17:04,166 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
[INFO|modeling_utils.py:4574] 2024-11-19 01:17:08,585 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-19 01:17:08,586 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-19 01:17:08,590 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-19 01:17:08,590 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 01:17:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 01:17:08 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,v_proj,up_proj,gate_proj,q_proj,k_proj,down_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/19/2024 01:17:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/19/2024 01:17:08 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/19/2024 01:17:08 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,up_proj,v_proj,o_proj,down_proj,q_proj,gate_proj
11/19/2024 01:17:09 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-19 01:17:09,441 >> Using auto half precision backend
11/19/2024 01:17:09 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-19 01:17:10,172 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-19 01:17:10,172 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-19 01:17:10,172 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-19 01:17:10,172 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-19 01:17:10,172 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-19 01:17:10,172 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-19 01:17:10,172 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-19 01:17:10,179 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:03<08:56,  3.60s/it]  1%|▏         | 2/150 [00:06<08:19,  3.37s/it]  2%|▏         | 3/150 [00:10<08:04,  3.30s/it]  3%|▎         | 4/150 [00:13<07:54,  3.25s/it]  3%|▎         | 5/150 [00:16<07:48,  3.23s/it]  4%|▍         | 6/150 [00:19<07:43,  3.22s/it]  5%|▍         | 7/150 [00:22<07:39,  3.21s/it]  5%|▌         | 8/150 [00:25<07:32,  3.19s/it]  6%|▌         | 9/150 [00:29<07:29,  3.19s/it]  7%|▋         | 10/150 [00:32<07:25,  3.18s/it]                                                {'loss': 2.1338, 'grad_norm': 2.80594801902771, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:32<07:25,  3.18s/it]  7%|▋         | 11/150 [00:35<07:23,  3.19s/it]  8%|▊         | 12/150 [00:38<07:18,  3.18s/it]  9%|▊         | 13/150 [00:41<07:15,  3.18s/it]  9%|▉         | 14/150 [00:44<07:12,  3.18s/it] 10%|█         | 15/150 [00:48<07:08,  3.17s/it] 11%|█         | 16/150 [00:51<07:04,  3.17s/it] 11%|█▏        | 17/150 [00:54<07:01,  3.17s/it] 12%|█▏        | 18/150 [00:57<06:58,  3.17s/it] 13%|█▎        | 19/150 [01:00<06:54,  3.17s/it] 13%|█▎        | 20/150 [01:04<06:51,  3.17s/it]                                                {'loss': 0.8535, 'grad_norm': 1.0157169103622437, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:04<06:51,  3.17s/it] 14%|█▍        | 21/150 [01:07<06:48,  3.17s/it] 15%|█▍        | 22/150 [01:10<06:44,  3.16s/it] 15%|█▌        | 23/150 [01:13<06:40,  3.15s/it] 16%|█▌        | 24/150 [01:16<06:38,  3.16s/it] 17%|█▋        | 25/150 [01:19<06:35,  3.16s/it] 17%|█▋        | 26/150 [01:22<06:31,  3.16s/it] 18%|█▊        | 27/150 [01:26<06:27,  3.15s/it] 19%|█▊        | 28/150 [01:29<06:24,  3.15s/it] 19%|█▉        | 29/150 [01:32<06:21,  3.15s/it] 20%|██        | 30/150 [01:35<06:18,  3.16s/it]                                                {'loss': 0.2616, 'grad_norm': 0.7650270462036133, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:35<06:18,  3.16s/it] 21%|██        | 31/150 [01:38<06:15,  3.16s/it] 21%|██▏       | 32/150 [01:41<06:12,  3.15s/it] 22%|██▏       | 33/150 [01:44<06:08,  3.15s/it] 23%|██▎       | 34/150 [01:48<06:04,  3.15s/it] 23%|██▎       | 35/150 [01:51<06:00,  3.14s/it] 24%|██▍       | 36/150 [01:54<05:57,  3.13s/it] 25%|██▍       | 37/150 [01:57<05:53,  3.13s/it] 25%|██▌       | 38/150 [02:00<05:50,  3.13s/it] 26%|██▌       | 39/150 [02:03<05:47,  3.13s/it] 27%|██▋       | 40/150 [02:06<05:44,  3.13s/it]                                                {'loss': 0.1363, 'grad_norm': 0.5651730895042419, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:06<05:44,  3.13s/it] 27%|██▋       | 41/150 [02:10<05:41,  3.14s/it] 28%|██▊       | 42/150 [02:13<05:38,  3.14s/it] 29%|██▊       | 43/150 [02:16<05:35,  3.14s/it] 29%|██▉       | 44/150 [02:19<05:33,  3.15s/it] 30%|███       | 45/150 [02:22<05:31,  3.15s/it] 31%|███       | 46/150 [02:25<05:28,  3.16s/it] 31%|███▏      | 47/150 [02:28<05:25,  3.16s/it] 32%|███▏      | 48/150 [02:32<05:24,  3.18s/it] 33%|███▎      | 49/150 [02:35<05:20,  3.17s/it] 33%|███▎      | 50/150 [02:38<05:17,  3.17s/it]                                                {'loss': 0.097, 'grad_norm': 0.7106617093086243, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [02:38<05:17,  3.17s/it] 34%|███▍      | 51/150 [02:41<05:14,  3.18s/it] 35%|███▍      | 52/150 [02:44<05:12,  3.18s/it] 35%|███▌      | 53/150 [02:48<05:09,  3.19s/it] 36%|███▌      | 54/150 [02:51<05:06,  3.19s/it] 37%|███▋      | 55/150 [02:54<05:03,  3.19s/it] 37%|███▋      | 56/150 [02:57<04:59,  3.19s/it] 38%|███▊      | 57/150 [03:00<04:55,  3.18s/it] 39%|███▊      | 58/150 [03:03<04:51,  3.17s/it] 39%|███▉      | 59/150 [03:07<04:48,  3.17s/it] 40%|████      | 60/150 [03:10<04:45,  3.18s/it]                                                {'loss': 0.0709, 'grad_norm': 0.371951699256897, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:10<04:45,  3.18s/it] 41%|████      | 61/150 [03:13<04:42,  3.18s/it] 41%|████▏     | 62/150 [03:16<04:39,  3.17s/it] 42%|████▏     | 63/150 [03:19<04:36,  3.18s/it] 43%|████▎     | 64/150 [03:23<04:34,  3.19s/it] 43%|████▎     | 65/150 [03:26<04:30,  3.19s/it] 44%|████▍     | 66/150 [03:29<04:27,  3.18s/it] 45%|████▍     | 67/150 [03:32<04:24,  3.19s/it] 45%|████▌     | 68/150 [03:35<04:21,  3.19s/it] 46%|████▌     | 69/150 [03:39<04:19,  3.20s/it] 47%|████▋     | 70/150 [03:42<04:15,  3.20s/it]                                                {'loss': 0.0596, 'grad_norm': 0.36631304025650024, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [03:42<04:15,  3.20s/it] 47%|████▋     | 71/150 [03:45<04:13,  3.20s/it] 48%|████▊     | 72/150 [03:48<04:09,  3.20s/it] 49%|████▊     | 73/150 [03:51<04:06,  3.20s/it] 49%|████▉     | 74/150 [03:55<04:02,  3.19s/it] 50%|█████     | 75/150 [03:58<03:59,  3.19s/it] 51%|█████     | 76/150 [04:01<03:56,  3.19s/it] 51%|█████▏    | 77/150 [04:04<03:52,  3.19s/it] 52%|█████▏    | 78/150 [04:07<03:49,  3.19s/it] 53%|█████▎    | 79/150 [04:10<03:46,  3.19s/it] 53%|█████▎    | 80/150 [04:14<03:43,  3.20s/it]                                                {'loss': 0.0603, 'grad_norm': 0.29711875319480896, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:14<03:43,  3.20s/it] 54%|█████▍    | 81/150 [04:17<03:40,  3.20s/it] 55%|█████▍    | 82/150 [04:20<03:37,  3.20s/it] 55%|█████▌    | 83/150 [04:23<03:34,  3.20s/it] 56%|█████▌    | 84/150 [04:27<03:31,  3.20s/it] 57%|█████▋    | 85/150 [04:30<03:27,  3.20s/it] 57%|█████▋    | 86/150 [04:33<03:24,  3.20s/it] 58%|█████▊    | 87/150 [04:36<03:21,  3.19s/it] 59%|█████▊    | 88/150 [04:39<03:17,  3.19s/it] 59%|█████▉    | 89/150 [04:42<03:15,  3.20s/it] 60%|██████    | 90/150 [04:46<03:11,  3.20s/it]                                                {'loss': 0.0698, 'grad_norm': 0.2611960172653198, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [04:46<03:11,  3.20s/it] 61%|██████    | 91/150 [04:49<03:09,  3.20s/it] 61%|██████▏   | 92/150 [04:52<03:05,  3.20s/it] 62%|██████▏   | 93/150 [04:55<03:02,  3.20s/it] 63%|██████▎   | 94/150 [04:59<02:59,  3.20s/it] 63%|██████▎   | 95/150 [05:02<02:55,  3.20s/it] 64%|██████▍   | 96/150 [05:05<02:52,  3.20s/it] 65%|██████▍   | 97/150 [05:08<02:48,  3.19s/it] 65%|██████▌   | 98/150 [05:11<02:45,  3.19s/it] 66%|██████▌   | 99/150 [05:14<02:42,  3.18s/it] 67%|██████▋   | 100/150 [05:18<02:39,  3.18s/it]                                                 {'loss': 0.0643, 'grad_norm': 0.4333186745643616, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [05:18<02:39,  3.18s/it] 67%|██████▋   | 101/150 [05:21<02:36,  3.19s/it] 68%|██████▊   | 102/150 [05:24<02:32,  3.18s/it] 69%|██████▊   | 103/150 [05:27<02:29,  3.18s/it] 69%|██████▉   | 104/150 [05:30<02:26,  3.18s/it] 70%|███████   | 105/150 [05:34<02:23,  3.18s/it] 71%|███████   | 106/150 [05:37<02:19,  3.18s/it] 71%|███████▏  | 107/150 [05:40<02:16,  3.19s/it] 72%|███████▏  | 108/150 [05:43<02:13,  3.19s/it] 73%|███████▎  | 109/150 [05:46<02:11,  3.20s/it] 73%|███████▎  | 110/150 [05:50<02:08,  3.20s/it]                                                 {'loss': 0.053, 'grad_norm': 0.49908119440078735, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [05:50<02:08,  3.20s/it] 74%|███████▍  | 111/150 [05:53<02:04,  3.19s/it] 75%|███████▍  | 112/150 [05:56<02:01,  3.19s/it] 75%|███████▌  | 113/150 [05:59<01:57,  3.18s/it] 76%|███████▌  | 114/150 [06:02<01:54,  3.17s/it] 77%|███████▋  | 115/150 [06:05<01:50,  3.17s/it] 77%|███████▋  | 116/150 [06:08<01:47,  3.17s/it] 78%|███████▊  | 117/150 [06:12<01:44,  3.16s/it] 79%|███████▊  | 118/150 [06:15<01:41,  3.17s/it] 79%|███████▉  | 119/150 [06:18<01:38,  3.17s/it] 80%|████████  | 120/150 [06:21<01:34,  3.16s/it]                                                 {'loss': 0.059, 'grad_norm': 0.38749927282333374, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [06:21<01:34,  3.16s/it] 81%|████████  | 121/150 [06:24<01:31,  3.16s/it] 81%|████████▏ | 122/150 [06:27<01:28,  3.17s/it] 82%|████████▏ | 123/150 [06:31<01:25,  3.17s/it] 83%|████████▎ | 124/150 [06:34<01:22,  3.18s/it] 83%|████████▎ | 125/150 [06:37<01:19,  3.17s/it] 84%|████████▍ | 126/150 [06:40<01:15,  3.16s/it] 85%|████████▍ | 127/150 [06:43<01:12,  3.16s/it] 85%|████████▌ | 128/150 [06:46<01:09,  3.16s/it] 86%|████████▌ | 129/150 [06:50<01:06,  3.16s/it] 87%|████████▋ | 130/150 [06:53<01:03,  3.16s/it]                                                 {'loss': 0.0411, 'grad_norm': 0.20631146430969238, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [06:53<01:03,  3.16s/it] 87%|████████▋ | 131/150 [06:56<01:00,  3.16s/it] 88%|████████▊ | 132/150 [06:59<00:56,  3.15s/it] 89%|████████▊ | 133/150 [07:02<00:53,  3.15s/it] 89%|████████▉ | 134/150 [07:05<00:50,  3.14s/it] 90%|█████████ | 135/150 [07:09<00:47,  3.14s/it] 91%|█████████ | 136/150 [07:12<00:43,  3.14s/it] 91%|█████████▏| 137/150 [07:15<00:40,  3.13s/it] 92%|█████████▏| 138/150 [07:18<00:37,  3.14s/it] 93%|█████████▎| 139/150 [07:21<00:34,  3.14s/it] 93%|█████████▎| 140/150 [07:24<00:31,  3.13s/it]                                                 {'loss': 0.0444, 'grad_norm': 0.38553905487060547, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [07:24<00:31,  3.13s/it] 94%|█████████▍| 141/150 [07:27<00:28,  3.12s/it] 95%|█████████▍| 142/150 [07:30<00:24,  3.12s/it] 95%|█████████▌| 143/150 [07:33<00:21,  3.11s/it] 96%|█████████▌| 144/150 [07:37<00:18,  3.11s/it] 97%|█████████▋| 145/150 [07:40<00:15,  3.12s/it] 97%|█████████▋| 146/150 [07:43<00:12,  3.13s/it] 98%|█████████▊| 147/150 [07:46<00:09,  3.14s/it] 99%|█████████▊| 148/150 [07:49<00:06,  3.15s/it] 99%|█████████▉| 149/150 [07:52<00:03,  3.15s/it]100%|██████████| 150/150 [07:56<00:00,  3.16s/it]                                                 {'loss': 0.0558, 'grad_norm': 0.18148992955684662, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [07:56<00:00,  3.16s/it][INFO|trainer.py:3705] 2024-11-19 01:25:06,952 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 01:25:06,988 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:25:06,990 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:25:07,146 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:25:07,146 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-19 01:25:07,862 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 477.6835, 'train_samples_per_second': 5.062, 'train_steps_per_second': 0.314, 'train_loss': 0.27068288187185924, 'epoch': 2.98}
100%|██████████| 150/150 [07:56<00:00,  3.16s/it]100%|██████████| 150/150 [07:56<00:00,  3.18s/it]
[INFO|trainer.py:3705] 2024-11-19 01:25:07,865 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 01:25:07,897 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 01:25:07,898 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 01:25:08,052 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 01:25:08,052 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 26471621GF
  train_loss               =     0.2707
  train_runtime            = 0:07:57.68
  train_samples_per_second =      5.062
  train_steps_per_second   =      0.314
Figure saved at: saves/Qwen2.5-7B-Instruct/vector_prompt/lora/sft/training_loss.png
11/19/2024 01:25:08 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 01:25:08 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 01:25:08,496 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 01:25:08,496 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 01:25:08,496 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  7%|▋         | 3/45 [00:00<00:02, 16.26it/s] 11%|█         | 5/45 [00:00<00:03, 13.14it/s] 16%|█▌        | 7/45 [00:00<00:03, 12.24it/s] 20%|██        | 9/45 [00:00<00:03, 11.86it/s] 24%|██▍       | 11/45 [00:00<00:02, 11.63it/s] 29%|██▉       | 13/45 [00:01<00:02, 11.51it/s] 33%|███▎      | 15/45 [00:01<00:02, 11.43it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.32it/s] 42%|████▏     | 19/45 [00:01<00:02, 11.26it/s] 47%|████▋     | 21/45 [00:01<00:02, 11.25it/s] 51%|█████     | 23/45 [00:01<00:01, 11.23it/s] 56%|█████▌    | 25/45 [00:02<00:01, 11.19it/s] 60%|██████    | 27/45 [00:02<00:01, 11.17it/s] 64%|██████▍   | 29/45 [00:02<00:01, 11.17it/s] 69%|██████▉   | 31/45 [00:02<00:01, 11.21it/s] 73%|███████▎  | 33/45 [00:02<00:01, 11.22it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.23it/s] 82%|████████▏ | 37/45 [00:03<00:00, 11.22it/s] 87%|████████▋ | 39/45 [00:03<00:00, 11.20it/s] 91%|█████████ | 41/45 [00:03<00:00, 11.22it/s] 96%|█████████▌| 43/45 [00:03<00:00, 11.18it/s]100%|██████████| 45/45 [00:03<00:00, 11.22it/s]100%|██████████| 45/45 [00:03<00:00, 11.42it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =      0.075
  eval_runtime            = 0:00:04.04
  eval_samples_per_second =     22.226
  eval_steps_per_second   =     11.113
[INFO|modelcard.py:449] 2024-11-19 01:25:12,546 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
