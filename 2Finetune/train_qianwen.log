nohup: ignoring input
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:52:43,477] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:52:47 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23711
[2024-11-18 21:52:49,278] torch.distributed.run: [WARNING] 
[2024-11-18 21:52:49,278] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 21:52:49,278] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 21:52:49,278] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:52:56,893] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 21:52:57,066] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:52:58 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:52:58 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 21:52:58,086 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:52:58,088 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,090 >> loading file tokenizer_config.json
11/18/2024 21:52:58 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:52:58 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:52:58,547 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 21:52:58,548 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:52:58,549 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:52:58,550 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:52:59,007 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 21:52:59 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:52:59 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 21:52:59 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/18/2024 21:52:59 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:52:59 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 896 examples [00:00, 12383.59 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 416.96 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2945.83 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 21:53:03 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:17, 47.37 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 95.98 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 142.83 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 186.10 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:02, 224.00 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 189.88 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 296.19 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 308.76 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:02<00:01, 319.71 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:02<00:00, 349.97 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 334.06 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 339.21 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 334.97 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 344.39 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 238.62 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 21:53:07,955 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:53:07,957 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3729] 2024-11-18 21:53:08,007 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/model.safetensors
[INFO|modeling_utils.py:1622] 2024-11-18 21:53:08,032 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 21:53:08,035 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:4574] 2024-11-18 21:53:08,763 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 21:53:08,764 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 21:53:08,767 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 21:53:08,768 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 21:53:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:53:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:53:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:53:08 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:53:08 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,gate_proj,q_proj,k_proj,down_proj,o_proj,up_proj
11/18/2024 21:53:09 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 21:53:09,191 >> Using auto half precision backend
11/18/2024 21:53:09 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:53:09 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:53:09 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:53:09 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:53:09 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,down_proj,v_proj,o_proj,gate_proj,q_proj,k_proj
11/18/2024 21:53:09 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826
[INFO|trainer.py:2243] 2024-11-18 21:53:10,188 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 21:53:10,189 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 21:53:10,189 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 21:53:10,189 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 21:53:10,189 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 21:53:10,189 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 21:53:10,189 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 21:53:10,194 >>   Number of trainable parameters = 4,399,104
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:03<08:36,  3.47s/it]  1%|▏         | 2/150 [00:06<07:53,  3.20s/it]  2%|▏         | 3/150 [00:09<07:31,  3.07s/it]  3%|▎         | 4/150 [00:12<07:15,  2.99s/it]  3%|▎         | 5/150 [00:15<07:03,  2.92s/it]  4%|▍         | 6/150 [00:17<06:50,  2.85s/it]  5%|▍         | 7/150 [00:20<06:44,  2.83s/it]  5%|▌         | 8/150 [00:23<06:35,  2.79s/it]  6%|▌         | 9/150 [00:25<06:29,  2.76s/it]  7%|▋         | 10/150 [00:28<06:25,  2.76s/it]                                                {'loss': 1.565, 'grad_norm': 3.146230936050415, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:28<06:25,  2.76s/it]  7%|▋         | 11/150 [00:31<06:22,  2.75s/it]  8%|▊         | 12/150 [00:34<06:18,  2.74s/it]  9%|▊         | 13/150 [00:36<06:14,  2.73s/it]  9%|▉         | 14/150 [00:39<06:11,  2.73s/it] 10%|█         | 15/150 [00:42<06:08,  2.73s/it] 11%|█         | 16/150 [00:45<06:04,  2.72s/it] 11%|█▏        | 17/150 [00:47<06:01,  2.72s/it] 12%|█▏        | 18/150 [00:50<05:58,  2.72s/it] 13%|█▎        | 19/150 [00:53<05:55,  2.72s/it] 13%|█▎        | 20/150 [00:55<05:53,  2.72s/it]                                                {'loss': 0.8625, 'grad_norm': 1.7427890300750732, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [00:55<05:53,  2.72s/it] 14%|█▍        | 21/150 [00:58<05:51,  2.72s/it] 15%|█▍        | 22/150 [01:01<05:49,  2.73s/it] 15%|█▌        | 23/150 [01:04<05:46,  2.73s/it] 16%|█▌        | 24/150 [01:06<05:43,  2.73s/it] 17%|█▋        | 25/150 [01:09<05:40,  2.72s/it] 17%|█▋        | 26/150 [01:12<05:38,  2.73s/it] 18%|█▊        | 27/150 [01:14<05:35,  2.73s/it] 19%|█▊        | 28/150 [01:17<05:32,  2.72s/it] 19%|█▉        | 29/150 [01:20<05:30,  2.73s/it] 20%|██        | 30/150 [01:23<05:27,  2.73s/it]                                                {'loss': 0.4993, 'grad_norm': 1.2200895547866821, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:23<05:27,  2.73s/it] 21%|██        | 31/150 [01:25<05:26,  2.74s/it] 21%|██▏       | 32/150 [01:28<05:25,  2.76s/it] 22%|██▏       | 33/150 [01:31<05:25,  2.78s/it] 23%|██▎       | 34/150 [01:34<05:24,  2.79s/it] 23%|██▎       | 35/150 [01:37<05:22,  2.81s/it] 24%|██▍       | 36/150 [01:40<05:21,  2.82s/it] 25%|██▍       | 37/150 [01:42<05:19,  2.83s/it] 25%|██▌       | 38/150 [01:45<05:16,  2.83s/it] 26%|██▌       | 39/150 [01:48<05:14,  2.83s/it] 27%|██▋       | 40/150 [01:51<05:11,  2.83s/it]                                                {'loss': 0.4078, 'grad_norm': 1.0799682140350342, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [01:51<05:11,  2.83s/it] 27%|██▋       | 41/150 [01:54<05:09,  2.84s/it] 28%|██▊       | 42/150 [01:57<05:05,  2.83s/it] 29%|██▊       | 43/150 [01:59<05:01,  2.82s/it] 29%|██▉       | 44/150 [02:02<04:57,  2.81s/it] 30%|███       | 45/150 [02:05<04:52,  2.79s/it] 31%|███       | 46/150 [02:08<04:50,  2.79s/it] 31%|███▏      | 47/150 [02:11<04:48,  2.80s/it] 32%|███▏      | 48/150 [02:13<04:44,  2.79s/it] 33%|███▎      | 49/150 [02:16<04:42,  2.79s/it] 33%|███▎      | 50/150 [02:19<04:39,  2.79s/it]                                                {'loss': 0.329, 'grad_norm': 1.1530330181121826, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [02:19<04:39,  2.79s/it] 34%|███▍      | 51/150 [02:22<04:37,  2.80s/it] 35%|███▍      | 52/150 [02:25<04:35,  2.82s/it] 35%|███▌      | 53/150 [02:28<04:37,  2.86s/it] 36%|███▌      | 54/150 [02:30<04:32,  2.84s/it] 37%|███▋      | 55/150 [02:33<04:28,  2.83s/it] 37%|███▋      | 56/150 [02:36<04:25,  2.82s/it] 38%|███▊      | 57/150 [02:39<04:21,  2.82s/it] 39%|███▊      | 58/150 [02:42<04:18,  2.81s/it] 39%|███▉      | 59/150 [02:44<04:15,  2.80s/it] 40%|████      | 60/150 [02:47<04:12,  2.81s/it]                                                {'loss': 0.2863, 'grad_norm': 0.9057292342185974, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [02:47<04:12,  2.81s/it] 41%|████      | 61/150 [02:50<04:09,  2.80s/it] 41%|████▏     | 62/150 [02:53<04:06,  2.80s/it] 42%|████▏     | 63/150 [02:56<04:03,  2.80s/it] 43%|████▎     | 64/150 [02:58<03:59,  2.79s/it] 43%|████▎     | 65/150 [03:01<03:55,  2.77s/it] 44%|████▍     | 66/150 [03:04<03:51,  2.76s/it] 45%|████▍     | 67/150 [03:06<03:48,  2.76s/it] 45%|████▌     | 68/150 [03:09<03:46,  2.76s/it] 46%|████▌     | 69/150 [03:12<03:43,  2.76s/it] 47%|████▋     | 70/150 [03:15<03:41,  2.76s/it]                                                {'loss': 0.2498, 'grad_norm': 1.3288178443908691, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [03:15<03:41,  2.76s/it] 47%|████▋     | 71/150 [03:18<03:38,  2.77s/it] 48%|████▊     | 72/150 [03:20<03:35,  2.76s/it] 49%|████▊     | 73/150 [03:23<03:32,  2.76s/it] 49%|████▉     | 74/150 [03:26<03:29,  2.76s/it] 50%|█████     | 75/150 [03:29<03:27,  2.76s/it] 51%|█████     | 76/150 [03:31<03:25,  2.77s/it] 51%|█████▏    | 77/150 [03:34<03:22,  2.78s/it] 52%|█████▏    | 78/150 [03:37<03:19,  2.77s/it] 53%|█████▎    | 79/150 [03:40<03:15,  2.76s/it] 53%|█████▎    | 80/150 [03:42<03:12,  2.75s/it]                                                {'loss': 0.2575, 'grad_norm': 1.0270346403121948, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [03:42<03:12,  2.75s/it] 54%|█████▍    | 81/150 [03:45<03:10,  2.75s/it] 55%|█████▍    | 82/150 [03:48<03:08,  2.77s/it] 55%|█████▌    | 83/150 [03:51<03:06,  2.79s/it] 56%|█████▌    | 84/150 [03:54<03:03,  2.79s/it] 57%|█████▋    | 85/150 [03:56<03:00,  2.78s/it] 57%|█████▋    | 86/150 [03:59<02:57,  2.77s/it] 58%|█████▊    | 87/150 [04:02<02:54,  2.76s/it] 59%|█████▊    | 88/150 [04:05<02:52,  2.77s/it] 59%|█████▉    | 89/150 [04:07<02:49,  2.78s/it] 60%|██████    | 90/150 [04:10<02:47,  2.79s/it]                                                {'loss': 0.2539, 'grad_norm': 1.19440495967865, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [04:10<02:47,  2.79s/it] 61%|██████    | 91/150 [04:13<02:45,  2.80s/it] 61%|██████▏   | 92/150 [04:16<02:42,  2.80s/it] 62%|██████▏   | 93/150 [04:19<02:39,  2.79s/it] 63%|██████▎   | 94/150 [04:21<02:35,  2.78s/it] 63%|██████▎   | 95/150 [04:24<02:33,  2.78s/it] 64%|██████▍   | 96/150 [04:27<02:30,  2.78s/it] 65%|██████▍   | 97/150 [04:30<02:26,  2.77s/it] 65%|██████▌   | 98/150 [04:32<02:23,  2.76s/it] 66%|██████▌   | 99/150 [04:35<02:20,  2.75s/it] 67%|██████▋   | 100/150 [04:38<02:17,  2.75s/it]                                                 {'loss': 0.2679, 'grad_norm': 1.1663830280303955, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [04:38<02:17,  2.75s/it] 67%|██████▋   | 101/150 [04:41<02:15,  2.76s/it] 68%|██████▊   | 102/150 [04:43<02:12,  2.76s/it] 69%|██████▊   | 103/150 [04:46<02:09,  2.76s/it] 69%|██████▉   | 104/150 [04:49<02:06,  2.75s/it] 70%|███████   | 105/150 [04:52<02:03,  2.75s/it] 71%|███████   | 106/150 [04:55<02:02,  2.78s/it] 71%|███████▏  | 107/150 [04:57<01:59,  2.77s/it] 72%|███████▏  | 108/150 [05:00<01:56,  2.77s/it] 73%|███████▎  | 109/150 [05:03<01:53,  2.77s/it] 73%|███████▎  | 110/150 [05:06<01:50,  2.77s/it]                                                 {'loss': 0.2347, 'grad_norm': 1.3754003047943115, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [05:06<01:50,  2.77s/it] 74%|███████▍  | 111/150 [05:08<01:48,  2.77s/it] 75%|███████▍  | 112/150 [05:11<01:45,  2.79s/it] 75%|███████▌  | 113/150 [05:14<01:43,  2.80s/it] 76%|███████▌  | 114/150 [05:17<01:40,  2.79s/it] 77%|███████▋  | 115/150 [05:20<01:38,  2.83s/it] 77%|███████▋  | 116/150 [05:22<01:35,  2.81s/it] 78%|███████▊  | 117/150 [05:25<01:32,  2.80s/it] 79%|███████▊  | 118/150 [05:28<01:29,  2.80s/it] 79%|███████▉  | 119/150 [05:31<01:26,  2.79s/it] 80%|████████  | 120/150 [05:34<01:23,  2.78s/it]                                                 {'loss': 0.2606, 'grad_norm': 1.0007938146591187, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [05:34<01:23,  2.78s/it] 81%|████████  | 121/150 [05:36<01:20,  2.79s/it] 81%|████████▏ | 122/150 [05:39<01:18,  2.79s/it] 82%|████████▏ | 123/150 [05:42<01:15,  2.80s/it] 83%|████████▎ | 124/150 [05:45<01:12,  2.80s/it] 83%|████████▎ | 125/150 [05:48<01:09,  2.80s/it] 84%|████████▍ | 126/150 [05:50<01:06,  2.79s/it] 85%|████████▍ | 127/150 [05:53<01:04,  2.78s/it] 85%|████████▌ | 128/150 [05:56<01:01,  2.78s/it] 86%|████████▌ | 129/150 [05:59<00:58,  2.78s/it] 87%|████████▋ | 130/150 [06:01<00:55,  2.79s/it]                                                 {'loss': 0.1932, 'grad_norm': 0.6761857271194458, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [06:01<00:55,  2.79s/it] 87%|████████▋ | 131/150 [06:04<00:52,  2.78s/it] 88%|████████▊ | 132/150 [06:07<00:50,  2.78s/it] 89%|████████▊ | 133/150 [06:10<00:47,  2.78s/it] 89%|████████▉ | 134/150 [06:13<00:44,  2.78s/it] 90%|█████████ | 135/150 [06:15<00:41,  2.77s/it] 91%|█████████ | 136/150 [06:18<00:38,  2.76s/it] 91%|█████████▏| 137/150 [06:21<00:36,  2.77s/it] 92%|█████████▏| 138/150 [06:24<00:33,  2.77s/it] 93%|█████████▎| 139/150 [06:26<00:30,  2.77s/it] 93%|█████████▎| 140/150 [06:29<00:27,  2.76s/it]                                                 {'loss': 0.2377, 'grad_norm': 1.1745046377182007, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [06:29<00:27,  2.76s/it] 94%|█████████▍| 141/150 [06:32<00:24,  2.77s/it] 95%|█████████▍| 142/150 [06:35<00:22,  2.77s/it] 95%|█████████▌| 143/150 [06:38<00:19,  2.77s/it] 96%|█████████▌| 144/150 [06:40<00:16,  2.78s/it] 97%|█████████▋| 145/150 [06:43<00:13,  2.78s/it] 97%|█████████▋| 146/150 [06:46<00:11,  2.78s/it] 98%|█████████▊| 147/150 [06:49<00:08,  2.78s/it] 99%|█████████▊| 148/150 [06:51<00:05,  2.77s/it] 99%|█████████▉| 149/150 [06:54<00:02,  2.77s/it]100%|██████████| 150/150 [06:57<00:00,  2.76s/it]                                                 {'loss': 0.1953, 'grad_norm': 1.160831093788147, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [06:57<00:00,  2.76s/it][INFO|trainer.py:3705] 2024-11-18 22:00:07,604 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:00:07,639 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:07,640 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:00:07,710 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:00:07,711 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:00:08,179 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 417.9846, 'train_samples_per_second': 5.785, 'train_steps_per_second': 0.359, 'train_loss': 0.40669291257858275, 'epoch': 2.98}
100%|██████████| 150/150 [06:57<00:00,  2.76s/it]100%|██████████| 150/150 [06:57<00:00,  2.79s/it]
[INFO|trainer.py:3705] 2024-11-18 22:00:08,182 >> Saving model checkpoint to saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:00:08,212 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-0.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:08,214 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:00:08,281 >> tokenizer config file saved in saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:00:08,282 >> Special tokens file saved in saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               =  4839786GF
  train_loss               =     0.4067
  train_runtime            = 0:06:57.98
  train_samples_per_second =      5.785
  train_steps_per_second   =      0.359
Figure saved at: saves/Qwen2.5-0.5B-Instruct/my_prompt_modelsize0.5B/lora/sft/training_loss.png
11/18/2024 22:00:08 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:00:08 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:00:08,745 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:00:08,745 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:00:08,745 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  7%|▋         | 3/45 [00:00<00:02, 17.96it/s] 11%|█         | 5/45 [00:00<00:02, 14.23it/s] 16%|█▌        | 7/45 [00:00<00:02, 13.23it/s] 20%|██        | 9/45 [00:00<00:02, 12.84it/s] 24%|██▍       | 11/45 [00:00<00:02, 12.62it/s] 29%|██▉       | 13/45 [00:00<00:02, 12.48it/s] 33%|███▎      | 15/45 [00:01<00:02, 12.41it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.31it/s] 42%|████▏     | 19/45 [00:01<00:02, 12.26it/s] 47%|████▋     | 21/45 [00:01<00:01, 12.22it/s] 51%|█████     | 23/45 [00:01<00:01, 12.19it/s] 56%|█████▌    | 25/45 [00:01<00:01, 12.17it/s] 60%|██████    | 27/45 [00:02<00:01, 12.14it/s] 64%|██████▍   | 29/45 [00:02<00:01, 12.13it/s] 69%|██████▉   | 31/45 [00:02<00:01, 12.14it/s] 73%|███████▎  | 33/45 [00:02<00:00, 12.13it/s] 78%|███████▊  | 35/45 [00:02<00:00, 12.12it/s] 82%|████████▏ | 37/45 [00:02<00:00, 12.11it/s] 87%|████████▋ | 39/45 [00:03<00:00, 12.11it/s] 91%|█████████ | 41/45 [00:03<00:00, 12.14it/s] 96%|█████████▌| 43/45 [00:03<00:00, 12.12it/s]100%|██████████| 45/45 [00:03<00:00, 12.19it/s]100%|██████████| 45/45 [00:03<00:00, 12.39it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.2124
  eval_runtime            = 0:00:03.73
  eval_samples_per_second =     24.097
  eval_steps_per_second   =     12.048
[INFO|modelcard.py:449] 2024-11-18 22:00:12,480 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:00:31,208] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:00:35 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26108
[2024-11-18 22:00:37,331] torch.distributed.run: [WARNING] 
[2024-11-18 22:00:37,331] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:00:37,331] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:00:37,331] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:00:44,758] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:00:44,839] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:00:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:00:46 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:00:46,331 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:46,332 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,334 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:00:46,791 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:00:46,792 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:46,793 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:46,794 >> loading file tokenizer_config.json
11/18/2024 22:00:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:00:46 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:00:47,258 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:00:47 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:00:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:00:47 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/18/2024 22:00:47 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:00:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 387.26 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3056.64 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:00:51 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:18, 45.14 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 91.54 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 135.96 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 176.16 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:02, 209.78 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 236.78 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:01, 256.30 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 273.14 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 222.63 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:02<00:00, 355.56 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 329.43 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 327.60 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 332.43 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 356.74 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 364.45 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 233.64 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:00:55,954 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:55,956 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:00:56,005 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/model.safetensors
[INFO|modeling_utils.py:1622] 2024-11-18 22:00:56,040 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:00:56,045 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:4574] 2024-11-18 22:00:57,255 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:00:57,255 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:00:57,259 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:00:57,260 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,gate_proj,v_proj,k_proj,up_proj,o_proj,down_proj
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,o_proj,q_proj,v_proj,gate_proj,up_proj
11/18/2024 22:00:57 - INFO - llamafactory.model.loader - trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:00:57,805 >> Using auto half precision backend
11/18/2024 22:00:57 - INFO - llamafactory.model.loader - trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945
[INFO|trainer.py:2243] 2024-11-18 22:00:58,464 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:00:58,465 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:00:58,465 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:00:58,465 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:00:58,465 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:00:58,465 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:00:58,465 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:00:58,471 >>   Number of trainable parameters = 9,232,384
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:03<09:36,  3.87s/it]  1%|▏         | 2/150 [00:07<08:54,  3.61s/it]  2%|▏         | 3/150 [00:10<08:18,  3.39s/it]  3%|▎         | 4/150 [00:13<07:58,  3.27s/it]  3%|▎         | 5/150 [00:16<07:46,  3.22s/it]  4%|▍         | 6/150 [00:19<07:36,  3.17s/it]  5%|▍         | 7/150 [00:22<07:29,  3.14s/it]  5%|▌         | 8/150 [00:25<07:23,  3.12s/it]  6%|▌         | 9/150 [00:28<07:19,  3.11s/it]  7%|▋         | 10/150 [00:32<07:15,  3.11s/it]                                                {'loss': 1.4929, 'grad_norm': 1.69330894947052, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:32<07:15,  3.11s/it]  7%|▋         | 11/150 [00:35<07:13,  3.12s/it]  8%|▊         | 12/150 [00:38<07:11,  3.12s/it]  9%|▊         | 13/150 [00:41<07:08,  3.12s/it]  9%|▉         | 14/150 [00:44<07:05,  3.13s/it] 10%|█         | 15/150 [00:47<07:02,  3.13s/it] 11%|█         | 16/150 [00:50<06:57,  3.12s/it] 11%|█▏        | 17/150 [00:53<06:54,  3.12s/it] 12%|█▏        | 18/150 [00:57<06:52,  3.12s/it] 13%|█▎        | 19/150 [01:00<06:49,  3.13s/it] 13%|█▎        | 20/150 [01:03<06:46,  3.13s/it]                                                {'loss': 0.8768, 'grad_norm': 1.067380666732788, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:03<06:46,  3.13s/it] 14%|█▍        | 21/150 [01:06<06:44,  3.13s/it] 15%|█▍        | 22/150 [01:09<06:41,  3.13s/it] 15%|█▌        | 23/150 [01:12<06:38,  3.14s/it] 16%|█▌        | 24/150 [01:15<06:34,  3.13s/it] 17%|█▋        | 25/150 [01:18<06:29,  3.12s/it] 17%|█▋        | 26/150 [01:22<06:26,  3.12s/it] 18%|█▊        | 27/150 [01:25<06:24,  3.13s/it] 19%|█▊        | 28/150 [01:28<06:21,  3.13s/it] 19%|█▉        | 29/150 [01:31<06:18,  3.13s/it] 20%|██        | 30/150 [01:34<06:15,  3.13s/it]                                                {'loss': 0.4753, 'grad_norm': 0.7049703001976013, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:34<06:15,  3.13s/it] 21%|██        | 31/150 [01:37<06:11,  3.12s/it] 21%|██▏       | 32/150 [01:40<06:08,  3.12s/it] 22%|██▏       | 33/150 [01:43<06:04,  3.12s/it] 23%|██▎       | 34/150 [01:47<06:02,  3.12s/it] 23%|██▎       | 35/150 [01:50<06:00,  3.13s/it] 24%|██▍       | 36/150 [01:53<05:56,  3.13s/it] 25%|██▍       | 37/150 [01:56<05:52,  3.12s/it] 25%|██▌       | 38/150 [01:59<05:48,  3.11s/it] 26%|██▌       | 39/150 [02:02<05:46,  3.12s/it] 27%|██▋       | 40/150 [02:05<05:43,  3.12s/it]                                                {'loss': 0.4047, 'grad_norm': 0.6960530877113342, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:05<05:43,  3.12s/it] 27%|██▋       | 41/150 [02:09<05:41,  3.13s/it] 28%|██▊       | 42/150 [02:12<05:38,  3.13s/it] 29%|██▊       | 43/150 [02:15<05:35,  3.14s/it] 29%|██▉       | 44/150 [02:18<05:33,  3.15s/it] 30%|███       | 45/150 [02:21<05:31,  3.15s/it] 31%|███       | 46/150 [02:24<05:28,  3.16s/it] 31%|███▏      | 47/150 [02:27<05:23,  3.14s/it] 32%|███▏      | 48/150 [02:30<05:18,  3.12s/it] 33%|███▎      | 49/150 [02:34<05:15,  3.12s/it] 33%|███▎      | 50/150 [02:37<05:13,  3.14s/it]                                                {'loss': 0.3138, 'grad_norm': 1.0264613628387451, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [02:37<05:13,  3.14s/it] 34%|███▍      | 51/150 [02:40<05:11,  3.15s/it] 35%|███▍      | 52/150 [02:43<05:08,  3.15s/it] 35%|███▌      | 53/150 [02:46<05:06,  3.16s/it] 36%|███▌      | 54/150 [02:49<05:03,  3.16s/it] 37%|███▋      | 55/150 [02:53<04:59,  3.15s/it] 37%|███▋      | 56/150 [02:56<04:54,  3.13s/it] 38%|███▊      | 57/150 [02:59<04:49,  3.12s/it] 39%|███▊      | 58/150 [03:02<04:46,  3.12s/it] 39%|███▉      | 59/150 [03:05<04:45,  3.14s/it] 40%|████      | 60/150 [03:08<04:43,  3.15s/it]                                                {'loss': 0.2707, 'grad_norm': 0.5754202008247375, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:08<04:43,  3.15s/it] 41%|████      | 61/150 [03:11<04:40,  3.15s/it] 41%|████▏     | 62/150 [03:15<04:37,  3.15s/it] 42%|████▏     | 63/150 [03:18<04:34,  3.15s/it] 43%|████▎     | 64/150 [03:21<04:30,  3.15s/it] 43%|████▎     | 65/150 [03:24<04:27,  3.14s/it] 44%|████▍     | 66/150 [03:27<04:24,  3.15s/it] 45%|████▍     | 67/150 [03:30<04:20,  3.14s/it] 45%|████▌     | 68/150 [03:33<04:16,  3.13s/it] 46%|████▌     | 69/150 [03:36<04:13,  3.13s/it] 47%|████▋     | 70/150 [03:40<04:09,  3.12s/it]                                                {'loss': 0.2406, 'grad_norm': 0.6395409107208252, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [03:40<04:09,  3.12s/it] 47%|████▋     | 71/150 [03:43<04:06,  3.13s/it] 48%|████▊     | 72/150 [03:46<04:04,  3.13s/it] 49%|████▊     | 73/150 [03:49<04:01,  3.14s/it] 49%|████▉     | 74/150 [03:52<03:58,  3.14s/it] 50%|█████     | 75/150 [03:55<03:55,  3.14s/it] 51%|█████     | 76/150 [03:58<03:52,  3.15s/it] 51%|█████▏    | 77/150 [04:02<03:50,  3.15s/it] 52%|█████▏    | 78/150 [04:05<03:47,  3.16s/it] 53%|█████▎    | 79/150 [04:08<03:44,  3.16s/it] 53%|█████▎    | 80/150 [04:11<03:41,  3.16s/it]                                                {'loss': 0.2414, 'grad_norm': 0.4743410050868988, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:11<03:41,  3.16s/it] 54%|█████▍    | 81/150 [04:14<03:37,  3.16s/it] 55%|█████▍    | 82/150 [04:17<03:34,  3.16s/it] 55%|█████▌    | 83/150 [04:21<03:30,  3.14s/it] 56%|█████▌    | 84/150 [04:24<03:27,  3.15s/it] 57%|█████▋    | 85/150 [04:27<03:25,  3.16s/it] 57%|█████▋    | 86/150 [04:30<03:22,  3.16s/it] 58%|█████▊    | 87/150 [04:33<03:19,  3.16s/it] 59%|█████▊    | 88/150 [04:36<03:16,  3.16s/it] 59%|█████▉    | 89/150 [04:40<03:13,  3.17s/it] 60%|██████    | 90/150 [04:43<03:10,  3.17s/it]                                                {'loss': 0.2472, 'grad_norm': 0.7082294821739197, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [04:43<03:10,  3.17s/it] 61%|██████    | 91/150 [04:46<03:06,  3.16s/it] 61%|██████▏   | 92/150 [04:49<03:03,  3.16s/it] 62%|██████▏   | 93/150 [04:52<02:59,  3.14s/it] 63%|██████▎   | 94/150 [04:55<02:55,  3.13s/it] 63%|██████▎   | 95/150 [04:58<02:52,  3.14s/it] 64%|██████▍   | 96/150 [05:02<02:50,  3.16s/it] 65%|██████▍   | 97/150 [05:05<02:47,  3.17s/it] 65%|██████▌   | 98/150 [05:08<02:45,  3.18s/it] 66%|██████▌   | 99/150 [05:11<02:42,  3.18s/it] 67%|██████▋   | 100/150 [05:14<02:39,  3.19s/it]                                                 {'loss': 0.2567, 'grad_norm': 0.7563595771789551, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [05:14<02:39,  3.19s/it] 67%|██████▋   | 101/150 [05:18<02:35,  3.18s/it] 68%|██████▊   | 102/150 [05:21<02:32,  3.18s/it] 69%|██████▊   | 103/150 [05:24<02:29,  3.17s/it] 69%|██████▉   | 104/150 [05:27<02:25,  3.17s/it] 70%|███████   | 105/150 [05:30<02:23,  3.18s/it] 71%|███████   | 106/150 [05:33<02:19,  3.18s/it] 71%|███████▏  | 107/150 [05:37<02:16,  3.18s/it] 72%|███████▏  | 108/150 [05:40<02:13,  3.18s/it] 73%|███████▎  | 109/150 [05:43<02:09,  3.16s/it] 73%|███████▎  | 110/150 [05:46<02:06,  3.16s/it]                                                 {'loss': 0.2274, 'grad_norm': 0.8658501505851746, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [05:46<02:06,  3.16s/it] 74%|███████▍  | 111/150 [05:49<02:03,  3.16s/it] 75%|███████▍  | 112/150 [05:52<01:59,  3.16s/it] 75%|███████▌  | 113/150 [05:56<01:56,  3.16s/it] 76%|███████▌  | 114/150 [05:59<01:53,  3.15s/it] 77%|███████▋  | 115/150 [06:02<01:50,  3.15s/it] 77%|███████▋  | 116/150 [06:05<01:46,  3.14s/it] 78%|███████▊  | 117/150 [06:08<01:43,  3.15s/it] 79%|███████▊  | 118/150 [06:11<01:40,  3.14s/it] 79%|███████▉  | 119/150 [06:14<01:37,  3.14s/it] 80%|████████  | 120/150 [06:17<01:34,  3.14s/it]                                                 {'loss': 0.2542, 'grad_norm': 0.566143274307251, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [06:18<01:34,  3.14s/it] 81%|████████  | 121/150 [06:21<01:30,  3.13s/it] 81%|████████▏ | 122/150 [06:24<01:27,  3.12s/it] 82%|████████▏ | 123/150 [06:27<01:24,  3.12s/it] 83%|████████▎ | 124/150 [06:30<01:20,  3.11s/it] 83%|████████▎ | 125/150 [06:33<01:17,  3.10s/it] 84%|████████▍ | 126/150 [06:36<01:14,  3.09s/it] 85%|████████▍ | 127/150 [06:39<01:10,  3.09s/it] 85%|████████▌ | 128/150 [06:42<01:07,  3.08s/it] 86%|████████▌ | 129/150 [06:45<01:04,  3.07s/it] 87%|████████▋ | 130/150 [06:48<01:01,  3.07s/it]                                                 {'loss': 0.1886, 'grad_norm': 0.40137094259262085, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [06:48<01:01,  3.07s/it] 87%|████████▋ | 131/150 [06:51<00:58,  3.07s/it] 88%|████████▊ | 132/150 [06:54<00:55,  3.07s/it] 89%|████████▊ | 133/150 [06:58<00:52,  3.07s/it] 89%|████████▉ | 134/150 [07:01<00:49,  3.07s/it] 90%|█████████ | 135/150 [07:04<00:46,  3.09s/it] 91%|█████████ | 136/150 [07:07<00:43,  3.10s/it] 91%|█████████▏| 137/150 [07:10<00:40,  3.10s/it] 92%|█████████▏| 138/150 [07:13<00:37,  3.11s/it] 93%|█████████▎| 139/150 [07:16<00:34,  3.13s/it] 93%|█████████▎| 140/150 [07:19<00:31,  3.13s/it]                                                 {'loss': 0.2342, 'grad_norm': 0.7242642045021057, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [07:19<00:31,  3.13s/it] 94%|█████████▍| 141/150 [07:23<00:28,  3.13s/it] 95%|█████████▍| 142/150 [07:26<00:24,  3.11s/it] 95%|█████████▌| 143/150 [07:29<00:21,  3.11s/it] 96%|█████████▌| 144/150 [07:32<00:18,  3.10s/it] 97%|█████████▋| 145/150 [07:35<00:15,  3.09s/it] 97%|█████████▋| 146/150 [07:38<00:12,  3.08s/it] 98%|█████████▊| 147/150 [07:41<00:09,  3.07s/it] 99%|█████████▊| 148/150 [07:44<00:06,  3.07s/it] 99%|█████████▉| 149/150 [07:47<00:03,  3.06s/it]100%|██████████| 150/150 [07:50<00:00,  3.05s/it]                                                 {'loss': 0.1932, 'grad_norm': 0.5683890581130981, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [07:50<00:00,  3.05s/it][INFO|trainer.py:3705] 2024-11-18 22:08:49,183 >> Saving model checkpoint to saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:08:49,227 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:08:49,228 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:08:49,327 >> tokenizer config file saved in saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:08:49,328 >> Special tokens file saved in saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:08:49,923 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 471.4515, 'train_samples_per_second': 5.129, 'train_steps_per_second': 0.318, 'train_loss': 0.3945071387290955, 'epoch': 2.98}
100%|██████████| 150/150 [07:51<00:00,  3.05s/it]100%|██████████| 150/150 [07:51<00:00,  3.14s/it]
[INFO|trainer.py:3705] 2024-11-18 22:08:49,926 >> Saving model checkpoint to saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:08:49,958 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:08:49,959 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:08:50,055 >> tokenizer config file saved in saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:08:50,055 >> Special tokens file saved in saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 17627658GF
  train_loss               =     0.3945
  train_runtime            = 0:07:51.45
  train_samples_per_second =      5.129
  train_steps_per_second   =      0.318
Figure saved at: saves/Qwen2.5-1.5B-Instruct/my_prompt_modelsize1.5B/lora/sft/training_loss.png
11/18/2024 22:08:50 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:08:50 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:08:50,514 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:08:50,514 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:08:50,515 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  7%|▋         | 3/45 [00:00<00:02, 15.94it/s] 11%|█         | 5/45 [00:00<00:03, 12.63it/s] 16%|█▌        | 7/45 [00:00<00:03, 11.73it/s] 20%|██        | 9/45 [00:00<00:03, 11.36it/s] 24%|██▍       | 11/45 [00:00<00:03, 11.14it/s] 29%|██▉       | 13/45 [00:01<00:02, 11.01it/s] 33%|███▎      | 15/45 [00:01<00:02, 10.92it/s] 38%|███▊      | 17/45 [00:01<00:02, 10.86it/s] 42%|████▏     | 19/45 [00:01<00:02, 10.82it/s] 47%|████▋     | 21/45 [00:01<00:02, 10.79it/s] 51%|█████     | 23/45 [00:02<00:02, 10.77it/s] 56%|█████▌    | 25/45 [00:02<00:01, 10.76it/s] 60%|██████    | 27/45 [00:02<00:01, 10.75it/s] 64%|██████▍   | 29/45 [00:02<00:01, 10.73it/s] 69%|██████▉   | 31/45 [00:02<00:01, 10.73it/s] 73%|███████▎  | 33/45 [00:02<00:01, 10.73it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.74it/s] 82%|████████▏ | 37/45 [00:03<00:00, 10.73it/s] 87%|████████▋ | 39/45 [00:03<00:00, 10.71it/s] 91%|█████████ | 41/45 [00:03<00:00, 10.70it/s] 96%|█████████▌| 43/45 [00:03<00:00, 10.69it/s]100%|██████████| 45/45 [00:04<00:00, 10.74it/s]100%|██████████| 45/45 [00:04<00:00, 10.94it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.2042
  eval_runtime            = 0:00:04.22
  eval_samples_per_second =     21.288
  eval_steps_per_second   =     10.644
[INFO|modelcard.py:449] 2024-11-18 22:08:54,742 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:09:14,996] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:09:18 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:28471
[2024-11-18 22:09:20,857] torch.distributed.run: [WARNING] 
[2024-11-18 22:09:20,857] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:09:20,857] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:09:20,857] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:09:28,288] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:09:28,466] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:09:29 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:09:29 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:09:29 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:09:29 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:09:29,644 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:09:29,645 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:29,647 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:09:30,146 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:09:30,147 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:09:30,148 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:09:30,149 >> loading file tokenizer_config.json
11/18/2024 22:09:30 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:09:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:09:30,638 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:09:30 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:09:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:09:30 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 408.23 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3013.21 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:09:35 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:18, 45.55 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 90.49 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 133.75 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:03, 171.97 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:03, 204.37 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 227.12 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 245.22 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 262.41 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 276.30 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:02<00:01, 283.33 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 344.50 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 325.57 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 360.31 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 339.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 299.39 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 224.48 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:09:39,752 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:09:39,754 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:09:39,805 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:09:39,806 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:09:39,808 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:09:42,050 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:09:42,050 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:09:42,055 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:09:42,056 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:09:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:09:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,up_proj,v_proj,k_proj,down_proj,o_proj
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.00s/it]
11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:09:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:09:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:09:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,gate_proj,q_proj,v_proj,o_proj,up_proj,k_proj
11/18/2024 22:09:42 - INFO - llamafactory.model.loader - trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:09:42,791 >> Using auto half precision backend
11/18/2024 22:09:42 - INFO - llamafactory.model.loader - trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827
[INFO|trainer.py:2243] 2024-11-18 22:09:43,554 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:09:43,554 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:09:43,554 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:09:43,554 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:09:43,554 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:09:43,554 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:09:43,554 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:09:43,562 >>   Number of trainable parameters = 14,966,784
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<11:23,  4.58s/it]  1%|▏         | 2/150 [00:08<10:38,  4.32s/it]  2%|▏         | 3/150 [00:12<10:24,  4.25s/it]  3%|▎         | 4/150 [00:17<10:15,  4.22s/it]  3%|▎         | 5/150 [00:21<10:05,  4.18s/it]  4%|▍         | 6/150 [00:25<09:59,  4.16s/it]  5%|▍         | 7/150 [00:29<09:55,  4.16s/it]  5%|▌         | 8/150 [00:33<09:49,  4.15s/it]  6%|▌         | 9/150 [00:37<09:43,  4.14s/it]  7%|▋         | 10/150 [00:41<09:39,  4.14s/it]                                                {'loss': 1.677, 'grad_norm': 2.084817409515381, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:41<09:39,  4.14s/it]  7%|▋         | 11/150 [00:45<09:35,  4.14s/it]  8%|▊         | 12/150 [00:50<09:30,  4.13s/it]  9%|▊         | 13/150 [00:54<09:24,  4.12s/it]  9%|▉         | 14/150 [00:58<09:19,  4.11s/it] 10%|█         | 15/150 [01:02<09:14,  4.11s/it] 11%|█         | 16/150 [01:06<09:10,  4.11s/it] 11%|█▏        | 17/150 [01:10<09:06,  4.11s/it] 12%|█▏        | 18/150 [01:14<09:04,  4.13s/it] 13%|█▎        | 19/150 [01:18<09:01,  4.13s/it] 13%|█▎        | 20/150 [01:23<08:57,  4.13s/it]                                                {'loss': 0.9145, 'grad_norm': 0.910891056060791, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:23<08:57,  4.13s/it] 14%|█▍        | 21/150 [01:27<08:54,  4.14s/it] 15%|█▍        | 22/150 [01:31<08:51,  4.15s/it] 15%|█▌        | 23/150 [01:35<08:46,  4.14s/it] 16%|█▌        | 24/150 [01:39<08:41,  4.14s/it] 17%|█▋        | 25/150 [01:43<08:36,  4.14s/it] 17%|█▋        | 26/150 [01:47<08:33,  4.14s/it] 18%|█▊        | 27/150 [01:52<08:29,  4.14s/it] 19%|█▊        | 28/150 [01:56<08:25,  4.14s/it] 19%|█▉        | 29/150 [02:00<08:21,  4.14s/it] 20%|██        | 30/150 [02:04<08:16,  4.13s/it]                                                {'loss': 0.466, 'grad_norm': 0.5494835376739502, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [02:04<08:16,  4.13s/it] 21%|██        | 31/150 [02:08<08:10,  4.12s/it] 21%|██▏       | 32/150 [02:12<08:04,  4.11s/it] 22%|██▏       | 33/150 [02:16<08:00,  4.11s/it] 23%|██▎       | 34/150 [02:20<07:55,  4.10s/it] 23%|██▎       | 35/150 [02:24<07:51,  4.10s/it] 24%|██▍       | 36/150 [02:29<07:48,  4.11s/it] 25%|██▍       | 37/150 [02:33<07:44,  4.11s/it] 25%|██▌       | 38/150 [02:37<07:40,  4.11s/it] 26%|██▌       | 39/150 [02:41<07:36,  4.11s/it] 27%|██▋       | 40/150 [02:45<07:32,  4.12s/it]                                                {'loss': 0.361, 'grad_norm': 0.5158628225326538, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:45<07:32,  4.12s/it] 27%|██▋       | 41/150 [02:49<07:27,  4.11s/it] 28%|██▊       | 42/150 [02:53<07:23,  4.11s/it] 29%|██▊       | 43/150 [02:57<07:18,  4.10s/it] 29%|██▉       | 44/150 [03:01<07:14,  4.10s/it] 30%|███       | 45/150 [03:05<07:10,  4.10s/it] 31%|███       | 46/150 [03:10<07:05,  4.10s/it] 31%|███▏      | 47/150 [03:14<07:01,  4.09s/it] 32%|███▏      | 48/150 [03:18<06:57,  4.09s/it] 33%|███▎      | 49/150 [03:22<06:53,  4.10s/it] 33%|███▎      | 50/150 [03:26<06:49,  4.10s/it]                                                {'loss': 0.2775, 'grad_norm': 0.512855589389801, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:26<06:49,  4.10s/it] 34%|███▍      | 51/150 [03:30<06:46,  4.11s/it] 35%|███▍      | 52/150 [03:34<06:42,  4.11s/it] 35%|███▌      | 53/150 [03:38<06:38,  4.11s/it] 36%|███▌      | 54/150 [03:42<06:33,  4.10s/it] 37%|███▋      | 55/150 [03:47<06:30,  4.11s/it] 37%|███▋      | 56/150 [03:51<06:26,  4.11s/it] 38%|███▊      | 57/150 [03:55<06:22,  4.11s/it] 39%|███▊      | 58/150 [03:59<06:17,  4.10s/it] 39%|███▉      | 59/150 [04:03<06:13,  4.10s/it] 40%|████      | 60/150 [04:07<06:09,  4.11s/it]                                                {'loss': 0.2452, 'grad_norm': 0.6020147204399109, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [04:07<06:09,  4.11s/it] 41%|████      | 61/150 [04:11<06:05,  4.10s/it] 41%|████▏     | 62/150 [04:15<06:01,  4.10s/it] 42%|████▏     | 63/150 [04:19<05:57,  4.11s/it] 43%|████▎     | 64/150 [04:23<05:53,  4.11s/it] 43%|████▎     | 65/150 [04:28<05:48,  4.10s/it] 44%|████▍     | 66/150 [04:32<05:43,  4.09s/it] 45%|████▍     | 67/150 [04:36<05:39,  4.10s/it] 45%|████▌     | 68/150 [04:40<05:36,  4.10s/it] 46%|████▌     | 69/150 [04:44<05:32,  4.10s/it] 47%|████▋     | 70/150 [04:48<05:27,  4.10s/it]                                                {'loss': 0.2215, 'grad_norm': 0.5350908637046814, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:48<05:27,  4.10s/it] 47%|████▋     | 71/150 [04:52<05:24,  4.11s/it] 48%|████▊     | 72/150 [04:56<05:21,  4.12s/it] 49%|████▊     | 73/150 [05:00<05:18,  4.13s/it] 49%|████▉     | 74/150 [05:05<05:15,  4.15s/it] 50%|█████     | 75/150 [05:09<05:10,  4.14s/it] 51%|█████     | 76/150 [05:13<05:04,  4.12s/it] 51%|█████▏    | 77/150 [05:17<05:00,  4.11s/it] 52%|█████▏    | 78/150 [05:21<04:55,  4.11s/it] 53%|█████▎    | 79/150 [05:25<04:51,  4.10s/it] 53%|█████▎    | 80/150 [05:29<04:47,  4.10s/it]                                                {'loss': 0.2274, 'grad_norm': 0.40712860226631165, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [05:29<04:47,  4.10s/it] 54%|█████▍    | 81/150 [05:33<04:44,  4.12s/it] 55%|█████▍    | 82/150 [05:38<04:40,  4.13s/it] 55%|█████▌    | 83/150 [05:42<04:36,  4.12s/it] 56%|█████▌    | 84/150 [05:46<04:32,  4.13s/it] 57%|█████▋    | 85/150 [05:50<04:27,  4.12s/it] 57%|█████▋    | 86/150 [05:54<04:24,  4.13s/it] 58%|█████▊    | 87/150 [05:58<04:21,  4.15s/it] 59%|█████▊    | 88/150 [06:02<04:17,  4.16s/it] 59%|█████▉    | 89/150 [06:07<04:12,  4.15s/it] 60%|██████    | 90/150 [06:11<04:09,  4.16s/it]                                                {'loss': 0.2328, 'grad_norm': 0.5300176739692688, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [06:11<04:09,  4.16s/it] 61%|██████    | 91/150 [06:15<04:05,  4.16s/it] 61%|██████▏   | 92/150 [06:19<04:01,  4.17s/it] 62%|██████▏   | 93/150 [06:23<03:57,  4.17s/it] 63%|██████▎   | 94/150 [06:27<03:53,  4.17s/it] 63%|██████▎   | 95/150 [06:32<03:48,  4.15s/it] 64%|██████▍   | 96/150 [06:36<03:43,  4.13s/it] 65%|██████▍   | 97/150 [06:40<03:38,  4.12s/it] 65%|██████▌   | 98/150 [06:44<03:34,  4.13s/it] 66%|██████▌   | 99/150 [06:48<03:30,  4.13s/it] 67%|██████▋   | 100/150 [06:52<03:26,  4.13s/it]                                                 {'loss': 0.2428, 'grad_norm': 0.4230273365974426, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:52<03:26,  4.13s/it] 67%|██████▋   | 101/150 [06:56<03:22,  4.14s/it] 68%|██████▊   | 102/150 [07:00<03:18,  4.13s/it] 69%|██████▊   | 103/150 [07:04<03:13,  4.11s/it] 69%|██████▉   | 104/150 [07:09<03:08,  4.09s/it] 70%|███████   | 105/150 [07:13<03:03,  4.08s/it] 71%|███████   | 106/150 [07:17<02:59,  4.07s/it] 71%|███████▏  | 107/150 [07:21<02:54,  4.07s/it] 72%|███████▏  | 108/150 [07:25<02:50,  4.06s/it] 73%|███████▎  | 109/150 [07:29<02:46,  4.07s/it] 73%|███████▎  | 110/150 [07:33<02:43,  4.09s/it]                                                 {'loss': 0.2114, 'grad_norm': 0.8159031271934509, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [07:33<02:43,  4.09s/it] 74%|███████▍  | 111/150 [07:37<02:39,  4.10s/it] 75%|███████▍  | 112/150 [07:41<02:35,  4.10s/it] 75%|███████▌  | 113/150 [07:45<02:31,  4.11s/it] 76%|███████▌  | 114/150 [07:49<02:27,  4.11s/it] 77%|███████▋  | 115/150 [07:54<02:23,  4.11s/it] 77%|███████▋  | 116/150 [07:58<02:19,  4.10s/it] 78%|███████▊  | 117/150 [08:02<02:15,  4.11s/it] 79%|███████▊  | 118/150 [08:06<02:11,  4.12s/it] 79%|███████▉  | 119/150 [08:10<02:08,  4.14s/it] 80%|████████  | 120/150 [08:14<02:04,  4.16s/it]                                                 {'loss': 0.2381, 'grad_norm': 0.6075013875961304, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [08:14<02:04,  4.16s/it] 81%|████████  | 121/150 [08:18<02:00,  4.16s/it] 81%|████████▏ | 122/150 [08:23<01:56,  4.16s/it] 82%|████████▏ | 123/150 [08:27<01:52,  4.17s/it] 83%|████████▎ | 124/150 [08:31<01:48,  4.16s/it] 83%|████████▎ | 125/150 [08:35<01:43,  4.14s/it] 84%|████████▍ | 126/150 [08:39<01:39,  4.13s/it] 85%|████████▍ | 127/150 [08:43<01:35,  4.14s/it] 85%|████████▌ | 128/150 [08:47<01:31,  4.15s/it] 86%|████████▌ | 129/150 [08:52<01:27,  4.16s/it] 87%|████████▋ | 130/150 [08:56<01:23,  4.16s/it]                                                 {'loss': 0.1796, 'grad_norm': 0.32810071110725403, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:56<01:23,  4.16s/it] 87%|████████▋ | 131/150 [09:00<01:19,  4.16s/it] 88%|████████▊ | 132/150 [09:04<01:14,  4.16s/it] 89%|████████▊ | 133/150 [09:08<01:10,  4.15s/it] 89%|████████▉ | 134/150 [09:12<01:06,  4.16s/it] 90%|█████████ | 135/150 [09:17<01:02,  4.16s/it] 91%|█████████ | 136/150 [09:21<00:58,  4.17s/it] 91%|█████████▏| 137/150 [09:25<00:54,  4.17s/it] 92%|█████████▏| 138/150 [09:29<00:49,  4.16s/it] 93%|█████████▎| 139/150 [09:33<00:45,  4.15s/it] 93%|█████████▎| 140/150 [09:37<00:41,  4.13s/it]                                                 {'loss': 0.2203, 'grad_norm': 0.5825507044792175, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [09:37<00:41,  4.13s/it] 94%|█████████▍| 141/150 [09:41<00:37,  4.12s/it] 95%|█████████▍| 142/150 [09:46<00:32,  4.11s/it] 95%|█████████▌| 143/150 [09:50<00:28,  4.11s/it] 96%|█████████▌| 144/150 [09:54<00:24,  4.11s/it] 97%|█████████▋| 145/150 [09:58<00:20,  4.11s/it] 97%|█████████▋| 146/150 [10:02<00:16,  4.12s/it] 98%|█████████▊| 147/150 [10:06<00:12,  4.11s/it] 99%|█████████▊| 148/150 [10:10<00:08,  4.12s/it] 99%|█████████▉| 149/150 [10:14<00:04,  4.12s/it]100%|██████████| 150/150 [10:18<00:00,  4.12s/it]                                                 {'loss': 0.1757, 'grad_norm': 0.47463667392730713, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [10:18<00:00,  4.12s/it][INFO|trainer.py:3705] 2024-11-18 22:20:02,787 >> Saving model checkpoint to saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:20:02,829 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:20:02,830 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:20:02,988 >> tokenizer config file saved in saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:20:02,988 >> Special tokens file saved in saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:20:03,669 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 620.1071, 'train_samples_per_second': 3.899, 'train_steps_per_second': 0.242, 'train_loss': 0.3927074464162191, 'epoch': 2.98}
100%|██████████| 150/150 [10:19<00:00,  4.12s/it]100%|██████████| 150/150 [10:19<00:00,  4.13s/it]
[INFO|trainer.py:3705] 2024-11-18 22:20:03,672 >> Saving model checkpoint to saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:20:03,709 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-3B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:20:03,710 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:20:03,859 >> tokenizer config file saved in saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:20:03,860 >> Special tokens file saved in saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 37267049GF
  train_loss               =     0.3927
  train_runtime            = 0:10:20.10
  train_samples_per_second =      3.899
  train_steps_per_second   =      0.242
Figure saved at: saves/Qwen2.5-3B-Instruct/my_prompt_modelsize3B/lora/sft/training_loss.png
11/18/2024 22:20:04 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:20:04 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:20:04,325 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:20:04,326 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:20:04,326 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 16.34it/s]  9%|▉         | 4/45 [00:00<00:03, 10.31it/s] 13%|█▎        | 6/45 [00:00<00:04,  9.24it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.88it/s] 20%|██        | 9/45 [00:00<00:04,  8.77it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.68it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.61it/s] 27%|██▋       | 12/45 [00:01<00:03,  8.53it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.50it/s] 31%|███       | 14/45 [00:01<00:03,  8.43it/s] 33%|███▎      | 15/45 [00:01<00:03,  8.34it/s] 36%|███▌      | 16/45 [00:01<00:03,  8.34it/s] 38%|███▊      | 17/45 [00:01<00:03,  8.33it/s] 40%|████      | 18/45 [00:02<00:03,  8.31it/s] 42%|████▏     | 19/45 [00:02<00:03,  8.32it/s] 44%|████▍     | 20/45 [00:02<00:03,  8.30it/s] 47%|████▋     | 21/45 [00:02<00:02,  8.33it/s] 49%|████▉     | 22/45 [00:02<00:02,  8.33it/s] 51%|█████     | 23/45 [00:02<00:02,  8.32it/s] 53%|█████▎    | 24/45 [00:02<00:02,  8.30it/s] 56%|█████▌    | 25/45 [00:02<00:02,  8.30it/s] 58%|█████▊    | 26/45 [00:03<00:02,  8.30it/s] 60%|██████    | 27/45 [00:03<00:02,  8.29it/s] 62%|██████▏   | 28/45 [00:03<00:02,  8.28it/s] 64%|██████▍   | 29/45 [00:03<00:01,  8.28it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.27it/s] 69%|██████▉   | 31/45 [00:03<00:01,  8.28it/s] 71%|███████   | 32/45 [00:03<00:01,  8.28it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.27it/s] 76%|███████▌  | 34/45 [00:03<00:01,  8.28it/s] 78%|███████▊  | 35/45 [00:04<00:01,  8.28it/s] 80%|████████  | 36/45 [00:04<00:01,  8.28it/s] 82%|████████▏ | 37/45 [00:04<00:00,  8.28it/s] 84%|████████▍ | 38/45 [00:04<00:00,  8.28it/s] 87%|████████▋ | 39/45 [00:04<00:00,  8.28it/s] 89%|████████▉ | 40/45 [00:04<00:00,  8.28it/s] 91%|█████████ | 41/45 [00:04<00:00,  8.28it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.26it/s] 96%|█████████▌| 43/45 [00:05<00:00,  8.22it/s] 98%|█████████▊| 44/45 [00:05<00:00,  8.24it/s]100%|██████████| 45/45 [00:05<00:00,  8.27it/s]100%|██████████| 45/45 [00:05<00:00,  8.47it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1997
  eval_runtime            = 0:00:05.45
  eval_samples_per_second =     16.502
  eval_steps_per_second   =      8.251
[INFO|modelcard.py:449] 2024-11-18 22:20:09,780 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:20:32,771] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:20:36 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:21131
[2024-11-18 22:20:38,596] torch.distributed.run: [WARNING] 
[2024-11-18 22:20:38,596] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:20:38,596] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:20:38,596] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:20:45,425] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:20:45,790] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:20:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:20:46 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:20:46 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:20:46 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:20:46,922 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:20:46,924 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:46,926 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:20:47,385 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:20:47,386 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:20:47,387 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:20:47,388 >> loading file tokenizer_config.json
11/18/2024 22:20:47 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:20:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:20:47,857 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:20:47 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:20:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:20:47 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 398.04 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3034.35 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:20:52 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:23, 36.21 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:06, 118.91 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:04, 150.67 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:03, 175.43 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 197.56 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 213.23 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 224.28 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:03<00:01, 222.00 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:03<00:01, 224.08 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 354.35 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:04<00:00, 251.00 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 305.41 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 206.21 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:20:57,213 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:20:57,215 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:20:57,265 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:20:57,266 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:20:57,267 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.07it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:21:01,079 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:21:01,079 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:21:01,083 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:21:01,084 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:21:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:21:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,q_proj,gate_proj,up_proj,o_proj,k_proj,v_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:21:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:21:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:21:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,o_proj,gate_proj,k_proj,v_proj,q_proj
11/18/2024 22:21:01 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:21:01,964 >> Using auto half precision backend
11/18/2024 22:21:02 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:21:03,037 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:21:03,037 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:21:03,037 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:21:03,037 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:21:03,037 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:21:03,037 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:21:03,037 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:21:03,043 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:24,  4.19s/it]  1%|▏         | 2/150 [00:07<09:39,  3.91s/it]  2%|▏         | 3/150 [00:11<09:21,  3.82s/it]  3%|▎         | 4/150 [00:15<09:11,  3.78s/it]  3%|▎         | 5/150 [00:19<09:04,  3.75s/it]  4%|▍         | 6/150 [00:22<08:59,  3.74s/it]  5%|▍         | 7/150 [00:26<08:53,  3.73s/it]  5%|▌         | 8/150 [00:30<08:48,  3.72s/it]  6%|▌         | 9/150 [00:33<08:44,  3.72s/it]  7%|▋         | 10/150 [00:37<08:40,  3.72s/it]                                                {'loss': 1.7336, 'grad_norm': 2.0301530361175537, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:37<08:40,  3.72s/it]  7%|▋         | 11/150 [00:41<08:38,  3.73s/it]  8%|▊         | 12/150 [00:45<08:33,  3.72s/it]  9%|▊         | 13/150 [00:48<08:28,  3.71s/it]  9%|▉         | 14/150 [00:52<08:23,  3.70s/it] 10%|█         | 15/150 [00:56<08:20,  3.71s/it] 11%|█         | 16/150 [00:59<08:15,  3.70s/it] 11%|█▏        | 17/150 [01:03<08:12,  3.70s/it] 12%|█▏        | 18/150 [01:07<08:08,  3.70s/it] 13%|█▎        | 19/150 [01:10<08:05,  3.71s/it] 13%|█▎        | 20/150 [01:14<08:01,  3.70s/it]                                                {'loss': 0.8769, 'grad_norm': 0.9407723546028137, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:14<08:01,  3.70s/it] 14%|█▍        | 21/150 [01:18<07:57,  3.70s/it] 15%|█▍        | 22/150 [01:22<07:52,  3.69s/it] 15%|█▌        | 23/150 [01:25<07:49,  3.69s/it] 16%|█▌        | 24/150 [01:29<07:45,  3.69s/it] 17%|█▋        | 25/150 [01:33<07:42,  3.70s/it] 17%|█▋        | 26/150 [01:36<07:38,  3.70s/it] 18%|█▊        | 27/150 [01:40<07:36,  3.71s/it] 19%|█▊        | 28/150 [01:44<07:31,  3.70s/it] 19%|█▉        | 29/150 [01:47<07:28,  3.70s/it] 20%|██        | 30/150 [01:51<07:23,  3.70s/it]                                                {'loss': 0.4196, 'grad_norm': 0.6428384184837341, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:51<07:23,  3.70s/it] 21%|██        | 31/150 [01:55<07:20,  3.70s/it] 21%|██▏       | 32/150 [01:59<07:17,  3.71s/it] 22%|██▏       | 33/150 [02:02<07:14,  3.71s/it] 23%|██▎       | 34/150 [02:06<07:10,  3.71s/it] 23%|██▎       | 35/150 [02:10<07:07,  3.71s/it] 24%|██▍       | 36/150 [02:13<07:02,  3.71s/it] 25%|██▍       | 37/150 [02:17<06:58,  3.71s/it] 25%|██▌       | 38/150 [02:21<06:55,  3.71s/it] 26%|██▌       | 39/150 [02:25<06:51,  3.71s/it] 27%|██▋       | 40/150 [02:28<06:48,  3.71s/it]                                                {'loss': 0.3302, 'grad_norm': 0.6049093008041382, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:28<06:48,  3.71s/it] 27%|██▋       | 41/150 [02:32<06:43,  3.70s/it] 28%|██▊       | 42/150 [02:36<06:39,  3.69s/it] 29%|██▊       | 43/150 [02:39<06:35,  3.70s/it] 29%|██▉       | 44/150 [02:43<06:31,  3.70s/it] 30%|███       | 45/150 [02:47<06:29,  3.71s/it] 31%|███       | 46/150 [02:50<06:24,  3.70s/it] 31%|███▏      | 47/150 [02:54<06:22,  3.71s/it] 32%|███▏      | 48/150 [02:58<06:17,  3.71s/it] 33%|███▎      | 49/150 [03:02<06:14,  3.71s/it] 33%|███▎      | 50/150 [03:05<06:10,  3.71s/it]                                                {'loss': 0.262, 'grad_norm': 0.63908851146698, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:05<06:10,  3.71s/it] 34%|███▍      | 51/150 [03:09<06:06,  3.70s/it] 35%|███▍      | 52/150 [03:13<06:02,  3.70s/it] 35%|███▌      | 53/150 [03:16<05:59,  3.70s/it] 36%|███▌      | 54/150 [03:20<05:54,  3.70s/it] 37%|███▋      | 55/150 [03:24<05:52,  3.71s/it] 37%|███▋      | 56/150 [03:27<05:48,  3.71s/it] 38%|███▊      | 57/150 [03:31<05:45,  3.71s/it] 39%|███▊      | 58/150 [03:35<05:41,  3.71s/it] 39%|███▉      | 59/150 [03:39<05:36,  3.70s/it] 40%|████      | 60/150 [03:42<05:33,  3.71s/it]                                                {'loss': 0.226, 'grad_norm': 0.45999783277511597, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:42<05:33,  3.71s/it] 41%|████      | 61/150 [03:46<05:29,  3.70s/it] 41%|████▏     | 62/150 [03:50<05:25,  3.69s/it] 42%|████▏     | 63/150 [03:53<05:22,  3.70s/it] 43%|████▎     | 64/150 [03:57<05:18,  3.70s/it] 43%|████▎     | 65/150 [04:01<05:14,  3.71s/it] 44%|████▍     | 66/150 [04:05<05:11,  3.71s/it] 45%|████▍     | 67/150 [04:08<05:07,  3.71s/it] 45%|████▌     | 68/150 [04:12<05:03,  3.70s/it] 46%|████▌     | 69/150 [04:16<04:59,  3.70s/it] 47%|████▋     | 70/150 [04:19<04:56,  3.70s/it]                                                {'loss': 0.2079, 'grad_norm': 0.7700252532958984, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:19<04:56,  3.70s/it] 47%|████▋     | 71/150 [04:23<04:52,  3.71s/it] 48%|████▊     | 72/150 [04:27<04:49,  3.71s/it] 49%|████▊     | 73/150 [04:30<04:45,  3.71s/it] 49%|████▉     | 74/150 [04:34<04:42,  3.72s/it] 50%|█████     | 75/150 [04:38<04:38,  3.71s/it] 51%|█████     | 76/150 [04:42<04:35,  3.72s/it] 51%|█████▏    | 77/150 [04:45<04:31,  3.72s/it] 52%|█████▏    | 78/150 [04:49<04:27,  3.72s/it] 53%|█████▎    | 79/150 [04:53<04:24,  3.72s/it] 53%|█████▎    | 80/150 [04:56<04:19,  3.71s/it]                                                {'loss': 0.2119, 'grad_norm': 0.6205257773399353, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:56<04:19,  3.71s/it] 54%|█████▍    | 81/150 [05:00<04:15,  3.70s/it] 55%|█████▍    | 82/150 [05:04<04:11,  3.71s/it] 55%|█████▌    | 83/150 [05:08<04:08,  3.71s/it] 56%|█████▌    | 84/150 [05:11<04:04,  3.71s/it] 57%|█████▋    | 85/150 [05:15<04:00,  3.70s/it] 57%|█████▋    | 86/150 [05:19<03:56,  3.70s/it] 58%|█████▊    | 87/150 [05:22<03:53,  3.71s/it] 59%|█████▊    | 88/150 [05:26<03:50,  3.71s/it] 59%|█████▉    | 89/150 [05:30<03:46,  3.71s/it] 60%|██████    | 90/150 [05:34<03:41,  3.70s/it]                                                {'loss': 0.2198, 'grad_norm': 0.5202314853668213, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:34<03:41,  3.70s/it] 61%|██████    | 91/150 [05:37<03:38,  3.70s/it] 61%|██████▏   | 92/150 [05:41<03:34,  3.70s/it] 62%|██████▏   | 93/150 [05:45<03:30,  3.70s/it] 63%|██████▎   | 94/150 [05:48<03:27,  3.71s/it] 63%|██████▎   | 95/150 [05:52<03:23,  3.70s/it] 64%|██████▍   | 96/150 [05:56<03:20,  3.71s/it] 65%|██████▍   | 97/150 [05:59<03:16,  3.70s/it] 65%|██████▌   | 98/150 [06:03<03:12,  3.71s/it] 66%|██████▌   | 99/150 [06:07<03:08,  3.70s/it] 67%|██████▋   | 100/150 [06:11<03:04,  3.70s/it]                                                 {'loss': 0.2253, 'grad_norm': 0.6259320378303528, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:11<03:04,  3.70s/it] 67%|██████▋   | 101/150 [06:14<03:00,  3.69s/it] 68%|██████▊   | 102/150 [06:18<02:57,  3.69s/it] 69%|██████▊   | 103/150 [06:22<02:53,  3.68s/it] 69%|██████▉   | 104/150 [06:25<02:49,  3.69s/it] 70%|███████   | 105/150 [06:29<02:45,  3.69s/it] 71%|███████   | 106/150 [06:33<02:42,  3.69s/it] 71%|███████▏  | 107/150 [06:36<02:39,  3.70s/it] 72%|███████▏  | 108/150 [06:40<02:35,  3.71s/it] 73%|███████▎  | 109/150 [06:44<02:32,  3.71s/it] 73%|███████▎  | 110/150 [06:48<02:28,  3.71s/it]                                                 {'loss': 0.1924, 'grad_norm': 0.8357245922088623, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:48<02:28,  3.71s/it] 74%|███████▍  | 111/150 [06:51<02:24,  3.71s/it] 75%|███████▍  | 112/150 [06:55<02:20,  3.71s/it] 75%|███████▌  | 113/150 [06:59<02:17,  3.70s/it] 76%|███████▌  | 114/150 [07:02<02:13,  3.70s/it] 77%|███████▋  | 115/150 [07:06<02:09,  3.70s/it] 77%|███████▋  | 116/150 [07:10<02:06,  3.71s/it] 78%|███████▊  | 117/150 [07:13<02:02,  3.71s/it] 79%|███████▊  | 118/150 [07:17<01:59,  3.72s/it] 79%|███████▉  | 119/150 [07:21<01:55,  3.71s/it] 80%|████████  | 120/150 [07:25<01:51,  3.71s/it]                                                 {'loss': 0.2236, 'grad_norm': 0.5709720849990845, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:25<01:51,  3.71s/it] 81%|████████  | 121/150 [07:28<01:47,  3.72s/it] 81%|████████▏ | 122/150 [07:32<01:44,  3.72s/it] 82%|████████▏ | 123/150 [07:36<01:40,  3.72s/it] 83%|████████▎ | 124/150 [07:40<01:36,  3.72s/it] 83%|████████▎ | 125/150 [07:43<01:33,  3.73s/it] 84%|████████▍ | 126/150 [07:47<01:29,  3.71s/it] 85%|████████▍ | 127/150 [07:51<01:25,  3.71s/it] 85%|████████▌ | 128/150 [07:54<01:21,  3.70s/it] 86%|████████▌ | 129/150 [07:58<01:17,  3.70s/it] 87%|████████▋ | 130/150 [08:02<01:13,  3.69s/it]                                                 {'loss': 0.1631, 'grad_norm': 0.32608500123023987, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:02<01:13,  3.69s/it] 87%|████████▋ | 131/150 [08:05<01:10,  3.70s/it] 88%|████████▊ | 132/150 [08:09<01:06,  3.69s/it] 89%|████████▊ | 133/150 [08:13<01:02,  3.70s/it] 89%|████████▉ | 134/150 [08:16<00:59,  3.70s/it] 90%|█████████ | 135/150 [08:20<00:55,  3.71s/it] 91%|█████████ | 136/150 [08:24<00:51,  3.71s/it] 91%|█████████▏| 137/150 [08:28<00:48,  3.71s/it] 92%|█████████▏| 138/150 [08:31<00:44,  3.72s/it] 93%|█████████▎| 139/150 [08:35<00:40,  3.71s/it] 93%|█████████▎| 140/150 [08:39<00:37,  3.71s/it]                                                 {'loss': 0.199, 'grad_norm': 0.609962522983551, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:39<00:37,  3.71s/it] 94%|█████████▍| 141/150 [08:42<00:33,  3.71s/it] 95%|█████████▍| 142/150 [08:46<00:29,  3.71s/it] 95%|█████████▌| 143/150 [08:50<00:25,  3.71s/it] 96%|█████████▌| 144/150 [08:54<00:22,  3.71s/it] 97%|█████████▋| 145/150 [08:57<00:18,  3.71s/it] 97%|█████████▋| 146/150 [09:01<00:14,  3.71s/it] 98%|█████████▊| 147/150 [09:05<00:11,  3.71s/it] 99%|█████████▊| 148/150 [09:08<00:07,  3.71s/it] 99%|█████████▉| 149/150 [09:12<00:03,  3.71s/it]100%|██████████| 150/150 [09:16<00:00,  3.70s/it]                                                 {'loss': 0.1629, 'grad_norm': 0.3822917938232422, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [09:16<00:00,  3.70s/it][INFO|trainer.py:3705] 2024-11-18 22:30:20,122 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:30:20,162 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:30:20,164 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:30:20,330 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:30:20,330 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:30:21,074 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 558.0306, 'train_samples_per_second': 4.333, 'train_steps_per_second': 0.269, 'train_loss': 0.37694912354151405, 'epoch': 2.98}
100%|██████████| 150/150 [09:17<00:00,  3.70s/it]100%|██████████| 150/150 [09:17<00:00,  3.72s/it]
[INFO|trainer.py:3705] 2024-11-18 22:30:21,077 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:30:21,110 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:30:21,111 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:30:21,265 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:30:21,266 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 94723273GF
  train_loss               =     0.3769
  train_runtime            = 0:09:18.03
  train_samples_per_second =      4.333
  train_steps_per_second   =      0.269
Figure saved at: saves/Qwen2.5-7B-Instruct/my_prompt_modelsize7B/lora/sft/training_loss.png
11/18/2024 22:30:21 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:30:21 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:30:21,728 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:30:21,729 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:30:21,729 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 15.90it/s]  9%|▉         | 4/45 [00:00<00:04, 10.07it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.94it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.75it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.50it/s] 20%|██        | 9/45 [00:01<00:04,  8.38it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.34it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.20it/s] 27%|██▋       | 12/45 [00:01<00:04,  8.19it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.10it/s] 31%|███       | 14/45 [00:01<00:03,  8.09it/s] 33%|███▎      | 15/45 [00:01<00:03,  7.97it/s] 36%|███▌      | 16/45 [00:01<00:03,  7.94it/s] 38%|███▊      | 17/45 [00:02<00:03,  8.03it/s] 40%|████      | 18/45 [00:02<00:03,  8.08it/s] 42%|████▏     | 19/45 [00:02<00:03,  8.09it/s] 44%|████▍     | 20/45 [00:02<00:03,  8.00it/s] 47%|████▋     | 21/45 [00:02<00:03,  8.00it/s] 49%|████▉     | 22/45 [00:02<00:02,  7.99it/s] 51%|█████     | 23/45 [00:02<00:02,  8.04it/s] 53%|█████▎    | 24/45 [00:02<00:02,  7.99it/s] 56%|█████▌    | 25/45 [00:02<00:02,  8.06it/s] 58%|█████▊    | 26/45 [00:03<00:02,  8.09it/s] 60%|██████    | 27/45 [00:03<00:02,  8.12it/s] 62%|██████▏   | 28/45 [00:03<00:02,  8.04it/s] 64%|██████▍   | 29/45 [00:03<00:01,  8.09it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.12it/s] 69%|██████▉   | 31/45 [00:03<00:01,  7.99it/s] 71%|███████   | 32/45 [00:03<00:01,  8.03it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.03it/s] 76%|███████▌  | 34/45 [00:04<00:01,  7.97it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.96it/s] 80%|████████  | 36/45 [00:04<00:01,  7.92it/s] 82%|████████▏ | 37/45 [00:04<00:01,  7.98it/s] 84%|████████▍ | 38/45 [00:04<00:00,  8.04it/s] 87%|████████▋ | 39/45 [00:04<00:00,  8.07it/s] 89%|████████▉ | 40/45 [00:04<00:00,  8.03it/s] 91%|█████████ | 41/45 [00:04<00:00,  7.97it/s] 93%|█████████▎| 42/45 [00:05<00:00,  8.05it/s] 96%|█████████▌| 43/45 [00:05<00:00,  8.03it/s] 98%|█████████▊| 44/45 [00:05<00:00,  8.05it/s]100%|██████████| 45/45 [00:05<00:00,  8.14it/s]100%|██████████| 45/45 [00:05<00:00,  8.21it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =      0.186
  eval_runtime            = 0:00:05.63
  eval_samples_per_second =     15.984
  eval_steps_per_second   =      7.992
[INFO|modelcard.py:449] 2024-11-18 22:30:27,360 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:30:46,123] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:30:50 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23812
[2024-11-18 22:30:52,133] torch.distributed.run: [WARNING] 
[2024-11-18 22:30:52,133] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:30:52,133] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:30:52,133] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:30:59,720] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:30:59,917] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:31:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:31:01 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:31:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:31:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:31:01,277 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:31:01,279 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,280 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:31:01,772 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:31:01,772 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:31:01,774 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,774 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,775 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,775 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,775 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,775 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:31:01,775 >> loading file tokenizer_config.json
11/18/2024 22:31:02 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:31:02 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:31:02,272 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:31:02 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:31:02 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:31:02 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 359.48 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2745.07 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:31:06 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:17, 47.60 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 96.37 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 122.15 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:04, 164.13 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:01<00:03, 203.68 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 237.68 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:01, 265.79 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 289.74 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 306.04 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:02<00:00, 373.18 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 361.52 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 363.57 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 380.39 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 370.55 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 249.14 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:31:10,870 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:31:10,872 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:31:10,926 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:31:10,926 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:31:10,928 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:08,  1.19s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:07,  1.13s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:06,  1.13s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:06,  1.15s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:05,  1.12s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:05,  1.14s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:04<00:04,  1.12s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:04<00:04,  1.13s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:05<00:03,  1.12s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:05<00:03,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.13s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.13s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:31:19,684 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:31:19,684 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:31:19,692 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:31:19,692 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:31:19 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:31:19 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,v_proj,q_proj,o_proj,gate_proj,up_proj,down_proj
Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.06s/it]
11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:31:19 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:31:19 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:31:19 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,o_proj,v_proj,up_proj,q_proj,k_proj,gate_proj
11/18/2024 22:31:20 - INFO - llamafactory.model.loader - trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:31:20,942 >> Using auto half precision backend
11/18/2024 22:31:21 - INFO - llamafactory.model.loader - trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:2243] 2024-11-18 22:31:22,048 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:31:22,048 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:31:22,048 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:31:22,048 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:31:22,048 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:31:22,048 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:31:22,048 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:31:22,059 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:07<18:47,  7.57s/it]  1%|▏         | 2/150 [00:14<18:05,  7.33s/it]  2%|▏         | 3/150 [00:21<17:46,  7.26s/it]  3%|▎         | 4/150 [00:29<17:33,  7.22s/it]  3%|▎         | 5/150 [00:36<17:21,  7.19s/it]  4%|▍         | 6/150 [00:43<17:15,  7.19s/it]  5%|▍         | 7/150 [00:50<17:05,  7.17s/it]  5%|▌         | 8/150 [00:57<16:54,  7.15s/it]  6%|▌         | 9/150 [01:04<16:46,  7.14s/it]  7%|▋         | 10/150 [01:11<16:38,  7.13s/it]                                                {'loss': 2.0036, 'grad_norm': 3.3586301803588867, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [01:11<16:38,  7.13s/it]  7%|▋         | 11/150 [01:19<16:32,  7.14s/it]  8%|▊         | 12/150 [01:26<16:26,  7.15s/it]  9%|▊         | 13/150 [01:33<16:20,  7.16s/it]  9%|▉         | 14/150 [01:40<16:10,  7.13s/it] 10%|█         | 15/150 [01:47<16:04,  7.15s/it] 11%|█         | 16/150 [01:54<15:54,  7.13s/it] 11%|█▏        | 17/150 [02:01<15:49,  7.14s/it] 12%|█▏        | 18/150 [02:08<15:41,  7.13s/it] 13%|█▎        | 19/150 [02:16<15:36,  7.15s/it] 13%|█▎        | 20/150 [02:23<15:27,  7.14s/it]                                                {'loss': 0.8524, 'grad_norm': 0.6918490529060364, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [02:23<15:27,  7.14s/it] 14%|█▍        | 21/150 [02:30<15:21,  7.14s/it] 15%|█▍        | 22/150 [02:37<15:09,  7.11s/it] 15%|█▌        | 23/150 [02:44<15:04,  7.12s/it] 16%|█▌        | 24/150 [02:51<14:54,  7.10s/it] 17%|█▋        | 25/150 [02:58<14:47,  7.10s/it] 17%|█▋        | 26/150 [03:05<14:42,  7.12s/it] 18%|█▊        | 27/150 [03:13<14:35,  7.12s/it] 19%|█▊        | 28/150 [03:20<14:25,  7.09s/it] 19%|█▉        | 29/150 [03:27<14:16,  7.08s/it] 20%|██        | 30/150 [03:34<14:10,  7.09s/it]                                                {'loss': 0.3831, 'grad_norm': 0.473868191242218, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [03:34<14:10,  7.09s/it] 21%|██        | 31/150 [03:41<14:03,  7.09s/it] 21%|██▏       | 32/150 [03:48<13:57,  7.09s/it] 22%|██▏       | 33/150 [03:55<13:51,  7.11s/it] 23%|██▎       | 34/150 [04:02<13:45,  7.11s/it] 23%|██▎       | 35/150 [04:09<13:40,  7.14s/it] 24%|██▍       | 36/150 [04:16<13:32,  7.13s/it] 25%|██▍       | 37/150 [04:24<13:24,  7.12s/it] 25%|██▌       | 38/150 [04:31<13:18,  7.13s/it] 26%|██▌       | 39/150 [04:38<13:12,  7.14s/it] 27%|██▋       | 40/150 [04:45<13:05,  7.14s/it]                                                {'loss': 0.3044, 'grad_norm': 0.43929633498191833, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [04:45<13:05,  7.14s/it] 27%|██▋       | 41/150 [04:52<12:55,  7.12s/it] 28%|██▊       | 42/150 [04:59<12:46,  7.10s/it] 29%|██▊       | 43/150 [05:06<12:41,  7.11s/it] 29%|██▉       | 44/150 [05:13<12:34,  7.11s/it] 30%|███       | 45/150 [05:21<12:27,  7.12s/it] 31%|███       | 46/150 [05:28<12:19,  7.11s/it] 31%|███▏      | 47/150 [05:35<12:15,  7.14s/it] 32%|███▏      | 48/150 [05:42<12:07,  7.13s/it] 33%|███▎      | 49/150 [05:49<12:00,  7.13s/it] 33%|███▎      | 50/150 [05:56<11:53,  7.13s/it]                                                {'loss': 0.2423, 'grad_norm': 0.4388812482357025, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [05:56<11:53,  7.13s/it] 34%|███▍      | 51/150 [06:03<11:45,  7.13s/it] 35%|███▍      | 52/150 [06:10<11:38,  7.12s/it] 35%|███▌      | 53/150 [06:18<11:30,  7.12s/it] 36%|███▌      | 54/150 [06:25<11:24,  7.13s/it] 37%|███▋      | 55/150 [06:32<11:18,  7.14s/it] 37%|███▋      | 56/150 [06:39<11:11,  7.14s/it] 38%|███▊      | 57/150 [06:46<11:04,  7.15s/it] 39%|███▊      | 58/150 [06:53<10:55,  7.12s/it] 39%|███▉      | 59/150 [07:00<10:49,  7.13s/it] 40%|████      | 60/150 [07:08<10:44,  7.16s/it]                                                {'loss': 0.2167, 'grad_norm': 0.38275137543678284, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [07:08<10:44,  7.16s/it] 41%|████      | 61/150 [07:15<10:36,  7.15s/it] 41%|████▏     | 62/150 [07:22<10:27,  7.13s/it] 42%|████▏     | 63/150 [07:29<10:23,  7.17s/it] 43%|████▎     | 64/150 [07:36<10:15,  7.15s/it] 43%|████▎     | 65/150 [07:43<10:08,  7.15s/it] 44%|████▍     | 66/150 [07:50<09:59,  7.14s/it] 45%|████▍     | 67/150 [07:58<09:53,  7.15s/it] 45%|████▌     | 68/150 [08:05<09:45,  7.14s/it] 46%|████▌     | 69/150 [08:12<09:39,  7.15s/it] 47%|████▋     | 70/150 [08:19<09:32,  7.16s/it]                                                {'loss': 0.1919, 'grad_norm': 0.43009138107299805, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [08:19<09:32,  7.16s/it] 47%|████▋     | 71/150 [08:26<09:25,  7.15s/it] 48%|████▊     | 72/150 [08:33<09:17,  7.14s/it] 49%|████▊     | 73/150 [08:41<09:09,  7.14s/it] 49%|████▉     | 74/150 [08:48<09:03,  7.15s/it] 50%|█████     | 75/150 [08:55<08:55,  7.14s/it] 51%|█████     | 76/150 [09:02<08:46,  7.11s/it] 51%|█████▏    | 77/150 [09:09<08:40,  7.13s/it] 52%|█████▏    | 78/150 [09:16<08:34,  7.14s/it] 53%|█████▎    | 79/150 [09:23<08:27,  7.15s/it] 53%|█████▎    | 80/150 [09:31<08:20,  7.16s/it]                                                {'loss': 0.201, 'grad_norm': 0.31474629044532776, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [09:31<08:20,  7.16s/it] 54%|█████▍    | 81/150 [09:38<08:11,  7.12s/it] 55%|█████▍    | 82/150 [09:45<08:04,  7.13s/it] 55%|█████▌    | 83/150 [09:52<07:57,  7.13s/it] 56%|█████▌    | 84/150 [09:59<07:50,  7.13s/it] 57%|█████▋    | 85/150 [10:06<07:43,  7.13s/it] 57%|█████▋    | 86/150 [10:13<07:36,  7.13s/it] 58%|█████▊    | 87/150 [10:20<07:29,  7.13s/it] 59%|█████▊    | 88/150 [10:28<07:22,  7.14s/it] 59%|█████▉    | 89/150 [10:35<07:14,  7.12s/it] 60%|██████    | 90/150 [10:42<07:07,  7.12s/it]                                                {'loss': 0.205, 'grad_norm': 0.3978399634361267, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [10:42<07:07,  7.12s/it] 61%|██████    | 91/150 [10:49<07:00,  7.13s/it] 61%|██████▏   | 92/150 [10:56<06:51,  7.10s/it] 62%|██████▏   | 93/150 [11:03<06:45,  7.11s/it] 63%|██████▎   | 94/150 [11:10<06:38,  7.12s/it] 63%|██████▎   | 95/150 [11:17<06:31,  7.12s/it] 64%|██████▍   | 96/150 [11:25<06:26,  7.16s/it] 65%|██████▍   | 97/150 [11:32<06:19,  7.16s/it] 65%|██████▌   | 98/150 [11:39<06:13,  7.18s/it] 66%|██████▌   | 99/150 [11:46<06:05,  7.17s/it] 67%|██████▋   | 100/150 [11:53<05:57,  7.16s/it]                                                 {'loss': 0.2139, 'grad_norm': 0.3208806812763214, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [11:53<05:57,  7.16s/it] 67%|██████▋   | 101/150 [12:00<05:51,  7.16s/it] 68%|██████▊   | 102/150 [12:08<05:44,  7.17s/it] 69%|██████▊   | 103/150 [12:15<05:35,  7.13s/it] 69%|██████▉   | 104/150 [12:22<05:27,  7.12s/it] 70%|███████   | 105/150 [12:29<05:20,  7.12s/it] 71%|███████   | 106/150 [12:36<05:13,  7.12s/it] 71%|███████▏  | 107/150 [12:43<05:06,  7.14s/it] 72%|███████▏  | 108/150 [12:50<04:59,  7.14s/it] 73%|███████▎  | 109/150 [12:58<04:53,  7.16s/it] 73%|███████▎  | 110/150 [13:05<04:45,  7.14s/it]                                                 {'loss': 0.1812, 'grad_norm': 0.6577368378639221, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [13:05<04:45,  7.14s/it] 74%|███████▍  | 111/150 [13:12<04:38,  7.13s/it] 75%|███████▍  | 112/150 [13:19<04:30,  7.13s/it] 75%|███████▌  | 113/150 [13:26<04:23,  7.13s/it] 76%|███████▌  | 114/150 [13:33<04:17,  7.14s/it] 77%|███████▋  | 115/150 [13:40<04:09,  7.13s/it] 77%|███████▋  | 116/150 [13:47<04:02,  7.14s/it] 78%|███████▊  | 117/150 [13:55<03:55,  7.14s/it] 79%|███████▊  | 118/150 [14:02<03:48,  7.14s/it] 79%|███████▉  | 119/150 [14:09<03:40,  7.12s/it] 80%|████████  | 120/150 [14:16<03:33,  7.13s/it]                                                 {'loss': 0.2163, 'grad_norm': 0.4074215590953827, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [14:16<03:33,  7.13s/it] 81%|████████  | 121/150 [14:23<03:26,  7.13s/it] 81%|████████▏ | 122/150 [14:30<03:19,  7.14s/it] 82%|████████▏ | 123/150 [14:37<03:12,  7.15s/it] 83%|████████▎ | 124/150 [14:44<03:05,  7.13s/it] 83%|████████▎ | 125/150 [14:52<02:58,  7.13s/it] 84%|████████▍ | 126/150 [14:59<02:52,  7.17s/it] 85%|████████▍ | 127/150 [15:06<02:45,  7.20s/it] 85%|████████▌ | 128/150 [15:13<02:37,  7.15s/it] 86%|████████▌ | 129/150 [15:20<02:29,  7.14s/it] 87%|████████▋ | 130/150 [15:27<02:22,  7.11s/it]                                                 {'loss': 0.1575, 'grad_norm': 0.26060914993286133, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [15:27<02:22,  7.11s/it] 87%|████████▋ | 131/150 [15:34<02:15,  7.12s/it] 88%|████████▊ | 132/150 [15:42<02:08,  7.12s/it] 89%|████████▊ | 133/150 [15:49<02:00,  7.10s/it] 89%|████████▉ | 134/150 [15:56<01:53,  7.11s/it] 90%|█████████ | 135/150 [16:03<01:46,  7.11s/it] 91%|█████████ | 136/150 [16:10<01:39,  7.11s/it] 91%|█████████▏| 137/150 [16:17<01:32,  7.11s/it] 92%|█████████▏| 138/150 [16:24<01:25,  7.12s/it] 93%|█████████▎| 139/150 [16:31<01:18,  7.12s/it] 93%|█████████▎| 140/150 [16:39<01:11,  7.12s/it]                                                 {'loss': 0.1933, 'grad_norm': 0.5520354509353638, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [16:39<01:11,  7.12s/it] 94%|█████████▍| 141/150 [16:46<01:03,  7.10s/it] 95%|█████████▍| 142/150 [16:53<00:56,  7.10s/it] 95%|█████████▌| 143/150 [17:00<00:49,  7.11s/it] 96%|█████████▌| 144/150 [17:07<00:42,  7.13s/it] 97%|█████████▋| 145/150 [17:14<00:35,  7.14s/it] 97%|█████████▋| 146/150 [17:21<00:28,  7.14s/it] 98%|█████████▊| 147/150 [17:28<00:21,  7.12s/it] 99%|█████████▊| 148/150 [17:35<00:14,  7.11s/it] 99%|█████████▉| 149/150 [17:43<00:07,  7.11s/it]100%|██████████| 150/150 [17:50<00:00,  7.12s/it]                                                 {'loss': 0.1553, 'grad_norm': 0.2942931056022644, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [17:50<00:00,  7.12s/it][INFO|trainer.py:3705] 2024-11-18 22:49:13,714 >> Saving model checkpoint to saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:49:13,762 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:13,763 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:49:14,054 >> tokenizer config file saved in saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:49:14,054 >> Special tokens file saved in saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:49:15,090 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1073.0308, 'train_samples_per_second': 2.253, 'train_steps_per_second': 0.14, 'train_loss': 0.3811916661262512, 'epoch': 2.98}
100%|██████████| 150/150 [17:51<00:00,  7.12s/it]100%|██████████| 150/150 [17:51<00:00,  7.14s/it]
[INFO|trainer.py:3705] 2024-11-18 22:49:15,093 >> Saving model checkpoint to saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:49:15,135 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:15,136 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:49:15,408 >> tokenizer config file saved in saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:49:15,408 >> Special tokens file saved in saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9777
  total_flos               = 187366132GF
  train_loss               =      0.3812
  train_runtime            =  0:17:53.03
  train_samples_per_second =       2.253
  train_steps_per_second   =        0.14
Figure saved at: saves/Qwen2.5-14B-Instruct/my_prompt_modelsize14B/lora/sft/training_loss.png
11/18/2024 22:49:15 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:49:15 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:49:15,869 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:49:15,869 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:49:15,869 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:05,  8.21it/s]  7%|▋         | 3/45 [00:00<00:07,  5.92it/s]  9%|▉         | 4/45 [00:00<00:08,  5.07it/s] 11%|█         | 5/45 [00:00<00:08,  4.64it/s] 13%|█▎        | 6/45 [00:01<00:08,  4.49it/s] 16%|█▌        | 7/45 [00:01<00:08,  4.42it/s] 18%|█▊        | 8/45 [00:01<00:08,  4.31it/s] 20%|██        | 9/45 [00:01<00:08,  4.26it/s] 22%|██▏       | 10/45 [00:02<00:08,  4.27it/s] 24%|██▍       | 11/45 [00:02<00:08,  4.22it/s] 27%|██▋       | 12/45 [00:02<00:07,  4.21it/s] 29%|██▉       | 13/45 [00:02<00:07,  4.18it/s] 31%|███       | 14/45 [00:03<00:07,  4.22it/s] 33%|███▎      | 15/45 [00:03<00:07,  4.20it/s] 36%|███▌      | 16/45 [00:03<00:06,  4.16it/s] 38%|███▊      | 17/45 [00:03<00:06,  4.19it/s] 40%|████      | 18/45 [00:04<00:06,  4.22it/s] 42%|████▏     | 19/45 [00:04<00:06,  4.20it/s] 44%|████▍     | 20/45 [00:04<00:06,  4.16it/s] 47%|████▋     | 21/45 [00:04<00:05,  4.20it/s] 49%|████▉     | 22/45 [00:05<00:05,  4.22it/s] 51%|█████     | 23/45 [00:05<00:05,  4.23it/s] 53%|█████▎    | 24/45 [00:05<00:05,  4.19it/s] 56%|█████▌    | 25/45 [00:05<00:04,  4.22it/s] 58%|█████▊    | 26/45 [00:05<00:04,  4.23it/s] 60%|██████    | 27/45 [00:06<00:04,  4.24it/s] 62%|██████▏   | 28/45 [00:06<00:04,  4.18it/s] 64%|██████▍   | 29/45 [00:06<00:03,  4.21it/s] 67%|██████▋   | 30/45 [00:06<00:03,  4.23it/s] 69%|██████▉   | 31/45 [00:07<00:03,  4.27it/s] 71%|███████   | 32/45 [00:07<00:03,  4.24it/s] 73%|███████▎  | 33/45 [00:07<00:02,  4.25it/s] 76%|███████▌  | 34/45 [00:07<00:02,  4.29it/s] 78%|███████▊  | 35/45 [00:08<00:02,  4.25it/s] 80%|████████  | 36/45 [00:08<00:02,  4.19it/s] 82%|████████▏ | 37/45 [00:08<00:01,  4.22it/s] 84%|████████▍ | 38/45 [00:08<00:01,  4.23it/s] 87%|████████▋ | 39/45 [00:09<00:01,  4.25it/s] 89%|████████▉ | 40/45 [00:09<00:01,  4.22it/s] 91%|█████████ | 41/45 [00:09<00:00,  4.20it/s] 93%|█████████▎| 42/45 [00:09<00:00,  4.22it/s] 96%|█████████▌| 43/45 [00:09<00:00,  4.20it/s] 98%|█████████▊| 44/45 [00:10<00:00,  4.18it/s]100%|██████████| 45/45 [00:10<00:00,  4.19it/s]100%|██████████| 45/45 [00:10<00:00,  4.29it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1787
  eval_runtime            = 0:00:10.75
  eval_samples_per_second =      8.372
  eval_steps_per_second   =      4.186
[INFO|modelcard.py:449] 2024-11-18 22:49:26,620 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:49:54,542] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:49:58 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29097
[2024-11-18 22:50:00,870] torch.distributed.run: [WARNING] 
[2024-11-18 22:50:00,870] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:50:00,870] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:50:00,870] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:50:08,457] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:50:08,479] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:50:09,646] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-18 22:50:09,646] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl
11/18/2024 22:50:09 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:50:09 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:50:09,657 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:50:09,659 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,660 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,660 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,660 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,660 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,660 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:09,661 >> loading file tokenizer_config.json
[2024-11-18 22:50:09,671] [INFO] [comm.py:637:init_distributed] cdb=None
11/18/2024 22:50:09 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:50:09 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:50:10,163 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:50:10,163 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:50:10,165 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,165 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,165 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,165 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,166 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,166 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:50:10,166 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:50:10,662 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:50:10 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:50:10 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:50:10 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
11/18/2024 22:50:10 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:50:10 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:02, 407.25 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 2977.29 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:50:14 - INFO - llamafactory.data.loader - Loading dataset ldbc_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:20, 40.98 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:09, 82.98 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 123.80 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:04, 160.40 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:03, 191.77 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 214.10 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 231.37 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 261.99 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 560/896 [00:03<00:01, 272.49 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 672/896 [00:03<00:00, 328.59 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 285.18 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 304.52 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 327.94 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 339.77 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:04<00:00, 216.95 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 13326, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 304, 23959, 19262, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 19870, 2039, 7443, 288, 304, 22443, 77983, 89, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 383, 1775, 383, 12302, 2145, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 38, 94991, 1668, 261, 1389, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 74639, 22036, 1557, 89817, 25972, 7660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 9499, 1775, 2763, 321, 51899, 1400, 4487, 13851, 2039, 791, 301, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3608, 1139, 2032, 62, 85047, 20871, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 32664, 38952, 1400, 81, 1776, 292, 2039, 37121, 112429, 99653, 9370, 93568, 5373, 109391, 5373, 102064, 33108, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Georg_Wilhelm_Friedrich_Hegel in Tarakan'的节点
label为forum,属性title='Group for Howard_Hughes in Gardēz'的节点
label为organisation,属性name='Gheorghe_Zane_University'的节点
label为organisation,属性name='Gaston_Berger_University'的节点
label为organisation,属性name='Gabriel_Dumont_Institute'的节点
label为tag,属性name='George_Frideric_Handel'的节点
label为tag,属性name='Georg_Wilhelm_Friedrich_Hegel'的节点
label为tag,属性name='Source_Tags_&_Codes'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找对George_Frideric_Handel感兴趣的人员的邮箱、性别、语言和match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 4648, 12724, 6294, 7, 77, 17, 25, 4578, 8, 220, 1380, 308, 17, 2644, 1131, 38952, 1400, 81, 1776, 292, 2039, 37121, 6, 470, 308, 17, 7315, 11, 308, 16, 9847, 11, 308, 17, 2644, 11, 308, 16, 72637, 11, 308, 16, 48337, 11, 308, 16, 31633, 1973, 553, 308, 17, 2644, 26, 151645]
labels:
match (n1:person)-[r1:hasinterest]->(n2:tag)  where n2.name='George_Frideric_Handel' return n2.url, n1.email, n2.name, n1.lastname, n1.gender, n1.language order by n2.name;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:50:19,858 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:50:19,860 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:50:19,916 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:3874] 2024-11-18 22:50:19,917 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1099] 2024-11-18 22:50:19,931 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[2024-11-18 22:50:23,202] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:00<00:14,  1.08it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:02<00:39,  2.46s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:02<00:17,  1.15s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:03<00:25,  1.71s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:03<00:16,  1.17s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:04<00:20,  1.47s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:04<00:15,  1.17s/it]Loading checkpoint shards:  24%|██▎       | 4/17 [00:05<00:17,  1.35s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:05<00:14,  1.19s/it]Loading checkpoint shards:  29%|██▉       | 5/17 [00:07<00:15,  1.32s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:07<00:13,  1.20s/it]Loading checkpoint shards:  35%|███▌      | 6/17 [00:08<00:14,  1.28s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:08<00:12,  1.20s/it]Loading checkpoint shards:  41%|████      | 7/17 [00:09<00:12,  1.26s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:09<00:10,  1.20s/it]Loading checkpoint shards:  47%|████▋     | 8/17 [00:10<00:11,  1.25s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:11<00:10,  1.33s/it]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:12<00:10,  1.27s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:12<00:08,  1.23s/it]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:13<00:08,  1.25s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:13<00:07,  1.22s/it]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:14<00:07,  1.23s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:14<00:06,  1.22s/it]Loading checkpoint shards:  71%|███████   | 12/17 [00:15<00:06,  1.22s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:15<00:04,  1.21s/it]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:17<00:04,  1.21s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:16<00:03,  1.21s/it]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:18<00:03,  1.22s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:18<00:02,  1.23s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:19<00:02,  1.23s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:19<00:01,  1.21s/it]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:20<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:19<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:19<00:00,  1.17s/it]
11/18/2024 22:50:44 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:50:44 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:50:44 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
11/18/2024 22:50:44 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:50:44 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,k_proj,gate_proj,up_proj,v_proj,down_proj,q_proj
Loading checkpoint shards: 100%|██████████| 17/17 [00:21<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:21<00:00,  1.27s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:50:45,032 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:50:45,032 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:50:45,042 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:50:45,042 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:50:45 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:50:45 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:50:45 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
11/18/2024 22:50:45 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:50:45 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,o_proj,k_proj,v_proj,gate_proj,q_proj
11/18/2024 22:50:46 - INFO - llamafactory.model.loader - trainable params: 67,108,864 || all params: 32,830,985,216 || trainable%: 0.2044
11/18/2024 22:50:47 - INFO - llamafactory.model.loader - trainable params: 67,108,864 || all params: 32,830,985,216 || trainable%: 0.2044
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:50:47,409 >> Using auto half precision backend
[2024-11-18 22:50:48,080] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown
[2024-11-18 22:50:48,222] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-11-18 22:50:48,237] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-11-18 22:50:48,237] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-11-18 22:50:48,461] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-11-18 22:50:48,461] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-11-18 22:50:48,462] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-11-18 22:50:48,462] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-11-18 22:50:48,769] [INFO] [utils.py:772:see_memory_usage] Stage 3 initialize beginning
[2024-11-18 22:50:48,770] [INFO] [utils.py:773:see_memory_usage] MA 30.64 GB         Max_MA 34.14 GB         CA 51.19 GB         Max_CA 60 GB 
[2024-11-18 22:50:48,770] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 33.99 GB, percent = 2.2%
[2024-11-18 22:50:48,797] [INFO] [stage3.py:130:__init__] Reduce bucket size 26214400
[2024-11-18 22:50:48,797] [INFO] [stage3.py:131:__init__] Prefetch bucket size 23592960
[2024-11-18 22:50:49,099] [INFO] [utils.py:772:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-11-18 22:50:49,100] [INFO] [utils.py:773:see_memory_usage] MA 30.64 GB         Max_MA 30.64 GB         CA 51.19 GB         Max_CA 51 GB 
[2024-11-18 22:50:49,100] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 33.99 GB, percent = 2.2%
Parameter Offload: Total persistent parameters: 25760768 in 1025 params
[2024-11-18 22:50:50,680] [INFO] [utils.py:772:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-11-18 22:50:50,681] [INFO] [utils.py:773:see_memory_usage] MA 30.58 GB         Max_MA 30.64 GB         CA 51.19 GB         Max_CA 51 GB 
[2024-11-18 22:50:50,681] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:51,083] [INFO] [utils.py:772:see_memory_usage] Before creating fp16 partitions
[2024-11-18 22:50:51,084] [INFO] [utils.py:773:see_memory_usage] MA 30.58 GB         Max_MA 30.58 GB         CA 51.19 GB         Max_CA 51 GB 
[2024-11-18 22:50:51,084] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:51,997] [INFO] [utils.py:772:see_memory_usage] After creating fp16 partitions: 1
[2024-11-18 22:50:51,998] [INFO] [utils.py:773:see_memory_usage] MA 30.58 GB         Max_MA 30.58 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:51,999] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:52,409] [INFO] [utils.py:772:see_memory_usage] Before creating fp32 partitions
[2024-11-18 22:50:52,410] [INFO] [utils.py:773:see_memory_usage] MA 30.58 GB         Max_MA 30.58 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:52,410] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:52,812] [INFO] [utils.py:772:see_memory_usage] After creating fp32 partitions
[2024-11-18 22:50:52,813] [INFO] [utils.py:773:see_memory_usage] MA 30.7 GB         Max_MA 30.77 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:52,813] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:53,215] [INFO] [utils.py:772:see_memory_usage] Before initializing optimizer states
[2024-11-18 22:50:53,216] [INFO] [utils.py:773:see_memory_usage] MA 30.7 GB         Max_MA 30.7 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:53,216] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:53,618] [INFO] [utils.py:772:see_memory_usage] After initializing optimizer states
[2024-11-18 22:50:53,619] [INFO] [utils.py:773:see_memory_usage] MA 30.7 GB         Max_MA 30.83 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:53,619] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.01 GB, percent = 2.3%
[2024-11-18 22:50:53,620] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-11-18 22:50:54,615] [INFO] [utils.py:772:see_memory_usage] After initializing ZeRO optimizer
[2024-11-18 22:50:54,616] [INFO] [utils.py:773:see_memory_usage] MA 30.81 GB         Max_MA 30.82 GB         CA 50.94 GB         Max_CA 51 GB 
[2024-11-18 22:50:54,616] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 34.03 GB, percent = 2.3%
[2024-11-18 22:50:54,616] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-11-18 22:50:54,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-11-18 22:50:54,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-11-18 22:50:54,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-11-18 22:50:54,630] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-11-18 22:50:54,631] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-11-18 22:50:54,631] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-11-18 22:50:54,631] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-11-18 22:50:54,631] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-11-18 22:50:54,631] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xfffebdf52190>
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-11-18 22:50:54,632] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-11-18 22:50:54,633] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   train_batch_size ............. 16
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   world_size ................... 2
[2024-11-18 22:50:54,634] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-11-18 22:50:54,635] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=26214400 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=23592960 param_persistence_threshold=51200 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-11-18 22:50:54,635] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-11-18 22:50:54,635] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-11-18 22:50:54,635] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-11-18 22:50:54,635] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 2.621440e+07, 
        "stage3_prefetch_bucket_size": 2.359296e+07, 
        "stage3_param_persistence_threshold": 5.120000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2243] 2024-11-18 22:50:54,635 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:50:54,635 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:50:54,635 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:50:54,635 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:50:54,635 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:50:54,635 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:50:54,635 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:50:54,654 >>   Number of trainable parameters = 67,108,864
  0%|          | 0/150 [00:00<?, ?it/s]Warning: Device do not support double dtype now, dtype cast repalce with float.
Warning: Device do not support double dtype now, dtype cast repalce with float.
[2024-11-18 22:51:51,480] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  1%|          | 1/150 [00:56<2:21:11, 56.85s/it]  1%|▏         | 2/150 [01:41<2:02:38, 49.72s/it]  2%|▏         | 3/150 [02:26<1:56:10, 47.42s/it]  3%|▎         | 4/150 [03:10<1:52:38, 46.29s/it]  3%|▎         | 5/150 [03:55<1:50:44, 45.82s/it]  4%|▍         | 6/150 [04:40<1:48:59, 45.41s/it]  5%|▍         | 7/150 [05:25<1:48:04, 45.35s/it]  5%|▌         | 8/150 [06:10<1:46:58, 45.20s/it]  6%|▌         | 9/150 [06:55<1:46:09, 45.17s/it]  7%|▋         | 10/150 [07:40<1:45:05, 45.04s/it]                                                  {'loss': 1.5729, 'grad_norm': 1.2371982336044312, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [07:40<1:45:05, 45.04s/it]  7%|▋         | 11/150 [08:25<1:44:07, 44.94s/it]  8%|▊         | 12/150 [09:10<1:43:20, 44.93s/it]  9%|▊         | 13/150 [09:54<1:42:22, 44.84s/it]  9%|▉         | 14/150 [10:39<1:41:27, 44.76s/it] 10%|█         | 15/150 [11:24<1:40:50, 44.82s/it] 11%|█         | 16/150 [12:09<1:40:12, 44.87s/it] 11%|█▏        | 17/150 [12:54<1:39:36, 44.93s/it] 12%|█▏        | 18/150 [13:38<1:38:41, 44.86s/it] 13%|█▎        | 19/150 [14:24<1:38:09, 44.96s/it] 13%|█▎        | 20/150 [15:09<1:37:39, 45.07s/it]                                                  {'loss': 0.8294, 'grad_norm': 0.5843471884727478, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [15:09<1:37:39, 45.07s/it] 14%|█▍        | 21/150 [15:54<1:36:57, 45.10s/it] 15%|█▍        | 22/150 [16:39<1:36:17, 45.14s/it] 15%|█▌        | 23/150 [17:25<1:35:43, 45.23s/it] 16%|█▌        | 24/150 [18:10<1:34:50, 45.16s/it] 17%|█▋        | 25/150 [18:54<1:33:46, 45.02s/it] 17%|█▋        | 26/150 [19:39<1:32:58, 44.99s/it] 18%|█▊        | 27/150 [20:25<1:32:20, 45.05s/it] 19%|█▊        | 28/150 [21:10<1:31:34, 45.04s/it] 19%|█▉        | 29/150 [21:54<1:30:41, 44.97s/it] 20%|██        | 30/150 [22:39<1:29:48, 44.90s/it]                                                  {'loss': 0.3774, 'grad_norm': 0.3095102608203888, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [22:39<1:29:48, 44.90s/it] 21%|██        | 31/150 [23:24<1:28:58, 44.86s/it] 21%|██▏       | 32/150 [24:09<1:28:09, 44.83s/it] 22%|██▏       | 33/150 [24:53<1:27:23, 44.82s/it] 23%|██▎       | 34/150 [25:38<1:26:32, 44.76s/it] 23%|██▎       | 35/150 [26:23<1:25:57, 44.85s/it] 24%|██▍       | 36/150 [27:08<1:25:12, 44.85s/it] 25%|██▍       | 37/150 [27:53<1:24:28, 44.85s/it] 25%|██▌       | 38/150 [28:38<1:24:07, 45.06s/it] 26%|██▌       | 39/150 [29:23<1:23:07, 44.93s/it] 27%|██▋       | 40/150 [30:08<1:22:22, 44.93s/it]                                                  {'loss': 0.3052, 'grad_norm': 0.28782618045806885, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [30:08<1:22:22, 44.93s/it] 27%|██▋       | 41/150 [30:53<1:21:41, 44.97s/it] 28%|██▊       | 42/150 [31:38<1:20:53, 44.94s/it] 29%|██▊       | 43/150 [32:23<1:20:13, 44.98s/it] 29%|██▉       | 44/150 [33:08<1:19:33, 45.03s/it] 30%|███       | 45/150 [33:53<1:18:40, 44.96s/it] 31%|███       | 46/150 [34:38<1:17:45, 44.86s/it] 31%|███▏      | 47/150 [35:22<1:16:59, 44.85s/it] 32%|███▏      | 48/150 [36:07<1:16:11, 44.82s/it] 33%|███▎      | 49/150 [36:52<1:15:29, 44.85s/it] 33%|███▎      | 50/150 [37:37<1:14:42, 44.83s/it]                                                  {'loss': 0.2437, 'grad_norm': 0.30659088492393494, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [37:37<1:14:42, 44.83s/it] 34%|███▍      | 51/150 [38:22<1:13:58, 44.83s/it] 35%|███▍      | 52/150 [39:07<1:13:14, 44.85s/it] 35%|███▌      | 53/150 [39:51<1:12:28, 44.83s/it] 36%|███▌      | 54/150 [40:35<1:11:08, 44.46s/it] 37%|███▋      | 55/150 [41:19<1:10:18, 44.40s/it] 37%|███▋      | 56/150 [42:04<1:09:45, 44.53s/it] 38%|███▊      | 57/150 [42:49<1:09:14, 44.67s/it] 39%|███▊      | 58/150 [43:34<1:08:35, 44.74s/it] 39%|███▉      | 59/150 [44:18<1:07:35, 44.56s/it] 40%|████      | 60/150 [45:01<1:06:12, 44.13s/it]                                                  {'loss': 0.2178, 'grad_norm': 0.27273014187812805, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [45:01<1:06:12, 44.13s/it] 41%|████      | 61/150 [45:45<1:05:17, 44.02s/it] 41%|████▏     | 62/150 [46:29<1:04:34, 44.03s/it] 42%|████▏     | 63/150 [47:13<1:03:54, 44.07s/it] 43%|████▎     | 64/150 [47:57<1:03:14, 44.12s/it] 43%|████▎     | 65/150 [48:42<1:02:39, 44.23s/it] 44%|████▍     | 66/150 [49:26<1:01:57, 44.25s/it] 45%|████▍     | 67/150 [50:10<1:01:12, 44.25s/it] 45%|████▌     | 68/150 [50:55<1:00:27, 44.24s/it] 46%|████▌     | 69/150 [51:38<59:19, 43.94s/it]   47%|████▋     | 70/150 [52:22<58:39, 43.99s/it]                                                {'loss': 0.1927, 'grad_norm': 0.2992996871471405, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [52:22<58:39, 43.99s/it] 47%|████▋     | 71/150 [53:06<58:05, 44.12s/it] 48%|████▊     | 72/150 [53:50<57:19, 44.10s/it] 49%|████▊     | 73/150 [54:35<56:43, 44.20s/it] 49%|████▉     | 74/150 [55:19<56:04, 44.26s/it] 50%|█████     | 75/150 [56:04<55:19, 44.26s/it] 51%|█████     | 76/150 [56:47<54:23, 44.11s/it] 51%|█████▏    | 77/150 [57:31<53:33, 44.02s/it] 52%|█████▏    | 78/150 [58:15<52:42, 43.92s/it] 53%|█████▎    | 79/150 [58:59<51:53, 43.86s/it] 53%|█████▎    | 80/150 [59:42<50:58, 43.69s/it]                                                {'loss': 0.1957, 'grad_norm': 0.21672987937927246, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [59:42<50:58, 43.69s/it] 54%|█████▍    | 81/150 [1:00:25<50:02, 43.52s/it] 55%|█████▍    | 82/150 [1:01:08<49:16, 43.47s/it] 55%|█████▌    | 83/150 [1:01:52<48:28, 43.40s/it] 56%|█████▌    | 84/150 [1:02:35<47:38, 43.31s/it] 57%|█████▋    | 85/150 [1:03:18<46:51, 43.25s/it] 57%|█████▋    | 86/150 [1:04:01<46:07, 43.25s/it] 58%|█████▊    | 87/150 [1:04:44<45:19, 43.17s/it] 59%|█████▊    | 88/150 [1:05:27<44:29, 43.05s/it] 59%|█████▉    | 89/150 [1:06:09<43:39, 42.94s/it] 60%|██████    | 90/150 [1:06:52<42:56, 42.95s/it]                                                  {'loss': 0.1984, 'grad_norm': 0.31348055601119995, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [1:06:52<42:56, 42.95s/it] 61%|██████    | 91/150 [1:07:35<42:08, 42.86s/it] 61%|██████▏   | 92/150 [1:08:18<41:23, 42.81s/it] 62%|██████▏   | 93/150 [1:09:00<40:37, 42.76s/it] 63%|██████▎   | 94/150 [1:09:43<39:52, 42.73s/it] 63%|██████▎   | 95/150 [1:10:26<39:16, 42.84s/it] 64%|██████▍   | 96/150 [1:11:09<38:30, 42.79s/it] 65%|██████▍   | 97/150 [1:11:52<37:45, 42.75s/it] 65%|██████▌   | 98/150 [1:12:35<37:09, 42.87s/it] 66%|██████▌   | 99/150 [1:13:18<36:26, 42.87s/it] 67%|██████▋   | 100/150 [1:14:00<35:41, 42.84s/it]                                                   {'loss': 0.2068, 'grad_norm': 0.2508975863456726, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [1:14:00<35:41, 42.84s/it] 67%|██████▋   | 101/150 [1:14:43<34:58, 42.83s/it] 68%|██████▊   | 102/150 [1:15:26<34:14, 42.81s/it] 69%|██████▊   | 103/150 [1:16:09<33:30, 42.77s/it] 69%|██████▉   | 104/150 [1:16:51<32:46, 42.75s/it] 70%|███████   | 105/150 [1:17:34<32:00, 42.68s/it] 71%|███████   | 106/150 [1:18:16<31:15, 42.63s/it] 71%|███████▏  | 107/150 [1:18:59<30:32, 42.61s/it] 72%|███████▏  | 108/150 [1:19:41<29:46, 42.54s/it] 73%|███████▎  | 109/150 [1:20:24<29:02, 42.50s/it] 73%|███████▎  | 110/150 [1:21:06<28:22, 42.55s/it]                                                   {'loss': 0.1778, 'grad_norm': 0.4557206928730011, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [1:21:06<28:22, 42.55s/it] 74%|███████▍  | 111/150 [1:21:49<27:41, 42.61s/it] 75%|███████▍  | 112/150 [1:22:32<27:04, 42.75s/it] 75%|███████▌  | 113/150 [1:23:15<26:21, 42.74s/it] 76%|███████▌  | 114/150 [1:23:57<25:37, 42.69s/it] 77%|███████▋  | 115/150 [1:24:40<24:52, 42.65s/it] 77%|███████▋  | 116/150 [1:25:23<24:09, 42.62s/it] 78%|███████▊  | 117/150 [1:26:05<23:28, 42.68s/it] 79%|███████▊  | 118/150 [1:26:48<22:47, 42.75s/it] 79%|███████▉  | 119/150 [1:27:31<22:04, 42.71s/it] 80%|████████  | 120/150 [1:28:14<21:21, 42.71s/it]                                                   {'loss': 0.2107, 'grad_norm': 0.31233182549476624, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [1:28:14<21:21, 42.71s/it] 81%|████████  | 121/150 [1:28:56<20:39, 42.73s/it] 81%|████████▏ | 122/150 [1:29:39<19:56, 42.72s/it] 82%|████████▏ | 123/150 [1:30:22<19:13, 42.71s/it] 83%|████████▎ | 124/150 [1:31:05<18:31, 42.73s/it] 83%|████████▎ | 125/150 [1:31:47<17:49, 42.79s/it] 84%|████████▍ | 126/150 [1:32:30<17:08, 42.86s/it] 85%|████████▍ | 127/150 [1:33:13<16:25, 42.83s/it] 85%|████████▌ | 128/150 [1:33:56<15:43, 42.88s/it] 86%|████████▌ | 129/150 [1:34:39<14:58, 42.80s/it] 87%|████████▋ | 130/150 [1:35:21<14:14, 42.71s/it]                                                   {'loss': 0.1553, 'grad_norm': 0.17616242170333862, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [1:35:21<14:14, 42.71s/it] 87%|████████▋ | 131/150 [1:36:04<13:30, 42.66s/it] 88%|████████▊ | 132/150 [1:36:46<12:47, 42.62s/it] 89%|████████▊ | 133/150 [1:37:29<12:04, 42.59s/it] 89%|████████▉ | 134/150 [1:38:11<11:20, 42.54s/it] 90%|█████████ | 135/150 [1:38:54<10:37, 42.48s/it] 91%|█████████ | 136/150 [1:39:36<09:55, 42.50s/it] 91%|█████████▏| 137/150 [1:40:19<09:11, 42.44s/it] 92%|█████████▏| 138/150 [1:41:01<08:29, 42.47s/it] 93%|█████████▎| 139/150 [1:41:44<07:47, 42.48s/it] 93%|█████████▎| 140/150 [1:42:26<07:05, 42.50s/it]                                                   {'loss': 0.1883, 'grad_norm': 0.3463320732116699, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [1:42:26<07:05, 42.50s/it] 94%|█████████▍| 141/150 [1:43:09<06:23, 42.56s/it] 95%|█████████▍| 142/150 [1:43:52<05:41, 42.64s/it] 95%|█████████▌| 143/150 [1:44:35<04:59, 42.79s/it] 96%|█████████▌| 144/150 [1:45:17<04:16, 42.76s/it] 97%|█████████▋| 145/150 [1:46:00<03:33, 42.75s/it] 97%|█████████▋| 146/150 [1:46:43<02:50, 42.72s/it] 98%|█████████▊| 147/150 [1:47:25<02:07, 42.66s/it] 99%|█████████▊| 148/150 [1:48:09<01:25, 42.95s/it] 99%|█████████▉| 149/150 [1:48:52<00:42, 42.88s/it]100%|██████████| 150/150 [1:49:34<00:00, 42.83s/it]                                                   {'loss': 0.157, 'grad_norm': 0.22555221617221832, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [1:49:34<00:00, 42.83s/it][INFO|trainer.py:3705] 2024-11-19 00:40:50,946 >> Saving model checkpoint to saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-19 00:40:50,971 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:40:50,972 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:40:51,259 >> tokenizer config file saved in saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:40:51,260 >> Special tokens file saved in saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/special_tokens_map.json
[2024-11-19 00:40:53,122] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is about to be saved!
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-11-19 00:40:53,291] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-11-19 00:40:53,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-11-19 00:40:53,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-11-19 00:40:53,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-11-19 00:40:54,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-11-19 00:40:54,374] [INFO] [engine.py:3483:_save_zero_checkpoint] zero checkpoint saved saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-11-19 00:40:54,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
[INFO|trainer.py:2505] 2024-11-19 00:40:54,511 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 6599.8568, 'train_samples_per_second': 0.366, 'train_steps_per_second': 0.023, 'train_loss': 0.348620662689209, 'epoch': 2.98}
100%|██████████| 150/150 [1:49:59<00:00, 42.83s/it]100%|██████████| 150/150 [1:49:59<00:00, 44.00s/it]
[INFO|trainer.py:3705] 2024-11-19 00:41:12,660 >> Saving model checkpoint to saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft
[INFO|configuration_utils.py:673] 2024-11-19 00:41:12,681 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-32B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-19 00:41:12,682 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 27648,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 64,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-19 00:41:12,967 >> tokenizer config file saved in saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-19 00:41:12,967 >> Special tokens file saved in saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               =   343851GF
  train_loss               =     0.3486
  train_runtime            = 1:49:59.85
  train_samples_per_second =      0.366
  train_steps_per_second   =      0.023
Figure saved at: saves/Qwen2.5-32B-Instruct/my_prompt_modelsize32B/lora/sft/training_loss.png
11/19/2024 00:41:14 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/19/2024 00:41:14 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-19 00:41:14,817 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-19 00:41:14,817 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-19 00:41:14,818 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:03<01:19,  1.84s/it]  7%|▋         | 3/45 [00:05<01:19,  1.89s/it]  9%|▉         | 4/45 [00:07<01:18,  1.92s/it] 11%|█         | 5/45 [00:09<01:17,  1.93s/it] 13%|█▎        | 6/45 [00:11<01:15,  1.95s/it] 16%|█▌        | 7/45 [00:13<01:14,  1.95s/it] 18%|█▊        | 8/45 [00:15<01:12,  1.95s/it] 20%|██        | 9/45 [00:17<01:10,  1.96s/it] 22%|██▏       | 10/45 [00:19<01:08,  1.96s/it] 24%|██▍       | 11/45 [00:21<01:06,  1.96s/it] 27%|██▋       | 12/45 [00:23<01:04,  1.96s/it] 29%|██▉       | 13/45 [00:25<01:02,  1.96s/it] 31%|███       | 14/45 [00:27<01:00,  1.96s/it] 33%|███▎      | 15/45 [00:29<00:58,  1.96s/it] 36%|███▌      | 16/45 [00:31<00:56,  1.96s/it] 38%|███▊      | 17/45 [00:33<00:54,  1.96s/it] 40%|████      | 18/45 [00:35<00:52,  1.96s/it] 42%|████▏     | 19/45 [00:37<00:50,  1.96s/it] 44%|████▍     | 20/45 [00:38<00:49,  1.96s/it] 47%|████▋     | 21/45 [00:40<00:47,  1.96s/it] 49%|████▉     | 22/45 [00:42<00:45,  1.96s/it] 51%|█████     | 23/45 [00:44<00:43,  1.96s/it] 53%|█████▎    | 24/45 [00:46<00:41,  1.96s/it] 56%|█████▌    | 25/45 [00:48<00:39,  1.96s/it] 58%|█████▊    | 26/45 [00:50<00:37,  1.96s/it] 60%|██████    | 27/45 [00:52<00:35,  1.96s/it] 62%|██████▏   | 28/45 [00:54<00:33,  1.96s/it] 64%|██████▍   | 29/45 [00:56<00:31,  1.96s/it] 67%|██████▋   | 30/45 [00:58<00:29,  1.96s/it] 69%|██████▉   | 31/45 [01:00<00:27,  1.96s/it] 71%|███████   | 32/45 [01:02<00:25,  1.96s/it] 73%|███████▎  | 33/45 [01:04<00:23,  1.96s/it] 76%|███████▌  | 34/45 [01:06<00:21,  1.96s/it] 78%|███████▊  | 35/45 [01:08<00:19,  1.96s/it] 80%|████████  | 36/45 [01:10<00:17,  1.96s/it] 82%|████████▏ | 37/45 [01:12<00:15,  1.96s/it] 84%|████████▍ | 38/45 [01:14<00:13,  1.96s/it] 87%|████████▋ | 39/45 [01:16<00:11,  1.96s/it] 89%|████████▉ | 40/45 [01:18<00:09,  1.96s/it] 91%|█████████ | 41/45 [01:20<00:07,  1.96s/it] 93%|█████████▎| 42/45 [01:22<00:05,  1.96s/it] 96%|█████████▌| 43/45 [01:24<00:03,  1.96s/it] 98%|█████████▊| 44/45 [01:26<00:01,  1.96s/it]100%|██████████| 45/45 [01:28<00:00,  1.96s/it]100%|██████████| 45/45 [01:28<00:00,  1.96s/it]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.1773
  eval_runtime            = 0:01:30.43
  eval_samples_per_second =      0.995
  eval_steps_per_second   =      0.498
[INFO|modelcard.py:449] 2024-11-19 00:42:45,258 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
