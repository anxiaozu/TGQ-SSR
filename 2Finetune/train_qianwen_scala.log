nohup: ignoring input
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:54:41,180] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:54:45 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:21378
[2024-11-18 21:54:47,163] torch.distributed.run: [WARNING] 
[2024-11-18 21:54:47,163] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 21:54:47,163] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 21:54:47,163] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:54:54,510] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 21:54:54,824] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:54:55 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:54:55 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 21:54:55,582 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:54:55,583 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:55,585 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:54:56,048 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 21:54:56,049 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:54:56,050 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:54:56,051 >> loading file tokenizer_config.json
11/18/2024 21:54:56 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:54:56 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:54:56,508 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 21:54:56 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:54:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 21:54:56 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_1_9.json...
11/18/2024 21:54:57 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:54:57 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 128 examples [00:00, 5275.85 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/128 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 8/128 [00:00<00:01, 64.82 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 128/128 [00:00<00:00, 431.57 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 21:55:00 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_1_9.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/128 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 8/128 [00:01<00:15,  7.60 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 16/128 [00:01<00:07, 14.68 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 24/128 [00:01<00:04, 20.89 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 40/128 [00:01<00:02, 29.66 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 48/128 [00:01<00:02, 32.51 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 56/128 [00:02<00:02, 34.78 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 64/128 [00:02<00:02, 29.10 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 80/128 [00:02<00:01, 33.79 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 104/128 [00:03<00:00, 42.99 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 112/128 [00:03<00:00, 36.49 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 128/128 [00:03<00:00, 45.86 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 128/128 [00:03<00:00, 32.18 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 57804, 11, 79256, 606, 1131, 42, 2351, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 6608, 268, 920, 337, 4757, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 41, 6130, 1466, 62, 30172, 5478, 2763, 6297, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 5050, 10102, 17416, 1088, 6720, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 47681, 2568, 8347, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 47241, 1098, 16789, 998, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 101599, 42, 13, 100623, 99729, 9370, 110932, 104597, 5373, 50377, 45785, 5373, 42, 13, 105899, 110821, 5373, 109391, 101034, 110932, 9370, 307, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 25039, 2203, 6294, 7, 77, 17, 25, 2203, 8, 220, 1380, 308, 16, 72056, 1131, 42, 3159, 470, 308, 17, 5406, 11, 308, 17, 97506, 1028, 11, 308, 16, 38611, 2591, 11, 308, 16, 48337, 11, 308, 17, 1764, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为organisation,属性name='K_L_University'的节点
label为organisation,属性name='Keen_College'的节点
label为organisation,属性name='Jinnah_University_for_Women'的节点
label为tag,属性name='My_Kinda_Party'的节点
label为tag,属性name='Kevin_Rudd'的节点
label为tag,属性name='Ken_Schrader'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找名为K.的人喜欢的帖子的内容、创建日期、K.使用的浏览器、性别以及帖子的id<|im_end|>
<|im_start|>assistant
match (n1:person)-[r1:likespost]->(n2:post)  where n1.firstname='K.' return n2.content, n2.creationdate, n1.browserused, n1.gender, n2.id;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 7287, 58, 81, 16, 25, 25039, 2203, 6294, 7, 77, 17, 25, 2203, 8, 220, 1380, 308, 16, 72056, 1131, 42, 3159, 470, 308, 17, 5406, 11, 308, 17, 97506, 1028, 11, 308, 16, 38611, 2591, 11, 308, 16, 48337, 11, 308, 17, 1764, 26, 151645]
labels:
match (n1:person)-[r1:likespost]->(n2:post)  where n1.firstname='K.' return n2.content, n2.creationdate, n1.browserused, n1.gender, n2.id;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 21:55:05,397 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:55:05,398 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 21:55:05,449 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 21:55:05,449 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 21:55:05,451 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 21:55:09,967 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 21:55:09,967 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 21:55:09,971 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 21:55:09,972 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 21:55:09 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:55:09 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:55:09 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:55:09 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:55:09 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,down_proj,o_proj,v_proj,k_proj,gate_proj,up_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
11/18/2024 21:55:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:55:10 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:55:10 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:55:10 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:55:10 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,up_proj,gate_proj,k_proj,v_proj,down_proj,q_proj
11/18/2024 21:55:10 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 21:55:10,685 >> Using auto half precision backend
11/18/2024 21:55:10 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 21:55:11,321 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 21:55:11,322 >>   Num examples = 115
[INFO|trainer.py:2245] 2024-11-18 21:55:11,322 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 21:55:11,322 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 21:55:11,322 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 21:55:11,322 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 21:55:11,322 >>   Total optimization steps = 21
[INFO|trainer.py:2252] 2024-11-18 21:55:11,328 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/21 [00:00<?, ?it/s]  5%|▍         | 1/21 [00:04<01:22,  4.14s/it] 10%|▉         | 2/21 [00:07<01:14,  3.90s/it] 14%|█▍        | 3/21 [00:11<01:08,  3.81s/it] 19%|█▉        | 4/21 [00:15<01:04,  3.77s/it] 24%|██▍       | 5/21 [00:18<00:59,  3.74s/it] 29%|██▊       | 6/21 [00:22<00:55,  3.73s/it] 33%|███▎      | 7/21 [00:26<00:52,  3.72s/it] 38%|███▊      | 8/21 [00:30<00:48,  3.71s/it] 43%|████▎     | 9/21 [00:33<00:44,  3.71s/it] 48%|████▊     | 10/21 [00:37<00:40,  3.72s/it]                                               {'loss': 1.4545, 'grad_norm': 1.1208100318908691, 'learning_rate': 6.710100716628344e-05, 'epoch': 1.38}
 48%|████▊     | 10/21 [00:37<00:40,  3.72s/it] 52%|█████▏    | 11/21 [00:41<00:37,  3.71s/it] 57%|█████▋    | 12/21 [00:44<00:33,  3.71s/it] 62%|██████▏   | 13/21 [00:48<00:29,  3.71s/it] 67%|██████▋   | 14/21 [00:52<00:25,  3.70s/it] 71%|███████▏  | 15/21 [00:56<00:22,  3.70s/it] 76%|███████▌  | 16/21 [00:59<00:18,  3.69s/it] 81%|████████  | 17/21 [01:03<00:14,  3.69s/it] 86%|████████▌ | 18/21 [01:07<00:11,  3.70s/it] 90%|█████████ | 19/21 [01:10<00:07,  3.70s/it] 95%|█████████▌| 20/21 [01:14<00:03,  3.70s/it]                                               {'loss': 0.6676, 'grad_norm': 0.756187379360199, 'learning_rate': 7.596123493895991e-07, 'epoch': 2.76}
 95%|█████████▌| 20/21 [01:14<00:03,  3.70s/it]100%|██████████| 21/21 [01:18<00:00,  3.72s/it][INFO|trainer.py:3705] 2024-11-18 21:56:30,293 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/checkpoint-21
[INFO|configuration_utils.py:673] 2024-11-18 21:56:30,330 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:56:30,332 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 21:56:30,499 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/checkpoint-21/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 21:56:30,499 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/checkpoint-21/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 21:56:31,207 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 79.8787, 'train_samples_per_second': 4.319, 'train_steps_per_second': 0.263, 'train_loss': 1.0291308235554468, 'epoch': 2.9}
100%|██████████| 21/21 [01:19<00:00,  3.72s/it]100%|██████████| 21/21 [01:19<00:00,  3.77s/it]
[INFO|trainer.py:3705] 2024-11-18 21:56:31,224 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 21:56:31,256 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:56:31,257 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 21:56:31,410 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 21:56:31,411 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.8966
  total_flos               = 13322188GF
  train_loss               =     1.0291
  train_runtime            = 0:01:19.87
  train_samples_per_second =      4.319
  train_steps_per_second   =      0.263
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_1_9/lora/sft/training_loss.png
11/18/2024 21:56:31 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 21:56:31 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 21:56:31,873 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 21:56:31,873 >>   Num examples = 13
[INFO|trainer.py:4026] 2024-11-18 21:56:31,873 >>   Batch size = 1
  0%|          | 0/7 [00:00<?, ?it/s] 29%|██▊       | 2/7 [00:00<00:00, 16.42it/s] 57%|█████▋    | 4/7 [00:00<00:00,  9.95it/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.01it/s]100%|██████████| 7/7 [00:00<00:00,  8.82it/s]100%|██████████| 7/7 [00:00<00:00,  9.37it/s]
***** eval metrics *****
  epoch                   =     2.8966
  eval_loss               =     0.6983
  eval_runtime            = 0:00:00.88
  eval_samples_per_second =      14.68
  eval_steps_per_second   =      7.905
[INFO|modelcard.py:449] 2024-11-18 21:56:32,759 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:56:55,275] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:56:59 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:25987
[2024-11-18 21:57:01,359] torch.distributed.run: [WARNING] 
[2024-11-18 21:57:01,359] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 21:57:01,359] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 21:57:01,359] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 21:57:08,715] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 21:57:08,920] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 21:57:09 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:57:09 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 21:57:09,825 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:57:09,827 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,828 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,829 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,829 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,829 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,829 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:09,829 >> loading file tokenizer_config.json
11/18/2024 21:57:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 21:57:10 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:57:10,305 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 21:57:10,306 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:57:10,307 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 21:57:10,308 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 21:57:10,792 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 21:57:10 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:57:10 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 21:57:10 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_2_8.json...
11/18/2024 21:57:11 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 21:57:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 256 examples [00:00, 8095.28 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/256 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 16/256 [00:00<00:01, 131.69 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 256/256 [00:00<00:00, 937.86 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 21:57:15 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_2_8.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/256 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 16/256 [00:01<00:17, 13.99 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 32/256 [00:01<00:08, 27.78 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 48/256 [00:01<00:05, 39.84 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 64/256 [00:01<00:03, 50.06 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 80/256 [00:01<00:03, 57.35 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 112/256 [00:02<00:02, 61.73 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 128/256 [00:02<00:02, 63.47 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 144/256 [00:02<00:01, 62.57 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 176/256 [00:03<00:00, 86.15 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 192/256 [00:03<00:00, 73.66 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 208/256 [00:03<00:00, 76.97 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 224/256 [00:03<00:00, 63.33 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 256/256 [00:04<00:00, 61.74 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 21, 315, 41601, 685, 4716, 19644, 33677, 265, 276, 84, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 23950, 2568, 266, 360, 78328, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 45007, 1245, 10432, 1566, 70, 324, 7231, 1139, 24, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 33603, 15359, 1557, 5120, 72, 920, 337, 4757, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 33, 404, 7751, 1604, 39537, 7751, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 74639, 461, 8294, 1098, 370, 266, 6591, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 675, 3575, 16068, 49223, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 785, 19015, 3575, 16068, 49223, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 101599, 74639, 461, 8294, 1098, 370, 266, 6591, 100136, 31905, 17714, 51, 16156, 4476, 9370, 105151, 29991, 90395, 59879, 29991, 99457, 32044, 108467, 3837, 31526, 24562, 21, 18947, 59151, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 4578, 7287, 58, 81, 16, 69651, 21754, 6294, 7, 77, 17, 25, 4578, 1040, 8, 220, 1380, 308, 16, 2644, 1131, 74639, 461, 8294, 1098, 370, 266, 6591, 6, 323, 308, 17, 2644, 1131, 51, 16156, 4476, 6, 470, 308, 16, 2644, 1973, 553, 308, 16, 2644, 6560, 3930, 220, 21, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Album 6 of Natalia Barbu Codreanu'的节点
label为organisation,属性name='Sam_Ratulangi_University'的节点
label为organisation,属性name='Jose_Maria_Agurrie_T9'的节点
label为organisation,属性name='Basanti_Devi_College'的节点
label为place,属性name='Birni_Nkonni'的节点
label为tag,属性name='Gabriela_Sabatini'的节点
label为tag,属性name='Name_of_the_Game'的节点
label为tag,属性name='The_Name_of_the_Game'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找名为Gabriela_Sabatini且类型为TennisPlayer的标签名称，并按名称降序排列，返回前6个结果<|im_end|>
<|im_start|>assistant
match (n1:tag)-[r1:hastype]->(n2:tagclass)  where n1.name='Gabriela_Sabatini' and n2.name='TennisPlayer' return n1.name order by n1.name desc limit 6;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 4578, 7287, 58, 81, 16, 69651, 21754, 6294, 7, 77, 17, 25, 4578, 1040, 8, 220, 1380, 308, 16, 2644, 1131, 74639, 461, 8294, 1098, 370, 266, 6591, 6, 323, 308, 17, 2644, 1131, 51, 16156, 4476, 6, 470, 308, 16, 2644, 1973, 553, 308, 16, 2644, 6560, 3930, 220, 21, 26, 151645]
labels:
match (n1:tag)-[r1:hastype]->(n2:tagclass)  where n1.name='Gabriela_Sabatini' and n2.name='TennisPlayer' return n1.name order by n1.name desc limit 6;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 21:57:20,303 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 21:57:20,305 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 21:57:20,354 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 21:57:20,354 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 21:57:20,356 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 21:57:24,843 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 21:57:24,843 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 21:57:24,847 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 21:57:24,848 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 21:57:24 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:57:24 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:57:24 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:57:24 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:57:24 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,gate_proj,up_proj,o_proj,v_proj,down_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]
11/18/2024 21:57:25 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 21:57:25 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 21:57:25 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 21:57:25 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 21:57:25 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,down_proj,gate_proj,k_proj,q_proj,v_proj,o_proj
11/18/2024 21:57:25 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 21:57:25,564 >> Using auto half precision backend
11/18/2024 21:57:25 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 21:57:26,182 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 21:57:26,183 >>   Num examples = 230
[INFO|trainer.py:2245] 2024-11-18 21:57:26,183 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 21:57:26,183 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 21:57:26,183 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 21:57:26,183 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 21:57:26,183 >>   Total optimization steps = 42
[INFO|trainer.py:2252] 2024-11-18 21:57:26,190 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/42 [00:00<?, ?it/s]  2%|▏         | 1/42 [00:04<02:48,  4.12s/it]  5%|▍         | 2/42 [00:07<02:35,  3.88s/it]  7%|▋         | 3/42 [00:11<02:27,  3.79s/it] 10%|▉         | 4/42 [00:15<02:22,  3.76s/it] 12%|█▏        | 5/42 [00:18<02:18,  3.74s/it] 14%|█▍        | 6/42 [00:22<02:14,  3.72s/it] 17%|█▋        | 7/42 [00:26<02:10,  3.72s/it] 19%|█▉        | 8/42 [00:30<02:06,  3.72s/it] 21%|██▏       | 9/42 [00:33<02:02,  3.71s/it] 24%|██▍       | 10/42 [00:37<01:59,  3.73s/it]                                               {'loss': 1.5734, 'grad_norm': 1.3268322944641113, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.7}
 24%|██▍       | 10/42 [00:37<01:59,  3.73s/it] 26%|██▌       | 11/42 [00:41<01:55,  3.73s/it] 29%|██▊       | 12/42 [00:44<01:51,  3.72s/it] 31%|███       | 13/42 [00:48<01:47,  3.71s/it] 33%|███▎      | 14/42 [00:52<01:44,  3.72s/it] 36%|███▌      | 15/42 [00:56<01:40,  3.71s/it] 38%|███▊      | 16/42 [00:59<01:36,  3.71s/it] 40%|████      | 17/42 [01:03<01:32,  3.70s/it] 43%|████▎     | 18/42 [01:07<01:28,  3.70s/it] 45%|████▌     | 19/42 [01:10<01:25,  3.70s/it] 48%|████▊     | 20/42 [01:14<01:21,  3.70s/it]                                               {'loss': 0.6617, 'grad_norm': 0.7134836912155151, 'learning_rate': 6.464113856382752e-05, 'epoch': 1.39}
 48%|████▊     | 20/42 [01:14<01:21,  3.70s/it] 50%|█████     | 21/42 [01:18<01:17,  3.69s/it] 52%|█████▏    | 22/42 [01:21<01:13,  3.70s/it] 55%|█████▍    | 23/42 [01:25<01:10,  3.70s/it] 57%|█████▋    | 24/42 [01:29<01:06,  3.70s/it] 60%|█████▉    | 25/42 [01:33<01:02,  3.70s/it] 62%|██████▏   | 26/42 [01:36<00:59,  3.70s/it] 64%|██████▍   | 27/42 [01:40<00:55,  3.70s/it] 67%|██████▋   | 28/42 [01:44<00:51,  3.71s/it] 69%|██████▉   | 29/42 [01:47<00:48,  3.70s/it] 71%|███████▏  | 30/42 [01:51<00:44,  3.73s/it]                                               {'loss': 0.3661, 'grad_norm': 0.5352288484573364, 'learning_rate': 2.3784635822138424e-05, 'epoch': 2.09}
 71%|███████▏  | 30/42 [01:51<00:44,  3.73s/it] 74%|███████▍  | 31/42 [01:55<00:41,  3.73s/it] 76%|███████▌  | 32/42 [01:59<00:37,  3.72s/it] 79%|███████▊  | 33/42 [02:02<00:33,  3.71s/it] 81%|████████  | 34/42 [02:06<00:29,  3.70s/it] 83%|████████▎ | 35/42 [02:10<00:25,  3.70s/it] 86%|████████▌ | 36/42 [02:13<00:22,  3.69s/it] 88%|████████▊ | 37/42 [02:17<00:18,  3.68s/it] 90%|█████████ | 38/42 [02:21<00:14,  3.68s/it] 93%|█████████▎| 39/42 [02:24<00:11,  3.69s/it] 95%|█████████▌| 40/42 [02:28<00:07,  3.69s/it]                                               {'loss': 0.3312, 'grad_norm': 0.7755017876625061, 'learning_rate': 7.192044826145771e-07, 'epoch': 2.78}
 95%|█████████▌| 40/42 [02:28<00:07,  3.69s/it] 98%|█████████▊| 41/42 [02:32<00:03,  3.69s/it]100%|██████████| 42/42 [02:35<00:00,  3.69s/it][INFO|trainer.py:3705] 2024-11-18 22:00:02,825 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/checkpoint-42
[INFO|configuration_utils.py:673] 2024-11-18 22:00:02,863 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:02,864 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:00:03,023 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/checkpoint-42/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:00:03,023 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/checkpoint-42/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:00:03,725 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 157.535, 'train_samples_per_second': 4.38, 'train_steps_per_second': 0.267, 'train_loss': 0.7160529238837106, 'epoch': 2.92}
100%|██████████| 42/42 [02:36<00:00,  3.69s/it]100%|██████████| 42/42 [02:36<00:00,  3.73s/it]
[INFO|trainer.py:3705] 2024-11-18 22:00:03,742 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:00:03,774 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:03,775 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:00:03,926 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:00:03,926 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9217
  total_flos               = 26522022GF
  train_loss               =     0.7161
  train_runtime            = 0:02:37.53
  train_samples_per_second =       4.38
  train_steps_per_second   =      0.267
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_2_8/lora/sft/training_loss.png
11/18/2024 22:00:04 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:00:04 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:00:04,385 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:00:04,385 >>   Num examples = 26
[INFO|trainer.py:4026] 2024-11-18 22:00:04,386 >>   Batch size = 1
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:00, 15.76it/s] 31%|███       | 4/13 [00:00<00:00, 10.02it/s] 46%|████▌     | 6/13 [00:00<00:00,  9.10it/s] 54%|█████▍    | 7/13 [00:00<00:00,  8.81it/s] 62%|██████▏   | 8/13 [00:00<00:00,  8.50it/s] 69%|██████▉   | 9/13 [00:00<00:00,  8.42it/s] 77%|███████▋  | 10/13 [00:01<00:00,  8.36it/s] 85%|████████▍ | 11/13 [00:01<00:00,  8.16it/s] 92%|█████████▏| 12/13 [00:01<00:00,  8.14it/s]100%|██████████| 13/13 [00:01<00:00,  8.16it/s]100%|██████████| 13/13 [00:01<00:00,  8.70it/s]
***** eval metrics *****
  epoch                   =     2.9217
  eval_loss               =     0.3978
  eval_runtime            = 0:00:01.63
  eval_samples_per_second =      15.92
  eval_steps_per_second   =       7.96
[INFO|modelcard.py:449] 2024-11-18 22:00:06,019 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:00:28,465] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:00:32 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:23722
[2024-11-18 22:00:34,501] torch.distributed.run: [WARNING] 
[2024-11-18 22:00:34,501] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:00:34,501] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:00:34,501] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:00:41,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:00:42,086] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:00:42 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:00:42 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:00:43 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:00:43 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:00:43,219 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:43,220 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,222 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:00:43,697 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:00:43,698 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:43,700 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,700 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,700 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,701 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,701 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,701 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:00:43,701 >> loading file tokenizer_config.json
11/18/2024 22:00:43 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:00:43 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:00:44,189 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:00:44 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:00:44 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:00:44 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_3_7.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 384 examples [00:00, 9725.98 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/384 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 24/384 [00:00<00:02, 143.17 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 384/384 [00:00<00:00, 1125.87 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:00:48 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_3_7.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/384 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 24/384 [00:01<00:17, 21.09 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 48/384 [00:01<00:08, 41.69 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 72/384 [00:01<00:05, 60.60 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 96/384 [00:01<00:03, 76.80 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 120/384 [00:01<00:02, 90.02 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 144/384 [00:02<00:02, 100.51 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 168/384 [00:02<00:02, 105.98 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 192/384 [00:02<00:01, 108.89 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 216/384 [00:02<00:01, 106.36 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 240/384 [00:02<00:01, 103.10 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 288/384 [00:03<00:00, 129.65 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 336/384 [00:03<00:00, 138.46 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 360/384 [00:03<00:00, 118.45 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 384/384 [00:04<00:00, 111.07 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 384/384 [00:04<00:00, 91.51 examples/s] 
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 78902, 62, 8850, 304, 71895, 6595, 343, 6070, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 42, 408, 541, 920, 337, 4757, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 42, 1370, 276, 1400, 5077, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 34, 21478, 24773, 59452, 2439, 920, 337, 4757, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 35, 2416, 7959, 3563, 604, 10102, 9917, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 47241, 1098, 16789, 998, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 42, 48258, 1245, 4747, 268, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 91180, 1245, 580, 43, 610, 10715, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 101599, 42, 48258, 100623, 101963, 9370, 102002, 9370, 307, 5373, 60396, 3837, 101034, 75882, 103947, 116617, 5373, 109391, 5373, 102064, 5373, 6347, 320, 77, 16, 25, 22585, 7287, 58, 81, 16, 25, 4648, 9597, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 42, 48258, 6, 470, 308, 17, 9847, 11, 308, 17, 48337, 11, 308, 17, 31633, 11, 308, 17, 38611, 2591, 11, 308, 17, 1764, 11, 308, 17, 72056, 11, 308, 16, 6067, 11, 308, 17, 948, 19951, 11, 308, 16, 1764, 11, 308, 17, 8219, 573, 11, 308, 17, 72637, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Alicia_Keys in Jalpaiguri'的节点
label为organisation,属性name='Kendall_College'的节点
label为organisation,属性name='Kazan_Federal_University'的节点
label为organisation,属性name='Cleveland_State_Community_College'的节点
label为place,属性name='Dera_Ismail_Khan'的节点
label为tag,属性name='Ken_Schrader'的节点
label为tag,属性name='Karl_Malden'的节点
label为tag,属性name='Kyle_MacLachlan'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找名为Karl的人加入的论坛的id、标题，以及该人的电子邮件、性别、语言、match (n1:forum)-[r1:hasmember]->(n2:person)  where n2.firstname='Karl' return n2.email, n2.gender, n2.language, n2.browserused, n2.id, n2.firstname, n1.title, n2.birthday, n1.id, n2.locationip, n2.lastname;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 22585, 7287, 58, 81, 16, 25, 4648, 9597, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 42, 48258, 6, 470, 308, 17, 9847, 11, 308, 17, 48337, 11, 308, 17, 31633, 11, 308, 17, 38611, 2591, 11, 308, 17, 1764, 11, 308, 17, 72056, 11, 308, 16, 6067, 11, 308, 17, 948, 19951, 11, 308, 16, 1764, 11, 308, 17, 8219, 573, 11, 308, 17, 72637, 26, 151645]
labels:
match (n1:forum)-[r1:hasmember]->(n2:person)  where n2.firstname='Karl' return n2.email, n2.gender, n2.language, n2.browserused, n2.id, n2.firstname, n1.title, n2.birthday, n1.id, n2.locationip, n2.lastname;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:00:53,893 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:00:53,895 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:00:53,946 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:00:53,946 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:00:53,948 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:00:57,864 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:00:57,865 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:00:57,869 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:00:57,869 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,o_proj,gate_proj,down_proj,q_proj,up_proj,v_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:00:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:00:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,o_proj,up_proj,gate_proj,down_proj,q_proj,k_proj
11/18/2024 22:00:58 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:00:58,574 >> Using auto half precision backend
11/18/2024 22:00:58 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:00:59,298 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:00:59,298 >>   Num examples = 345
[INFO|trainer.py:2245] 2024-11-18 22:00:59,298 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:00:59,298 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:00:59,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:00:59,298 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:00:59,298 >>   Total optimization steps = 63
[INFO|trainer.py:2252] 2024-11-18 22:00:59,305 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/63 [00:00<?, ?it/s]  2%|▏         | 1/63 [00:04<04:19,  4.19s/it]  3%|▎         | 2/63 [00:07<03:59,  3.92s/it]  5%|▍         | 3/63 [00:11<03:49,  3.83s/it]  6%|▋         | 4/63 [00:15<03:43,  3.78s/it]  8%|▊         | 5/63 [00:19<03:38,  3.76s/it] 10%|▉         | 6/63 [00:22<03:32,  3.73s/it] 11%|█         | 7/63 [00:26<03:28,  3.73s/it] 13%|█▎        | 8/63 [00:30<03:24,  3.73s/it] 14%|█▍        | 9/63 [00:33<03:21,  3.73s/it] 16%|█▌        | 10/63 [00:37<03:17,  3.73s/it]                                               {'loss': 1.6436, 'grad_norm': 1.7438089847564697, 'learning_rate': 9.92935509259118e-05, 'epoch': 0.46}
 16%|█▌        | 10/63 [00:37<03:17,  3.73s/it] 17%|█▋        | 11/63 [00:41<03:14,  3.73s/it] 19%|█▉        | 12/63 [00:45<03:10,  3.73s/it] 21%|██        | 13/63 [00:48<03:06,  3.72s/it] 22%|██▏       | 14/63 [00:52<03:02,  3.72s/it] 24%|██▍       | 15/63 [00:56<02:58,  3.73s/it] 25%|██▌       | 16/63 [00:59<02:54,  3.72s/it] 27%|██▋       | 17/63 [01:03<02:50,  3.71s/it] 29%|██▊       | 18/63 [01:07<02:46,  3.71s/it] 30%|███       | 19/63 [01:11<02:43,  3.71s/it] 32%|███▏      | 20/63 [01:14<02:39,  3.71s/it]                                               {'loss': 0.6671, 'grad_norm': 0.6485623717308044, 'learning_rate': 8.728210824415827e-05, 'epoch': 0.92}
 32%|███▏      | 20/63 [01:14<02:39,  3.71s/it] 33%|███▎      | 21/63 [01:18<02:36,  3.71s/it] 35%|███▍      | 22/63 [01:22<02:32,  3.72s/it] 37%|███▋      | 23/63 [01:25<02:28,  3.71s/it] 38%|███▊      | 24/63 [01:29<02:24,  3.71s/it] 40%|███▉      | 25/63 [01:33<02:20,  3.71s/it] 41%|████▏     | 26/63 [01:37<02:17,  3.71s/it] 43%|████▎     | 27/63 [01:40<02:13,  3.70s/it] 44%|████▍     | 28/63 [01:44<02:10,  3.72s/it] 46%|████▌     | 29/63 [01:48<02:06,  3.72s/it] 48%|████▊     | 30/63 [01:51<02:02,  3.72s/it]                                               {'loss': 0.3831, 'grad_norm': 1.1769264936447144, 'learning_rate': 6.384177557124247e-05, 'epoch': 1.39}
 48%|████▊     | 30/63 [01:51<02:02,  3.72s/it] 49%|████▉     | 31/63 [01:55<01:58,  3.72s/it] 51%|█████     | 32/63 [01:59<01:55,  3.72s/it] 52%|█████▏    | 33/63 [02:03<01:51,  3.71s/it] 54%|█████▍    | 34/63 [02:06<01:47,  3.71s/it] 56%|█████▌    | 35/63 [02:10<01:43,  3.71s/it] 57%|█████▋    | 36/63 [02:14<01:40,  3.72s/it] 59%|█████▊    | 37/63 [02:17<01:36,  3.72s/it] 60%|██████    | 38/63 [02:21<01:32,  3.72s/it] 62%|██████▏   | 39/63 [02:25<01:29,  3.72s/it] 63%|██████▎   | 40/63 [02:29<01:25,  3.71s/it]                                               {'loss': 0.2966, 'grad_norm': 0.6559790372848511, 'learning_rate': 3.6158224428757535e-05, 'epoch': 1.85}
 63%|██████▎   | 40/63 [02:29<01:25,  3.71s/it] 65%|██████▌   | 41/63 [02:32<01:21,  3.72s/it] 67%|██████▋   | 42/63 [02:36<01:18,  3.72s/it] 68%|██████▊   | 43/63 [02:40<01:14,  3.71s/it] 70%|██████▉   | 44/63 [02:43<01:10,  3.72s/it] 71%|███████▏  | 45/63 [02:47<01:06,  3.72s/it] 73%|███████▎  | 46/63 [02:51<01:03,  3.71s/it] 75%|███████▍  | 47/63 [02:55<00:59,  3.72s/it] 76%|███████▌  | 48/63 [02:58<00:55,  3.71s/it] 78%|███████▊  | 49/63 [03:02<00:51,  3.71s/it] 79%|███████▉  | 50/63 [03:06<00:48,  3.72s/it]                                               {'loss': 0.271, 'grad_norm': 0.7402876019477844, 'learning_rate': 1.2717891755841722e-05, 'epoch': 2.31}
 79%|███████▉  | 50/63 [03:06<00:48,  3.72s/it] 81%|████████  | 51/63 [03:09<00:44,  3.72s/it] 83%|████████▎ | 52/63 [03:13<00:40,  3.71s/it] 84%|████████▍ | 53/63 [03:17<00:37,  3.71s/it] 86%|████████▌ | 54/63 [03:21<00:33,  3.71s/it] 87%|████████▋ | 55/63 [03:24<00:29,  3.70s/it] 89%|████████▉ | 56/63 [03:28<00:25,  3.71s/it] 90%|█████████ | 57/63 [03:32<00:22,  3.71s/it] 92%|█████████▏| 58/63 [03:35<00:18,  3.71s/it] 94%|█████████▎| 59/63 [03:39<00:14,  3.72s/it] 95%|█████████▌| 60/63 [03:43<00:11,  3.71s/it]                                               {'loss': 0.2459, 'grad_norm': 0.5384693741798401, 'learning_rate': 7.064490740882057e-07, 'epoch': 2.77}
 95%|█████████▌| 60/63 [03:43<00:11,  3.71s/it] 97%|█████████▋| 61/63 [03:47<00:07,  3.71s/it] 98%|█████████▊| 62/63 [03:50<00:03,  3.71s/it]100%|██████████| 63/63 [03:54<00:00,  3.71s/it][INFO|trainer.py:3705] 2024-11-18 22:04:54,533 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/checkpoint-63
[INFO|configuration_utils.py:673] 2024-11-18 22:04:54,570 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:04:54,571 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:04:54,733 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:04:54,734 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/checkpoint-63/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:04:55,437 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 236.1329, 'train_samples_per_second': 4.383, 'train_steps_per_second': 0.267, 'train_loss': 0.569338477793194, 'epoch': 2.91}
100%|██████████| 63/63 [03:55<00:00,  3.71s/it]100%|██████████| 63/63 [03:55<00:00,  3.74s/it]
[INFO|trainer.py:3705] 2024-11-18 22:04:55,456 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:04:55,488 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:04:55,489 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:04:55,647 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:04:55,648 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9133
  total_flos               = 39757039GF
  train_loss               =     0.5693
  train_runtime            = 0:03:56.13
  train_samples_per_second =      4.383
  train_steps_per_second   =      0.267
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_3_7/lora/sft/training_loss.png
11/18/2024 22:04:56 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:04:56 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:04:56,102 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:04:56,102 >>   Num examples = 39
[INFO|trainer.py:4026] 2024-11-18 22:04:56,102 >>   Batch size = 1
  0%|          | 0/20 [00:00<?, ?it/s] 10%|█         | 2/20 [00:00<00:01, 15.13it/s] 20%|██        | 4/20 [00:00<00:01,  9.82it/s] 30%|███       | 6/20 [00:00<00:01,  8.86it/s] 35%|███▌      | 7/20 [00:00<00:01,  8.66it/s] 40%|████      | 8/20 [00:00<00:01,  8.53it/s] 45%|████▌     | 9/20 [00:01<00:01,  8.41it/s] 50%|█████     | 10/20 [00:01<00:01,  8.23it/s] 55%|█████▌    | 11/20 [00:01<00:01,  8.16it/s] 60%|██████    | 12/20 [00:01<00:00,  8.01it/s] 65%|██████▌   | 13/20 [00:01<00:00,  7.95it/s] 70%|███████   | 14/20 [00:01<00:00,  8.00it/s] 75%|███████▌  | 15/20 [00:01<00:00,  8.05it/s] 80%|████████  | 16/20 [00:01<00:00,  8.07it/s] 85%|████████▌ | 17/20 [00:02<00:00,  8.05it/s] 90%|█████████ | 18/20 [00:02<00:00,  8.02it/s] 95%|█████████▌| 19/20 [00:02<00:00,  8.04it/s]100%|██████████| 20/20 [00:02<00:00,  8.14it/s]100%|██████████| 20/20 [00:02<00:00,  8.39it/s]
***** eval metrics *****
  epoch                   =     2.9133
  eval_loss               =     0.2665
  eval_runtime            = 0:00:02.52
  eval_samples_per_second =      15.46
  eval_steps_per_second   =      7.928
[INFO|modelcard.py:449] 2024-11-18 22:04:58,626 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:05:16,680] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:05:20 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:28188
[2024-11-18 22:05:22,649] torch.distributed.run: [WARNING] 
[2024-11-18 22:05:22,649] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:05:22,649] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:05:22,649] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:05:29,946] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:05:30,235] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:05:31 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:05:31 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:05:31 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:05:31 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:05:31,506 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:05:31,508 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,509 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:05:31,965 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:05:31,966 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:05:31,967 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:05:31,968 >> loading file tokenizer_config.json
11/18/2024 22:05:32 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:05:32 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:05:32,423 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:05:32 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:05:32 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:05:32 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_4_6.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 512 examples [00:00, 10606.01 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/512 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 32/512 [00:00<00:01, 258.13 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 512/512 [00:00<00:00, 1724.83 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:05:36 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_4_6.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/512 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 32/512 [00:01<00:17, 27.69 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 64/512 [00:01<00:08, 52.42 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 96/512 [00:01<00:05, 74.12 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 128/512 [00:01<00:04, 91.94 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 160/512 [00:02<00:03, 109.97 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 192/512 [00:02<00:03, 91.85 examples/s] Running tokenizer on dataset (num_proc=16):  50%|█████     | 256/512 [00:02<00:01, 135.21 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 288/512 [00:02<00:01, 140.52 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 320/512 [00:03<00:01, 146.64 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 352/512 [00:03<00:01, 118.69 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 416/512 [00:03<00:00, 153.29 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 448/512 [00:03<00:00, 155.21 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 480/512 [00:04<00:00, 149.11 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 512/512 [00:04<00:00, 154.09 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 512/512 [00:04<00:00, 113.60 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 79374, 1604, 3891, 84728, 304, 78985, 3376, 19150, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 2808, 369, 12375, 36578, 36871, 304, 647, 632, 1279, 74, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 5770, 977, 70645, 823, 276, 2763, 1047, 4246, 129451, 62, 30172, 1243, 2763, 1561, 672, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 54, 27687, 97248, 62, 30172, 3575, 1139, 68, 2125, 2449, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 37575, 1245, 865, 570, 52222, 62, 30172, 1243, 1088, 9510, 3376, 19150, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 40586, 2334, 578, 74, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 54, 44871, 1047, 97248, 61917, 2334, 15428, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 3036, 81, 45794, 2763, 1630, 3235, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 101419, 17714, 54, 21265, 24161, 331, 100623, 9370, 101395, 104057, 6347, 320, 77, 16, 25, 8987, 8, 220, 1380, 308, 16, 72056, 1131, 54, 21265, 24161, 331, 6, 470, 308, 16, 72637, 11, 308, 16, 72056, 11, 308, 16, 48337, 11, 308, 16, 97506, 1028, 11, 308, 16, 38611, 2591, 11, 308, 16, 948, 19951, 11, 308, 16, 9847, 11, 308, 16, 1764, 11, 308, 16, 8219, 573, 11, 308, 16, 31633, 3930, 220, 16, 22, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Group for Friedrich_Nietzsche in Poznań'的节点
label为forum,属性title='Group for William_Shakespeare in Vitebsk'的节点
label为organisation,属性name='Cardinal_Stefan_Wyszyński_University_in_Warsaw'的节点
label为organisation,属性name='Wrocław_University_of_Technology'的节点
label为organisation,属性name='Adam_Mickiewicz_University_in_Poznań'的节点
label为tag,属性name='Daniel_Vacek'的节点
label为tag,属性name='Władysław_IV_Vasa'的节点
label为tag,属性name='Andrzej_Wajda'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找名字为Wojciech的人的姓氏match (n1:person)  where n1.firstname='Wojciech' return n1.lastname, n1.firstname, n1.gender, n1.creationdate, n1.browserused, n1.birthday, n1.email, n1.id, n1.locationip, n1.language limit 17;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 8987, 8, 220, 1380, 308, 16, 72056, 1131, 54, 21265, 24161, 331, 6, 470, 308, 16, 72637, 11, 308, 16, 72056, 11, 308, 16, 48337, 11, 308, 16, 97506, 1028, 11, 308, 16, 38611, 2591, 11, 308, 16, 948, 19951, 11, 308, 16, 9847, 11, 308, 16, 1764, 11, 308, 16, 8219, 573, 11, 308, 16, 31633, 3930, 220, 16, 22, 26, 151645]
labels:
match (n1:person)  where n1.firstname='Wojciech' return n1.lastname, n1.firstname, n1.gender, n1.creationdate, n1.browserused, n1.birthday, n1.email, n1.id, n1.locationip, n1.language limit 17;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:05:41,743 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:05:41,745 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:05:41,796 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:05:41,796 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:05:41,798 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:05:45,702 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:05:45,702 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:05:45,706 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:05:45,706 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:05:45 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:05:45 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:05:45 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:05:45 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:05:45 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,down_proj,k_proj,v_proj,up_proj,q_proj,o_proj
11/18/2024 22:05:46 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:05:46,434 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
11/18/2024 22:05:46 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:05:46 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:05:46 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:05:46 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:05:46 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,q_proj,k_proj,down_proj,o_proj,v_proj
11/18/2024 22:05:47 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:05:48,082 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:05:48,082 >>   Num examples = 460
[INFO|trainer.py:2245] 2024-11-18 22:05:48,082 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:05:48,082 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:05:48,082 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:05:48,083 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:05:48,083 >>   Total optimization steps = 84
[INFO|trainer.py:2252] 2024-11-18 22:05:48,089 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/84 [00:00<?, ?it/s]  1%|          | 1/84 [00:04<05:46,  4.17s/it]  2%|▏         | 2/84 [00:07<05:21,  3.93s/it]  4%|▎         | 3/84 [00:11<05:10,  3.83s/it]  5%|▍         | 4/84 [00:15<05:01,  3.77s/it]  6%|▌         | 5/84 [00:19<04:56,  3.76s/it]  7%|▋         | 6/84 [00:22<04:51,  3.74s/it]  8%|▊         | 7/84 [00:26<04:46,  3.73s/it] 10%|▉         | 8/84 [00:30<04:42,  3.72s/it] 11%|█         | 9/84 [00:33<04:38,  3.72s/it] 12%|█▏        | 10/84 [00:37<04:34,  3.70s/it]                                               {'loss': 1.6497, 'grad_norm': 1.4675709009170532, 'learning_rate': 9.995614150494293e-05, 'epoch': 0.35}
 12%|█▏        | 10/84 [00:37<04:34,  3.70s/it] 13%|█▎        | 11/84 [00:41<04:31,  3.72s/it] 14%|█▍        | 12/84 [00:44<04:27,  3.71s/it] 15%|█▌        | 13/84 [00:48<04:22,  3.70s/it] 17%|█▋        | 14/84 [00:52<04:19,  3.70s/it] 18%|█▊        | 15/84 [00:56<04:14,  3.69s/it] 19%|█▉        | 16/84 [00:59<04:11,  3.70s/it] 20%|██        | 17/84 [01:03<04:07,  3.70s/it] 21%|██▏       | 18/84 [01:07<04:03,  3.69s/it] 23%|██▎       | 19/84 [01:10<04:00,  3.70s/it] 24%|██▍       | 20/84 [01:14<03:56,  3.69s/it]                                               {'loss': 0.653, 'grad_norm': 0.8870422840118408, 'learning_rate': 9.478558801197065e-05, 'epoch': 0.7}
 24%|██▍       | 20/84 [01:14<03:56,  3.69s/it] 25%|██▌       | 21/84 [01:18<03:52,  3.69s/it] 26%|██▌       | 22/84 [01:21<03:48,  3.68s/it] 27%|██▋       | 23/84 [01:25<03:45,  3.69s/it] 29%|██▊       | 24/84 [01:29<03:42,  3.70s/it] 30%|██▉       | 25/84 [01:33<03:38,  3.70s/it] 31%|███       | 26/84 [01:36<03:34,  3.69s/it] 32%|███▏      | 27/84 [01:40<03:30,  3.69s/it] 33%|███▎      | 28/84 [01:44<03:26,  3.68s/it] 35%|███▍      | 29/84 [01:47<03:22,  3.69s/it] 36%|███▌      | 30/84 [01:51<03:19,  3.69s/it]                                               {'loss': 0.4019, 'grad_norm': 0.7514861226081848, 'learning_rate': 8.18711994874345e-05, 'epoch': 1.04}
 36%|███▌      | 30/84 [01:51<03:19,  3.69s/it] 37%|███▋      | 31/84 [01:55<03:15,  3.69s/it] 38%|███▊      | 32/84 [01:58<03:11,  3.68s/it] 39%|███▉      | 33/84 [02:02<03:09,  3.71s/it] 40%|████      | 34/84 [02:06<03:05,  3.71s/it] 42%|████▏     | 35/84 [02:09<03:01,  3.70s/it] 43%|████▎     | 36/84 [02:13<02:57,  3.70s/it] 44%|████▍     | 37/84 [02:17<02:53,  3.69s/it] 45%|████▌     | 38/84 [02:21<02:50,  3.70s/it] 46%|████▋     | 39/84 [02:24<02:46,  3.69s/it] 48%|████▊     | 40/84 [02:28<02:42,  3.69s/it]                                               {'loss': 0.2587, 'grad_norm': 0.9622588157653809, 'learning_rate': 6.344599103076329e-05, 'epoch': 1.39}
 48%|████▊     | 40/84 [02:28<02:42,  3.69s/it] 49%|████▉     | 41/84 [02:32<02:38,  3.69s/it] 50%|█████     | 42/84 [02:35<02:35,  3.69s/it] 51%|█████     | 43/84 [02:39<02:30,  3.68s/it] 52%|█████▏    | 44/84 [02:43<02:27,  3.68s/it] 54%|█████▎    | 45/84 [02:46<02:23,  3.69s/it] 55%|█████▍    | 46/84 [02:50<02:19,  3.68s/it] 56%|█████▌    | 47/84 [02:54<02:16,  3.68s/it] 57%|█████▋    | 48/84 [02:57<02:12,  3.69s/it] 58%|█████▊    | 49/84 [03:01<02:09,  3.69s/it] 60%|█████▉    | 50/84 [03:05<02:05,  3.69s/it]                                               {'loss': 0.2507, 'grad_norm': 0.6433479189872742, 'learning_rate': 4.269584857187943e-05, 'epoch': 1.74}
 60%|█████▉    | 50/84 [03:05<02:05,  3.69s/it] 61%|██████    | 51/84 [03:08<02:02,  3.70s/it] 62%|██████▏   | 52/84 [03:12<01:59,  3.74s/it] 63%|██████▎   | 53/84 [03:16<01:55,  3.74s/it] 64%|██████▍   | 54/84 [03:20<01:51,  3.73s/it] 65%|██████▌   | 55/84 [03:23<01:47,  3.72s/it] 67%|██████▋   | 56/84 [03:27<01:43,  3.71s/it] 68%|██████▊   | 57/84 [03:31<01:39,  3.70s/it] 69%|██████▉   | 58/84 [03:35<01:36,  3.71s/it] 70%|███████   | 59/84 [03:38<01:32,  3.70s/it] 71%|███████▏  | 60/84 [03:42<01:28,  3.70s/it]                                               {'loss': 0.2542, 'grad_norm': 0.566170871257782, 'learning_rate': 2.3208660251050158e-05, 'epoch': 2.09}
 71%|███████▏  | 60/84 [03:42<01:28,  3.70s/it] 73%|███████▎  | 61/84 [03:46<01:24,  3.69s/it] 74%|███████▍  | 62/84 [03:49<01:21,  3.70s/it] 75%|███████▌  | 63/84 [03:53<01:17,  3.70s/it] 76%|███████▌  | 64/84 [03:57<01:14,  3.71s/it] 77%|███████▋  | 65/84 [04:00<01:10,  3.71s/it] 79%|███████▊  | 66/84 [04:04<01:06,  3.70s/it] 80%|███████▉  | 67/84 [04:08<01:02,  3.70s/it] 81%|████████  | 68/84 [04:11<00:59,  3.69s/it] 82%|████████▏ | 69/84 [04:15<00:55,  3.69s/it] 83%|████████▎ | 70/84 [04:19<00:51,  3.70s/it]                                               {'loss': 0.2215, 'grad_norm': 0.5483571290969849, 'learning_rate': 8.353937964495029e-06, 'epoch': 2.43}
 83%|████████▎ | 70/84 [04:19<00:51,  3.70s/it] 85%|████████▍ | 71/84 [04:23<00:48,  3.70s/it] 86%|████████▌ | 72/84 [04:26<00:44,  3.71s/it] 87%|████████▋ | 73/84 [04:30<00:40,  3.69s/it] 88%|████████▊ | 74/84 [04:34<00:36,  3.69s/it] 89%|████████▉ | 75/84 [04:37<00:33,  3.69s/it] 90%|█████████ | 76/84 [04:41<00:29,  3.68s/it] 92%|█████████▏| 77/84 [04:45<00:25,  3.68s/it] 93%|█████████▎| 78/84 [04:48<00:22,  3.69s/it] 94%|█████████▍| 79/84 [04:52<00:18,  3.69s/it] 95%|█████████▌| 80/84 [04:56<00:14,  3.70s/it]                                               {'loss': 0.1915, 'grad_norm': 0.5854292511940002, 'learning_rate': 7.001981464747565e-07, 'epoch': 2.78}
 95%|█████████▌| 80/84 [04:56<00:14,  3.70s/it] 96%|█████████▋| 81/84 [04:59<00:11,  3.69s/it] 98%|█████████▊| 82/84 [05:03<00:07,  3.69s/it] 99%|█████████▉| 83/84 [05:07<00:03,  3.69s/it]100%|██████████| 84/84 [05:11<00:00,  3.70s/it][INFO|trainer.py:3705] 2024-11-18 22:10:59,894 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/checkpoint-84
[INFO|configuration_utils.py:673] 2024-11-18 22:10:59,931 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:10:59,933 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:11:00,097 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/checkpoint-84/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:11:00,097 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/checkpoint-84/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:11:00,816 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 312.7269, 'train_samples_per_second': 4.413, 'train_steps_per_second': 0.269, 'train_loss': 0.47179029243333, 'epoch': 2.92}
100%|██████████| 84/84 [05:12<00:00,  3.70s/it]100%|██████████| 84/84 [05:12<00:00,  3.71s/it]
[INFO|trainer.py:3705] 2024-11-18 22:11:00,834 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:11:00,866 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:11:00,867 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:11:01,024 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:11:01,024 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9217
  total_flos               = 53120436GF
  train_loss               =     0.4718
  train_runtime            = 0:05:12.72
  train_samples_per_second =      4.413
  train_steps_per_second   =      0.269
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_4_6/lora/sft/training_loss.png
11/18/2024 22:11:01 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:11:01 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:11:01,501 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:11:01,501 >>   Num examples = 52
[INFO|trainer.py:4026] 2024-11-18 22:11:01,501 >>   Batch size = 1
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:01, 15.23it/s] 15%|█▌        | 4/26 [00:00<00:02,  9.88it/s] 23%|██▎       | 6/26 [00:00<00:02,  8.90it/s] 27%|██▋       | 7/26 [00:00<00:02,  8.71it/s] 31%|███       | 8/26 [00:00<00:02,  8.46it/s] 35%|███▍      | 9/26 [00:01<00:02,  8.37it/s] 38%|███▊      | 10/26 [00:01<00:01,  8.31it/s] 42%|████▏     | 11/26 [00:01<00:01,  8.23it/s] 46%|████▌     | 12/26 [00:01<00:01,  8.20it/s] 50%|█████     | 13/26 [00:01<00:01,  8.20it/s] 54%|█████▍    | 14/26 [00:01<00:01,  8.18it/s] 58%|█████▊    | 15/26 [00:01<00:01,  8.11it/s] 62%|██████▏   | 16/26 [00:01<00:01,  8.11it/s] 65%|██████▌   | 17/26 [00:01<00:01,  8.02it/s] 69%|██████▉   | 18/26 [00:02<00:00,  8.07it/s] 73%|███████▎  | 19/26 [00:02<00:00,  8.05it/s] 77%|███████▋  | 20/26 [00:02<00:00,  8.03it/s] 81%|████████  | 21/26 [00:02<00:00,  7.96it/s] 85%|████████▍ | 22/26 [00:02<00:00,  7.93it/s] 88%|████████▊ | 23/26 [00:02<00:00,  7.89it/s] 92%|█████████▏| 24/26 [00:02<00:00,  7.88it/s] 96%|█████████▌| 25/26 [00:03<00:00,  7.97it/s]100%|██████████| 26/26 [00:03<00:00,  8.03it/s]100%|██████████| 26/26 [00:03<00:00,  8.31it/s]
***** eval metrics *****
  epoch                   =     2.9217
  eval_loss               =     0.1813
  eval_runtime            = 0:00:03.27
  eval_samples_per_second =     15.874
  eval_steps_per_second   =      7.937
[INFO|modelcard.py:449] 2024-11-18 22:11:04,777 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:11:25,141] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:11:28 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20302
[2024-11-18 22:11:31,098] torch.distributed.run: [WARNING] 
[2024-11-18 22:11:31,098] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:11:31,098] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:11:31,098] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:11:38,353] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:11:38,602] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:11:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:11:39 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:11:39,457 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:11:39,459 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,460 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,460 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,461 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,461 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,461 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,461 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:11:39,912 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:11:39,912 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:11:39,914 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,914 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,914 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,914 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,915 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,915 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:11:39,915 >> loading file tokenizer_config.json
11/18/2024 22:11:39 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:11:39 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:11:40,371 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:11:40 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:11:40 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:11:40 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_5_5.json...
11/18/2024 22:11:40 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:11:40 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 640 examples [00:00, 11571.79 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/640 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 40/640 [00:00<00:01, 300.63 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 640/640 [00:00<00:00, 2115.60 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:11:44 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_5_5.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/640 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 40/640 [00:01<00:20, 28.79 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 80/640 [00:01<00:09, 59.13 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 160/640 [00:01<00:03, 131.10 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 200/640 [00:01<00:03, 141.49 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 280/640 [00:02<00:01, 189.83 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 320/640 [00:02<00:01, 191.57 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 360/640 [00:02<00:01, 191.65 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 440/640 [00:03<00:01, 177.21 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 480/640 [00:03<00:00, 186.43 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 520/640 [00:03<00:00, 188.40 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 600/640 [00:03<00:00, 244.44 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 640/640 [00:03<00:00, 215.93 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 640/640 [00:04<00:00, 156.60 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 57804, 11, 79256, 606, 1131, 50, 37712, 1557, 75516, 1245, 336, 21913, 1245, 291, 938, 920, 337, 4757, 8378, 2039, 6298, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 34882, 402, 1466, 3362, 69213, 14980, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 13218, 351, 6743, 2351, 2970, 62, 3558, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 28082, 51154, 510, 109547, 67071, 2016, 86, 1915, 50377, 9370, 85641, 104597, 5373, 110821, 37029, 99559, 5373, 50377, 45785, 5373, 81812, 3298, 5373, 98402, 101034, 2016, 86, 1915, 109824, 5373, 915, 5373, 105511, 5373, 116617, 5373, 101395, 104057, 5373, 50377, 45785, 5373, 81812, 3298, 5373, 109391, 33108, 101419, 3837, 91572, 31526, 100145, 9370, 50377, 45785, 1773, 151645, 198, 151644, 77091, 198, 6347, 320, 77, 16, 25, 6182, 7287, 58, 81, 16, 25, 4648, 32398, 6182, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 2016, 86, 1915, 6, 470, 308, 17, 31633, 11, 308, 16, 5406, 11, 308, 17, 1764, 11, 308, 16, 38611, 2591, 11, 308, 17, 948, 19951, 11, 308, 17, 9847, 11, 308, 17, 72637, 11, 308, 17, 97506, 1028, 11, 308, 16, 97506, 1028, 11, 308, 17, 8219, 573, 11, 308, 16, 1764, 11, 308, 16, 8219, 573, 11, 308, 17, 48337, 11, 308, 17, 72056, 11, 435, 16, 97506, 1028, 11, 308, 17, 38611, 2591, 11, 308, 16, 1954, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为organisation,属性name='Sagar_Dutta_Memorial_Medical_College_and_Hospital'的节点
label为tag,属性name='Satavahana_dynasty'的节点
label为tag,属性name='Swagga_Like_Us'的节点

请将以下自然语言翻译为对该数据库的Cypher查询:
查找由Shweta创建的评论的内容、浏览器使用情况、创建日期、位置IP、长度以及Shweta的语言、ID、生日、电子邮件、姓氏、创建日期、位置IP、性别和名字，同时返回关系的创建日期。<|im_end|>
<|im_start|>assistant
match (n1:comment)-[r1:hascreatorcomment]->(n2:person)  where n2.firstname='Shweta' return n2.language, n1.content, n2.id, n1.browserused, n2.birthday, n2.email, n2.lastname, n2.creationdate, n1.creationdate, n2.locationip, n1.id, n1.locationip, n2.gender, n2.firstname, r1.creationdate, n2.browserused, n1.length;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 6182, 7287, 58, 81, 16, 25, 4648, 32398, 6182, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 2016, 86, 1915, 6, 470, 308, 17, 31633, 11, 308, 16, 5406, 11, 308, 17, 1764, 11, 308, 16, 38611, 2591, 11, 308, 17, 948, 19951, 11, 308, 17, 9847, 11, 308, 17, 72637, 11, 308, 17, 97506, 1028, 11, 308, 16, 97506, 1028, 11, 308, 17, 8219, 573, 11, 308, 16, 1764, 11, 308, 16, 8219, 573, 11, 308, 17, 48337, 11, 308, 17, 72056, 11, 435, 16, 97506, 1028, 11, 308, 17, 38611, 2591, 11, 308, 16, 1954, 26, 151645]
labels:
match (n1:comment)-[r1:hascreatorcomment]->(n2:person)  where n2.firstname='Shweta' return n2.language, n1.content, n2.id, n1.browserused, n2.birthday, n2.email, n2.lastname, n2.creationdate, n1.creationdate, n2.locationip, n1.id, n1.locationip, n2.gender, n2.firstname, r1.creationdate, n2.browserused, n1.length;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:11:49,650 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:11:49,651 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:11:49,702 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:11:49,703 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:11:49,705 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:11:53,273 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:11:53,273 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:11:53,277 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:11:53,278 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:11:53 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:11:53 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:11:53 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:11:53 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:11:53 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,gate_proj,o_proj,v_proj,up_proj,down_proj,q_proj
11/18/2024 22:11:53 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:11:54,018 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
11/18/2024 22:11:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:11:54 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:11:54 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:11:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:11:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,o_proj,down_proj,q_proj,gate_proj,k_proj,v_proj
11/18/2024 22:11:54 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:11:55,324 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:11:55,324 >>   Num examples = 576
[INFO|trainer.py:2245] 2024-11-18 22:11:55,324 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:11:55,324 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:11:55,324 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:11:55,324 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:11:55,324 >>   Total optimization steps = 108
[INFO|trainer.py:2252] 2024-11-18 22:11:55,331 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/108 [00:00<?, ?it/s]  1%|          | 1/108 [00:04<07:23,  4.14s/it]  2%|▏         | 2/108 [00:07<06:51,  3.88s/it]  3%|▎         | 3/108 [00:11<06:40,  3.81s/it]  4%|▎         | 4/108 [00:15<06:32,  3.77s/it]  5%|▍         | 5/108 [00:18<06:25,  3.74s/it]  6%|▌         | 6/108 [00:22<06:20,  3.73s/it]  6%|▋         | 7/108 [00:26<06:15,  3.72s/it]  7%|▋         | 8/108 [00:30<06:11,  3.71s/it]  8%|▊         | 9/108 [00:33<06:08,  3.72s/it]  9%|▉         | 10/108 [00:37<06:04,  3.72s/it]                                                {'loss': 1.6949, 'grad_norm': 1.609142780303955, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.28}
  9%|▉         | 10/108 [00:37<06:04,  3.72s/it] 10%|█         | 11/108 [00:41<06:00,  3.72s/it] 11%|█         | 12/108 [00:44<05:56,  3.71s/it] 12%|█▏        | 13/108 [00:48<05:52,  3.71s/it] 13%|█▎        | 14/108 [00:52<05:48,  3.71s/it] 14%|█▍        | 15/108 [00:56<05:45,  3.71s/it] 15%|█▍        | 16/108 [00:59<05:41,  3.71s/it] 16%|█▌        | 17/108 [01:03<05:37,  3.71s/it] 17%|█▋        | 18/108 [01:07<05:33,  3.70s/it] 18%|█▊        | 19/108 [01:10<05:29,  3.70s/it] 19%|█▊        | 20/108 [01:14<05:26,  3.71s/it]                                                {'loss': 0.7711, 'grad_norm': 0.9964374899864197, 'learning_rate': 9.789086620939936e-05, 'epoch': 0.56}
 19%|█▊        | 20/108 [01:14<05:26,  3.71s/it] 19%|█▉        | 21/108 [01:18<05:23,  3.71s/it] 20%|██        | 22/108 [01:22<05:18,  3.71s/it] 21%|██▏       | 23/108 [01:25<05:15,  3.71s/it] 22%|██▏       | 24/108 [01:29<05:11,  3.71s/it] 23%|██▎       | 25/108 [01:33<05:07,  3.71s/it] 24%|██▍       | 26/108 [01:36<05:03,  3.71s/it] 25%|██▌       | 27/108 [01:40<04:59,  3.70s/it] 26%|██▌       | 28/108 [01:44<04:56,  3.70s/it] 27%|██▋       | 29/108 [01:47<04:52,  3.70s/it] 28%|██▊       | 30/108 [01:51<04:48,  3.69s/it]                                                {'loss': 0.416, 'grad_norm': 0.7052762508392334, 'learning_rate': 9.082818315286055e-05, 'epoch': 0.83}
 28%|██▊       | 30/108 [01:51<04:48,  3.69s/it] 29%|██▊       | 31/108 [01:55<04:44,  3.69s/it] 30%|██▉       | 32/108 [01:58<04:40,  3.69s/it] 31%|███       | 33/108 [02:02<04:37,  3.69s/it] 31%|███▏      | 34/108 [02:06<04:33,  3.70s/it] 32%|███▏      | 35/108 [02:10<04:30,  3.71s/it] 33%|███▎      | 36/108 [02:13<04:26,  3.71s/it] 34%|███▍      | 37/108 [02:17<04:23,  3.71s/it] 35%|███▌      | 38/108 [02:21<04:19,  3.70s/it] 36%|███▌      | 39/108 [02:24<04:15,  3.70s/it] 37%|███▋      | 40/108 [02:28<04:11,  3.70s/it]                                                {'loss': 0.2296, 'grad_norm': 0.5591787695884705, 'learning_rate': 7.952011865029614e-05, 'epoch': 1.11}
 37%|███▋      | 40/108 [02:28<04:11,  3.70s/it] 38%|███▊      | 41/108 [02:32<04:11,  3.75s/it] 39%|███▉      | 42/108 [02:36<04:09,  3.79s/it] 40%|███▉      | 43/108 [02:40<04:04,  3.76s/it] 41%|████      | 44/108 [02:43<03:59,  3.74s/it] 42%|████▏     | 45/108 [02:47<03:55,  3.74s/it] 43%|████▎     | 46/108 [02:51<03:50,  3.72s/it] 44%|████▎     | 47/108 [02:54<03:46,  3.72s/it] 44%|████▍     | 48/108 [02:58<03:43,  3.72s/it] 45%|████▌     | 49/108 [03:02<03:40,  3.74s/it] 46%|████▋     | 50/108 [03:06<03:37,  3.74s/it]                                                {'loss': 0.2295, 'grad_norm': 0.6939743757247925, 'learning_rate': 6.514250379489753e-05, 'epoch': 1.39}
 46%|████▋     | 50/108 [03:06<03:37,  3.74s/it] 47%|████▋     | 51/108 [03:09<03:32,  3.74s/it] 48%|████▊     | 52/108 [03:13<03:28,  3.73s/it] 49%|████▉     | 53/108 [03:17<03:24,  3.71s/it] 50%|█████     | 54/108 [03:20<03:20,  3.71s/it] 51%|█████     | 55/108 [03:24<03:17,  3.72s/it] 52%|█████▏    | 56/108 [03:28<03:13,  3.71s/it] 53%|█████▎    | 57/108 [03:32<03:09,  3.71s/it] 54%|█████▎    | 58/108 [03:35<03:06,  3.72s/it] 55%|█████▍    | 59/108 [03:39<03:02,  3.72s/it] 56%|█████▌    | 60/108 [03:43<02:58,  3.71s/it]                                                {'loss': 0.2537, 'grad_norm': 0.7441008687019348, 'learning_rate': 4.919034655987493e-05, 'epoch': 1.67}
 56%|█████▌    | 60/108 [03:43<02:58,  3.71s/it] 56%|█████▋    | 61/108 [03:46<02:53,  3.70s/it] 57%|█████▋    | 62/108 [03:50<02:50,  3.70s/it] 58%|█████▊    | 63/108 [03:54<02:46,  3.70s/it] 59%|█████▉    | 64/108 [03:58<02:42,  3.70s/it] 60%|██████    | 65/108 [04:01<02:39,  3.70s/it] 61%|██████    | 66/108 [04:05<02:35,  3.70s/it] 62%|██████▏   | 67/108 [04:09<02:32,  3.71s/it] 63%|██████▎   | 68/108 [04:12<02:28,  3.71s/it] 64%|██████▍   | 69/108 [04:16<02:24,  3.71s/it] 65%|██████▍   | 70/108 [04:20<02:20,  3.70s/it]                                                {'loss': 0.2328, 'grad_norm': 0.5420487523078918, 'learning_rate': 3.332237841745898e-05, 'epoch': 1.94}
 65%|██████▍   | 70/108 [04:20<02:20,  3.70s/it] 66%|██████▌   | 71/108 [04:23<02:16,  3.70s/it] 67%|██████▋   | 72/108 [04:27<02:13,  3.70s/it] 68%|██████▊   | 73/108 [04:31<02:09,  3.70s/it] 69%|██████▊   | 74/108 [04:35<02:06,  3.71s/it] 69%|██████▉   | 75/108 [04:38<02:02,  3.71s/it] 70%|███████   | 76/108 [04:42<01:58,  3.70s/it] 71%|███████▏  | 77/108 [04:46<01:54,  3.70s/it] 72%|███████▏  | 78/108 [04:49<01:50,  3.70s/it] 73%|███████▎  | 79/108 [04:53<01:47,  3.70s/it] 74%|███████▍  | 80/108 [04:57<01:43,  3.71s/it]                                                {'loss': 0.2299, 'grad_norm': 0.5207471251487732, 'learning_rate': 1.9188576719953633e-05, 'epoch': 2.22}
 74%|███████▍  | 80/108 [04:57<01:43,  3.71s/it] 75%|███████▌  | 81/108 [05:01<01:40,  3.70s/it] 76%|███████▌  | 82/108 [05:04<01:36,  3.71s/it] 77%|███████▋  | 83/108 [05:08<01:32,  3.71s/it] 78%|███████▊  | 84/108 [05:12<01:28,  3.71s/it] 79%|███████▊  | 85/108 [05:15<01:25,  3.70s/it] 80%|███████▉  | 86/108 [05:19<01:21,  3.71s/it] 81%|████████  | 87/108 [05:23<01:17,  3.71s/it] 81%|████████▏ | 88/108 [05:26<01:14,  3.70s/it] 82%|████████▏ | 89/108 [05:30<01:10,  3.70s/it] 83%|████████▎ | 90/108 [05:34<01:06,  3.70s/it]                                                {'loss': 0.1827, 'grad_norm': 0.5495545864105225, 'learning_rate': 8.25859734853645e-06, 'epoch': 2.5}
 83%|████████▎ | 90/108 [05:34<01:06,  3.70s/it] 84%|████████▍ | 91/108 [05:38<01:02,  3.69s/it] 85%|████████▌ | 92/108 [05:41<00:59,  3.69s/it] 86%|████████▌ | 93/108 [05:45<00:55,  3.70s/it] 87%|████████▋ | 94/108 [05:49<00:51,  3.70s/it] 88%|████████▊ | 95/108 [05:52<00:48,  3.70s/it] 89%|████████▉ | 96/108 [05:56<00:44,  3.71s/it] 90%|████████▉ | 97/108 [06:00<00:40,  3.70s/it] 91%|█████████ | 98/108 [06:03<00:36,  3.70s/it] 92%|█████████▏| 99/108 [06:07<00:33,  3.70s/it] 93%|█████████▎| 100/108 [06:11<00:29,  3.70s/it]                                                 {'loss': 0.2055, 'grad_norm': 0.5036492347717285, 'learning_rate': 1.6689574843694433e-06, 'epoch': 2.78}
 93%|█████████▎| 100/108 [06:11<00:29,  3.70s/it] 94%|█████████▎| 101/108 [06:15<00:25,  3.71s/it] 94%|█████████▍| 102/108 [06:18<00:22,  3.70s/it] 95%|█████████▌| 103/108 [06:22<00:18,  3.69s/it] 96%|█████████▋| 104/108 [06:26<00:14,  3.69s/it] 97%|█████████▋| 105/108 [06:29<00:11,  3.71s/it] 98%|█████████▊| 106/108 [06:33<00:07,  3.71s/it] 99%|█████████▉| 107/108 [06:37<00:03,  3.71s/it]100%|██████████| 108/108 [06:41<00:00,  3.70s/it][INFO|trainer.py:3705] 2024-11-18 22:18:37,048 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/checkpoint-108
[INFO|configuration_utils.py:673] 2024-11-18 22:18:37,086 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:18:37,087 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:18:37,279 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/checkpoint-108/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:18:37,280 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/checkpoint-108/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:18:38,201 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 402.8703, 'train_samples_per_second': 4.289, 'train_steps_per_second': 0.268, 'train_loss': 0.42410006898420827, 'epoch': 3.0}
100%|██████████| 108/108 [06:42<00:00,  3.70s/it]100%|██████████| 108/108 [06:42<00:00,  3.72s/it]
[INFO|trainer.py:3705] 2024-11-18 22:18:38,219 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:18:38,252 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:18:38,253 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:18:38,421 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:18:38,421 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  total_flos               = 68206956GF
  train_loss               =     0.4241
  train_runtime            = 0:06:42.87
  train_samples_per_second =      4.289
  train_steps_per_second   =      0.268
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_5_5/lora/sft/training_loss.png
11/18/2024 22:18:38 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:18:38 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:18:38,891 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:18:38,891 >>   Num examples = 64
[INFO|trainer.py:4026] 2024-11-18 22:18:38,892 >>   Batch size = 1
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:02, 14.91it/s] 12%|█▎        | 4/32 [00:00<00:02,  9.51it/s] 19%|█▉        | 6/32 [00:00<00:02,  8.73it/s] 22%|██▏       | 7/32 [00:00<00:02,  8.57it/s] 25%|██▌       | 8/32 [00:00<00:02,  8.46it/s] 28%|██▊       | 9/32 [00:01<00:02,  8.28it/s] 31%|███▏      | 10/32 [00:01<00:02,  8.25it/s] 34%|███▍      | 11/32 [00:01<00:02,  8.12it/s] 38%|███▊      | 12/32 [00:01<00:02,  8.02it/s] 41%|████      | 13/32 [00:01<00:02,  8.07it/s] 44%|████▍     | 14/32 [00:01<00:02,  8.06it/s] 47%|████▋     | 15/32 [00:01<00:02,  7.95it/s] 50%|█████     | 16/32 [00:01<00:02,  7.99it/s] 53%|█████▎    | 17/32 [00:02<00:01,  8.01it/s] 56%|█████▋    | 18/32 [00:02<00:01,  7.95it/s] 59%|█████▉    | 19/32 [00:02<00:01,  8.02it/s] 62%|██████▎   | 20/32 [00:02<00:01,  8.07it/s] 66%|██████▌   | 21/32 [00:02<00:01,  8.10it/s] 69%|██████▉   | 22/32 [00:02<00:01,  8.01it/s] 72%|███████▏  | 23/32 [00:02<00:01,  7.99it/s] 75%|███████▌  | 24/32 [00:02<00:01,  7.94it/s] 78%|███████▊  | 25/32 [00:03<00:00,  7.90it/s] 81%|████████▏ | 26/32 [00:03<00:00,  7.88it/s] 84%|████████▍ | 27/32 [00:03<00:00,  7.93it/s] 88%|████████▊ | 28/32 [00:03<00:00,  8.00it/s] 91%|█████████ | 29/32 [00:03<00:00,  8.05it/s] 94%|█████████▍| 30/32 [00:03<00:00,  7.98it/s] 97%|█████████▋| 31/32 [00:03<00:00,  8.02it/s]100%|██████████| 32/32 [00:03<00:00,  8.12it/s]100%|██████████| 32/32 [00:03<00:00,  8.21it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     0.1862
  eval_runtime            = 0:00:04.03
  eval_samples_per_second =     15.844
  eval_steps_per_second   =      7.922
[INFO|modelcard.py:449] 2024-11-18 22:18:42,931 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:19:02,973] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:19:06 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29402
[2024-11-18 22:19:08,937] torch.distributed.run: [WARNING] 
[2024-11-18 22:19:08,937] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:19:08,937] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:19:08,937] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:19:16,092] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:19:16,341] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:19:17 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:19:17 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:19:17,177 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:19:17,178 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,180 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:19:17,635 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:19:17,636 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:19:17,637 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:19:17,638 >> loading file tokenizer_config.json
11/18/2024 22:19:17 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:19:17 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:19:18,099 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:19:18 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:19:18 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:19:18 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_6_4.json...
11/18/2024 22:19:18 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:19:18 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 768 examples [00:00, 12157.08 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/768 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 48/768 [00:00<00:01, 378.07 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 768/768 [00:00<00:00, 2541.66 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:19:22 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_6_4.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/768 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 48/768 [00:01<00:20, 35.10 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 96/768 [00:01<00:09, 71.96 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 144/768 [00:01<00:05, 108.89 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 192/768 [00:01<00:04, 144.00 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 240/768 [00:02<00:03, 167.13 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 336/768 [00:02<00:01, 228.99 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 384/768 [00:02<00:01, 217.01 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 432/768 [00:02<00:01, 207.33 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 528/768 [00:03<00:00, 258.39 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 624/768 [00:03<00:00, 281.44 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 672/768 [00:03<00:00, 292.25 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 720/768 [00:03<00:00, 272.27 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 768/768 [00:04<00:00, 253.54 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 768/768 [00:04<00:00, 187.79 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 18, 20, 315, 63990, 48435, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 18, 15, 315, 63990, 48435, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 17, 20, 315, 63990, 48435, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 1427, 309, 2039, 436, 48435, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 4121, 404, 258, 10598, 359, 2462, 920, 337, 4757, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 37, 585, 404, 1245, 56472, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 852, 3575, 2568, 1466, 55, 23544, 263, 53241, 82, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 39, 436, 7751, 1245, 392, 27885, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 39, 355, 48435, 3575, 10598, 13396, 6, 9370, 92374, 271, 14880, 44063, 87752, 99795, 102064, 105395, 17714, 109683, 74393, 9370, 56715, 6347, 320, 77, 16, 25, 22585, 7287, 58, 81, 16, 25, 4648, 2593, 39522, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 39, 436, 48435, 6, 323, 308, 16, 6067, 1131, 32378, 220, 18, 20, 315, 63990, 48435, 1752, 283, 12982, 6, 470, 308, 17, 8219, 573, 11, 308, 17, 72056, 11, 435, 16, 97506, 1028, 11, 308, 16, 1764, 3930, 220, 17, 15, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Album 35 of Hossein Forouhar'的节点
label为forum,属性title='Album 30 of Hossein Forouhar'的节点
label为forum,属性title='Album 25 of Hossein Forouhar'的节点
label为organisation,属性name='Imam_Hossein_University'的节点
label为organisation,属性name='Obirin_Junior_College'的节点
label为organisation,属性name='Fakir_Mohan_University'的节点
label为tag,属性name='List_of_RahXephon_albums'的节点
label为tag,属性name='Hosni_Mubarak'的节点
label为tag,属性name='Hussein_of_Jordan'的节点

请将以下自然语言翻译为对该数据库的Cymatch (n1:forum)-[r1:hasmoderator]->(n2:person)  where n2.firstname='Hossein' and n1.title='Album 35 of Hossein Forouhar' return n2.locationip, n2.firstname, r1.creationdate, n1.id limit 20;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 22585, 7287, 58, 81, 16, 25, 4648, 2593, 39522, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 39, 436, 48435, 6, 323, 308, 16, 6067, 1131, 32378, 220, 18, 20, 315, 63990, 48435, 1752, 283, 12982, 6, 470, 308, 17, 8219, 573, 11, 308, 17, 72056, 11, 435, 16, 97506, 1028, 11, 308, 16, 1764, 3930, 220, 17, 15, 26, 151645]
labels:
match (n1:forum)-[r1:hasmoderator]->(n2:person)  where n2.firstname='Hossein' and n1.title='Album 35 of Hossein Forouhar' return n2.locationip, n2.firstname, r1.creationdate, n1.id limit 20;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:19:27,344 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:19:27,345 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:19:27,396 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:19:27,396 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:19:27,398 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.10it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.07it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:19:31,190 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:19:31,190 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:19:31,194 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:19:31,195 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:19:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:19:31 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,down_proj,o_proj,v_proj,gate_proj,q_proj
11/18/2024 22:19:31 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:19:31,891 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:19:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:19:31 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:19:31 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,v_proj,up_proj,q_proj,k_proj,down_proj,gate_proj
11/18/2024 22:19:32 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:19:33,134 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:19:33,134 >>   Num examples = 691
[INFO|trainer.py:2245] 2024-11-18 22:19:33,134 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:19:33,134 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:19:33,134 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:19:33,135 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:19:33,135 >>   Total optimization steps = 129
[INFO|trainer.py:2252] 2024-11-18 22:19:33,141 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/129 [00:00<?, ?it/s]  1%|          | 1/129 [00:04<08:52,  4.16s/it]  2%|▏         | 2/129 [00:07<08:12,  3.88s/it]  2%|▏         | 3/129 [00:11<07:57,  3.79s/it]  3%|▎         | 4/129 [00:15<07:49,  3.76s/it]  4%|▍         | 5/129 [00:18<07:41,  3.72s/it]  5%|▍         | 6/129 [00:22<07:43,  3.77s/it]  5%|▌         | 7/129 [00:26<07:37,  3.75s/it]  6%|▌         | 8/129 [00:30<07:30,  3.72s/it]  7%|▋         | 9/129 [00:33<07:24,  3.71s/it]  8%|▊         | 10/129 [00:37<07:21,  3.71s/it]                                                {'loss': 1.7022, 'grad_norm': 2.30812931060791, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.23}
  8%|▊         | 10/129 [00:37<07:21,  3.71s/it]  9%|▊         | 11/129 [00:41<07:17,  3.70s/it]  9%|▉         | 12/129 [00:44<07:10,  3.68s/it] 10%|█         | 13/129 [00:48<07:06,  3.68s/it] 11%|█         | 14/129 [00:52<07:02,  3.68s/it] 12%|█▏        | 15/129 [00:55<06:59,  3.68s/it] 12%|█▏        | 16/129 [00:59<06:56,  3.69s/it] 13%|█▎        | 17/129 [01:03<06:53,  3.69s/it] 14%|█▍        | 18/129 [01:06<06:49,  3.69s/it] 15%|█▍        | 19/129 [01:10<06:46,  3.69s/it] 16%|█▌        | 20/129 [01:14<06:42,  3.69s/it]                                                {'loss': 0.8178, 'grad_norm': 0.7666638493537903, 'learning_rate': 9.91041841371078e-05, 'epoch': 0.46}
 16%|█▌        | 20/129 [01:14<06:42,  3.69s/it] 16%|█▋        | 21/129 [01:18<06:39,  3.70s/it] 17%|█▋        | 22/129 [01:21<06:36,  3.70s/it] 18%|█▊        | 23/129 [01:25<06:31,  3.70s/it] 19%|█▊        | 24/129 [01:29<06:27,  3.69s/it] 19%|█▉        | 25/129 [01:32<06:23,  3.69s/it] 20%|██        | 26/129 [01:36<06:18,  3.68s/it] 21%|██        | 27/129 [01:40<06:15,  3.68s/it] 22%|██▏       | 28/129 [01:43<06:11,  3.68s/it] 22%|██▏       | 29/129 [01:47<06:08,  3.68s/it] 23%|██▎       | 30/129 [01:51<06:04,  3.68s/it]                                                {'loss': 0.4593, 'grad_norm': 0.8268113136291504, 'learning_rate': 9.47936130379344e-05, 'epoch': 0.69}
 23%|██▎       | 30/129 [01:51<06:04,  3.68s/it] 24%|██▍       | 31/129 [01:54<06:01,  3.68s/it] 25%|██▍       | 32/129 [01:58<05:57,  3.68s/it] 26%|██▌       | 33/129 [02:02<05:57,  3.72s/it] 26%|██▋       | 34/129 [02:06<05:52,  3.71s/it] 27%|██▋       | 35/129 [02:09<05:47,  3.70s/it] 28%|██▊       | 36/129 [02:13<05:43,  3.70s/it] 29%|██▊       | 37/129 [02:17<05:39,  3.69s/it] 29%|██▉       | 38/129 [02:20<05:35,  3.69s/it] 30%|███       | 39/129 [02:24<05:32,  3.70s/it] 31%|███       | 40/129 [02:28<05:28,  3.70s/it]                                                {'loss': 0.294, 'grad_norm': 0.5587651133537292, 'learning_rate': 8.721758687811352e-05, 'epoch': 0.92}
 31%|███       | 40/129 [02:28<05:28,  3.70s/it] 32%|███▏      | 41/129 [02:31<05:26,  3.71s/it] 33%|███▎      | 42/129 [02:35<05:24,  3.73s/it] 33%|███▎      | 43/129 [02:39<05:19,  3.72s/it] 34%|███▍      | 44/129 [02:43<05:14,  3.70s/it] 35%|███▍      | 45/129 [02:46<05:10,  3.70s/it] 36%|███▌      | 46/129 [02:50<05:06,  3.69s/it] 36%|███▋      | 47/129 [02:54<05:02,  3.69s/it] 37%|███▋      | 48/129 [02:57<04:59,  3.69s/it] 38%|███▊      | 49/129 [03:01<04:55,  3.69s/it] 39%|███▉      | 50/129 [03:05<04:51,  3.69s/it]                                                {'loss': 0.255, 'grad_norm': 0.46701163053512573, 'learning_rate': 7.692839807804521e-05, 'epoch': 1.16}
 39%|███▉      | 50/129 [03:05<04:51,  3.69s/it] 40%|███▉      | 51/129 [03:08<04:48,  3.70s/it] 40%|████      | 52/129 [03:12<04:44,  3.70s/it] 41%|████      | 53/129 [03:16<04:40,  3.70s/it] 42%|████▏     | 54/129 [03:20<04:37,  3.70s/it] 43%|████▎     | 55/129 [03:23<04:33,  3.70s/it] 43%|████▎     | 56/129 [03:27<04:29,  3.69s/it] 44%|████▍     | 57/129 [03:31<04:26,  3.70s/it] 45%|████▍     | 58/129 [03:34<04:22,  3.70s/it] 46%|████▌     | 59/129 [03:38<04:19,  3.71s/it] 47%|████▋     | 60/129 [03:42<04:15,  3.70s/it]                                                {'loss': 0.2367, 'grad_norm': 0.5348514318466187, 'learning_rate': 6.467612865519674e-05, 'epoch': 1.39}
 47%|████▋     | 60/129 [03:42<04:15,  3.70s/it] 47%|████▋     | 61/129 [03:45<04:11,  3.69s/it] 48%|████▊     | 62/129 [03:49<04:07,  3.69s/it] 49%|████▉     | 63/129 [03:53<04:03,  3.69s/it] 50%|████▉     | 64/129 [03:56<03:59,  3.69s/it] 50%|█████     | 65/129 [04:00<03:55,  3.68s/it] 51%|█████     | 66/129 [04:04<03:51,  3.68s/it] 52%|█████▏    | 67/129 [04:07<03:48,  3.68s/it] 53%|█████▎    | 68/129 [04:11<03:44,  3.68s/it] 53%|█████▎    | 69/129 [04:15<03:40,  3.68s/it] 54%|█████▍    | 70/129 [04:19<03:37,  3.68s/it]                                                {'loss': 0.2647, 'grad_norm': 0.5312705636024475, 'learning_rate': 5.135396923380673e-05, 'epoch': 1.62}
 54%|█████▍    | 70/129 [04:19<03:37,  3.68s/it] 55%|█████▌    | 71/129 [04:22<03:34,  3.70s/it] 56%|█████▌    | 72/129 [04:26<03:32,  3.73s/it] 57%|█████▋    | 73/129 [04:30<03:27,  3.71s/it] 57%|█████▋    | 74/129 [04:33<03:23,  3.70s/it] 58%|█████▊    | 75/129 [04:37<03:19,  3.70s/it] 59%|█████▉    | 76/129 [04:41<03:15,  3.70s/it] 60%|█████▉    | 77/129 [04:44<03:12,  3.69s/it] 60%|██████    | 78/129 [04:48<03:07,  3.68s/it] 61%|██████    | 79/129 [04:52<03:03,  3.67s/it] 62%|██████▏   | 80/129 [04:55<03:00,  3.67s/it]                                                {'loss': 0.2068, 'grad_norm': 0.4779268205165863, 'learning_rate': 3.793310543501473e-05, 'epoch': 1.85}
 62%|██████▏   | 80/129 [04:55<03:00,  3.67s/it] 63%|██████▎   | 81/129 [04:59<02:56,  3.69s/it] 64%|██████▎   | 82/129 [05:03<02:52,  3.68s/it] 64%|██████▍   | 83/129 [05:07<02:49,  3.68s/it] 65%|██████▌   | 84/129 [05:10<02:46,  3.69s/it] 66%|██████▌   | 85/129 [05:14<02:42,  3.68s/it] 67%|██████▋   | 86/129 [05:18<02:38,  3.68s/it] 67%|██████▋   | 87/129 [05:21<02:34,  3.68s/it] 68%|██████▊   | 88/129 [05:25<02:30,  3.67s/it] 69%|██████▉   | 89/129 [05:29<02:27,  3.68s/it] 70%|██████▉   | 90/129 [05:32<02:23,  3.67s/it]                                                {'loss': 0.2164, 'grad_norm': 0.4731956422328949, 'learning_rate': 2.539191843054963e-05, 'epoch': 2.08}
 70%|██████▉   | 90/129 [05:32<02:23,  3.67s/it] 71%|███████   | 91/129 [05:36<02:19,  3.67s/it] 71%|███████▏  | 92/129 [05:40<02:16,  3.68s/it] 72%|███████▏  | 93/129 [05:43<02:12,  3.68s/it] 73%|███████▎  | 94/129 [05:47<02:09,  3.71s/it] 74%|███████▎  | 95/129 [05:51<02:05,  3.70s/it] 74%|███████▍  | 96/129 [05:54<02:01,  3.69s/it] 75%|███████▌  | 97/129 [05:58<01:58,  3.69s/it] 76%|███████▌  | 98/129 [06:02<01:53,  3.68s/it] 77%|███████▋  | 99/129 [06:05<01:50,  3.67s/it] 78%|███████▊  | 100/129 [06:09<01:46,  3.67s/it]                                                 {'loss': 0.1861, 'grad_norm': 0.5850450992584229, 'learning_rate': 1.4644660940672627e-05, 'epoch': 2.31}
 78%|███████▊  | 100/129 [06:09<01:46,  3.67s/it] 78%|███████▊  | 101/129 [06:13<01:42,  3.67s/it] 79%|███████▉  | 102/129 [06:16<01:39,  3.67s/it] 80%|███████▉  | 103/129 [06:20<01:35,  3.67s/it] 81%|████████  | 104/129 [06:24<01:31,  3.68s/it] 81%|████████▏ | 105/129 [06:27<01:28,  3.68s/it] 82%|████████▏ | 106/129 [06:31<01:24,  3.68s/it] 83%|████████▎ | 107/129 [06:35<01:20,  3.68s/it] 84%|████████▎ | 108/129 [06:39<01:17,  3.67s/it] 84%|████████▍ | 109/129 [06:42<01:13,  3.68s/it] 85%|████████▌ | 110/129 [06:46<01:09,  3.68s/it]                                                 {'loss': 0.1844, 'grad_norm': 0.39618638157844543, 'learning_rate': 6.474808197191401e-06, 'epoch': 2.54}
 85%|████████▌ | 110/129 [06:46<01:09,  3.68s/it] 86%|████████▌ | 111/129 [06:50<01:06,  3.68s/it] 87%|████████▋ | 112/129 [06:53<01:02,  3.68s/it] 88%|████████▊ | 113/129 [06:57<00:58,  3.68s/it] 88%|████████▊ | 114/129 [07:01<00:55,  3.68s/it] 89%|████████▉ | 115/129 [07:04<00:51,  3.68s/it] 90%|████████▉ | 116/129 [07:08<00:47,  3.69s/it] 91%|█████████ | 117/129 [07:12<00:44,  3.69s/it] 91%|█████████▏| 118/129 [07:15<00:40,  3.68s/it] 92%|█████████▏| 119/129 [07:19<00:36,  3.68s/it] 93%|█████████▎| 120/129 [07:23<00:33,  3.68s/it]                                                 {'loss': 0.1789, 'grad_norm': 0.5589019656181335, 'learning_rate': 1.4779425873394259e-06, 'epoch': 2.77}
 93%|█████████▎| 120/129 [07:23<00:33,  3.68s/it] 94%|█████████▍| 121/129 [07:26<00:29,  3.68s/it] 95%|█████████▍| 122/129 [07:30<00:25,  3.67s/it] 95%|█████████▌| 123/129 [07:34<00:22,  3.68s/it] 96%|█████████▌| 124/129 [07:37<00:18,  3.69s/it] 97%|█████████▋| 125/129 [07:41<00:14,  3.70s/it] 98%|█████████▊| 126/129 [07:45<00:11,  3.71s/it] 98%|█████████▊| 127/129 [07:49<00:07,  3.69s/it] 99%|█████████▉| 128/129 [07:52<00:03,  3.68s/it]100%|██████████| 129/129 [07:56<00:00,  3.68s/it][INFO|trainer.py:3705] 2024-11-18 22:27:30,219 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/checkpoint-129
[INFO|configuration_utils.py:673] 2024-11-18 22:27:30,257 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:27:30,258 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:27:30,421 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/checkpoint-129/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:27:30,421 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/checkpoint-129/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:27:31,153 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 478.0125, 'train_samples_per_second': 4.337, 'train_steps_per_second': 0.27, 'train_loss': 0.40058720943539644, 'epoch': 2.98}
100%|██████████| 129/129 [07:57<00:00,  3.68s/it]100%|██████████| 129/129 [07:57<00:00,  3.70s/it]
[INFO|trainer.py:3705] 2024-11-18 22:27:31,171 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:27:31,204 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:27:31,205 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:27:31,361 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:27:31,361 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9827
  total_flos               = 81398548GF
  train_loss               =     0.4006
  train_runtime            = 0:07:58.01
  train_samples_per_second =      4.337
  train_steps_per_second   =       0.27
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_6_4/lora/sft/training_loss.png
11/18/2024 22:27:31 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:27:31 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:27:31,827 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:27:31,827 >>   Num examples = 77
[INFO|trainer.py:4026] 2024-11-18 22:27:31,827 >>   Batch size = 1
  0%|          | 0/39 [00:00<?, ?it/s]  5%|▌         | 2/39 [00:00<00:02, 15.94it/s] 10%|█         | 4/39 [00:00<00:03, 10.09it/s] 15%|█▌        | 6/39 [00:00<00:03,  8.94it/s] 18%|█▊        | 7/39 [00:00<00:03,  8.71it/s] 21%|██        | 8/39 [00:00<00:03,  8.51it/s] 23%|██▎       | 9/39 [00:00<00:03,  8.44it/s] 26%|██▌       | 10/39 [00:01<00:03,  8.27it/s] 28%|██▊       | 11/39 [00:01<00:03,  8.25it/s] 31%|███       | 12/39 [00:01<00:03,  8.21it/s] 33%|███▎      | 13/39 [00:01<00:03,  8.22it/s] 36%|███▌      | 14/39 [00:01<00:03,  8.22it/s] 38%|███▊      | 15/39 [00:01<00:02,  8.11it/s] 41%|████      | 16/39 [00:01<00:02,  8.03it/s] 44%|████▎     | 17/39 [00:01<00:02,  7.99it/s] 46%|████▌     | 18/39 [00:02<00:02,  7.95it/s] 49%|████▊     | 19/39 [00:02<00:02,  7.92it/s] 51%|█████▏    | 20/39 [00:02<00:02,  7.87it/s] 54%|█████▍    | 21/39 [00:02<00:02,  7.98it/s] 56%|█████▋    | 22/39 [00:02<00:02,  8.04it/s] 59%|█████▉    | 23/39 [00:02<00:02,  7.98it/s] 62%|██████▏   | 24/39 [00:02<00:01,  7.94it/s] 64%|██████▍   | 25/39 [00:02<00:01,  8.03it/s] 67%|██████▋   | 26/39 [00:03<00:01,  8.09it/s] 69%|██████▉   | 27/39 [00:03<00:01,  8.02it/s] 72%|███████▏  | 28/39 [00:03<00:01,  8.08it/s] 74%|███████▍  | 29/39 [00:03<00:01,  8.10it/s] 77%|███████▋  | 30/39 [00:03<00:01,  8.00it/s] 79%|███████▉  | 31/39 [00:03<00:01,  7.95it/s] 82%|████████▏ | 32/39 [00:03<00:00,  8.03it/s] 85%|████████▍ | 33/39 [00:03<00:00,  7.98it/s] 87%|████████▋ | 34/39 [00:04<00:00,  8.02it/s] 90%|████████▉ | 35/39 [00:04<00:00,  7.96it/s] 92%|█████████▏| 36/39 [00:04<00:00,  8.05it/s] 95%|█████████▍| 37/39 [00:04<00:00,  8.10it/s] 97%|█████████▋| 38/39 [00:04<00:00,  8.14it/s]100%|██████████| 39/39 [00:04<00:00,  8.23it/s]100%|██████████| 39/39 [00:04<00:00,  8.24it/s]
***** eval metrics *****
  epoch                   =     2.9827
  eval_loss               =     0.1949
  eval_runtime            = 0:00:04.86
  eval_samples_per_second =     15.814
  eval_steps_per_second   =       8.01
[INFO|modelcard.py:449] 2024-11-18 22:27:36,697 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:27:56,161] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:28:00 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:28056
[2024-11-18 22:28:02,142] torch.distributed.run: [WARNING] 
[2024-11-18 22:28:02,142] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:28:02,142] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:28:02,142] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:28:09,309] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:28:09,341] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:28:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:28:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:28:10,460 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:28:10,462 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,463 >> loading file tokenizer_config.json
11/18/2024 22:28:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:28:10 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:28:10,945 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:28:10,946 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:28:10,947 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:28:10,948 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:28:11,466 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:28:11 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:28:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:28:11 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_7_3.json...
11/18/2024 22:28:11 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:28:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 896 examples [00:00, 12663.67 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 56/896 [00:00<00:01, 458.07 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 896/896 [00:00<00:00, 3165.76 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:28:15 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_7_3.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/896 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 56/896 [00:01<00:18, 44.32 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▎        | 112/896 [00:01<00:08, 89.12 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 168/896 [00:01<00:05, 131.80 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 224/896 [00:01<00:04, 167.07 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 280/896 [00:02<00:03, 195.95 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 336/896 [00:02<00:02, 220.57 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 392/896 [00:02<00:02, 243.76 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 448/896 [00:02<00:01, 258.40 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 504/896 [00:02<00:01, 270.81 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 616/896 [00:03<00:01, 269.56 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 728/896 [00:03<00:00, 345.22 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 784/896 [00:03<00:00, 344.81 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 840/896 [00:03<00:00, 344.55 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 330.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 896/896 [00:03<00:00, 224.79 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 57804, 11, 79256, 606, 1131, 12730, 62, 12699, 1566, 24536, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 12730, 62, 12699, 1566, 24536, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 19384, 35641, 1566, 24536, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 1254, 1110, 17640, 5713, 905, 36793, 30, 654, 28, 17, 20, 13, 20, 18, 15, 23, 23, 24, 11, 21, 24, 13, 15, 16, 22, 18, 15, 21, 5, 2154, 77, 28, 15, 13, 15, 16, 11, 15, 13, 15, 16, 63225, 27221, 62735, 28, 17, 20, 13, 20, 18, 15, 23, 23, 24, 11, 21, 24, 13, 15, 16, 22, 18, 15, 21, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 63848, 53640, 920, 487, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 49, 266, 14649, 3362, 1566, 404, 403, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 33605, 1566, 3834, 2870, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 693, 91913, 16068, 1098, 266, 41660, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 1655, 1098, 276, 62, 2183, 43409, 6, 9370, 92374, 271, 6347, 320, 77, 16, 25, 57804, 7287, 58, 81, 16, 25, 285, 39463, 258, 8462, 6294, 7, 77, 17, 25, 2007, 8, 220, 1380, 308, 16, 2644, 1131, 35641, 11870, 1566, 404, 2284, 6, 470, 308, 16, 1764, 11, 308, 16, 2644, 11, 308, 17, 7315, 3930, 220, 18, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为organisation,属性name='City_Star_Airlines'的节点
label为organisation,属性name='City_Star_Airlines'的节点
label为organisation,属性name='AlphaJet_Airlines'的节点
label为place,属性name='http://maps.google.com/maps?ll=25.530889,69.017306&spn=0.01,0.01&t=m&q=25.530889,69.017306'的节点
label为place,属性name='Ramadan_City'的节点
label为place,属性name='Ratmalana_Airport'的节点
label为tag,属性name='Destination_Anywhere'的节点
label为tag,属性name='Recovering_the_Satellites'的节点
label为tag,属性name='At_San_Quentin'的节点

match (n1:organisation)-[r1:islocatedinorgan]->(n2:place)  where n1.name='Jetstar_Airways' return n1.id, n1.name, n2.url limit 3;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 57804, 7287, 58, 81, 16, 25, 285, 39463, 258, 8462, 6294, 7, 77, 17, 25, 2007, 8, 220, 1380, 308, 16, 2644, 1131, 35641, 11870, 1566, 404, 2284, 6, 470, 308, 16, 1764, 11, 308, 16, 2644, 11, 308, 17, 7315, 3930, 220, 18, 26, 151645]
labels:
match (n1:organisation)-[r1:islocatedinorgan]->(n2:place)  where n1.name='Jetstar_Airways' return n1.id, n1.name, n2.url limit 3;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:28:20,634 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:28:20,635 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:28:20,686 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:28:20,686 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:28:20,688 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:28:25,183 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:28:25,183 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:28:25,217 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:28:25,217 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:28:25 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:28:25 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,down_proj,k_proj,o_proj,up_proj,v_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]
11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:28:25 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:28:25 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:28:25 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,up_proj,o_proj,k_proj,down_proj,v_proj
11/18/2024 22:28:25 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:28:25,971 >> Using auto half precision backend
11/18/2024 22:28:26 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:28:26,690 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:28:26,690 >>   Num examples = 806
[INFO|trainer.py:2245] 2024-11-18 22:28:26,690 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:28:26,690 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:28:26,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:28:26,690 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:28:26,690 >>   Total optimization steps = 150
[INFO|trainer.py:2252] 2024-11-18 22:28:26,697 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:04<10:15,  4.13s/it]  1%|▏         | 2/150 [00:07<09:35,  3.89s/it]  2%|▏         | 3/150 [00:11<09:20,  3.81s/it]  3%|▎         | 4/150 [00:15<09:15,  3.80s/it]  3%|▎         | 5/150 [00:19<09:07,  3.77s/it]  4%|▍         | 6/150 [00:22<09:01,  3.76s/it]  5%|▍         | 7/150 [00:26<08:57,  3.76s/it]  5%|▌         | 8/150 [00:30<08:51,  3.74s/it]  6%|▌         | 9/150 [00:34<08:48,  3.75s/it]  7%|▋         | 10/150 [00:37<08:42,  3.74s/it]                                                {'loss': 1.8511, 'grad_norm': 1.9511393308639526, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.2}
  7%|▋         | 10/150 [00:37<08:42,  3.74s/it]  7%|▋         | 11/150 [00:41<08:38,  3.73s/it]  8%|▊         | 12/150 [00:45<08:35,  3.74s/it]  9%|▊         | 13/150 [00:48<08:31,  3.73s/it]  9%|▉         | 14/150 [00:52<08:28,  3.74s/it] 10%|█         | 15/150 [00:56<08:22,  3.72s/it] 11%|█         | 16/150 [01:00<08:17,  3.71s/it] 11%|█▏        | 17/150 [01:03<08:13,  3.71s/it] 12%|█▏        | 18/150 [01:07<08:09,  3.71s/it] 13%|█▎        | 19/150 [01:11<08:05,  3.71s/it] 13%|█▎        | 20/150 [01:14<08:01,  3.70s/it]                                                {'loss': 0.9194, 'grad_norm': 0.8596189022064209, 'learning_rate': 9.966191788709716e-05, 'epoch': 0.4}
 13%|█▎        | 20/150 [01:14<08:01,  3.70s/it] 14%|█▍        | 21/150 [01:18<07:57,  3.70s/it] 15%|█▍        | 22/150 [01:22<07:53,  3.70s/it] 15%|█▌        | 23/150 [01:25<07:49,  3.70s/it] 16%|█▌        | 24/150 [01:29<07:46,  3.70s/it] 17%|█▋        | 25/150 [01:33<07:42,  3.70s/it] 17%|█▋        | 26/150 [01:37<07:39,  3.71s/it] 18%|█▊        | 27/150 [01:40<07:35,  3.71s/it] 19%|█▊        | 28/150 [01:44<07:32,  3.71s/it] 19%|█▉        | 29/150 [01:48<07:28,  3.71s/it] 20%|██        | 30/150 [01:51<07:25,  3.71s/it]                                                {'loss': 0.3983, 'grad_norm': 0.7841836810112, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.6}
 20%|██        | 30/150 [01:51<07:25,  3.71s/it] 21%|██        | 31/150 [01:55<07:21,  3.71s/it] 21%|██▏       | 32/150 [01:59<07:18,  3.72s/it] 22%|██▏       | 33/150 [02:03<07:13,  3.71s/it] 23%|██▎       | 34/150 [02:06<07:10,  3.71s/it] 23%|██▎       | 35/150 [02:10<07:06,  3.71s/it] 24%|██▍       | 36/150 [02:14<07:03,  3.71s/it] 25%|██▍       | 37/150 [02:17<06:59,  3.71s/it] 25%|██▌       | 38/150 [02:21<06:54,  3.70s/it] 26%|██▌       | 39/150 [02:25<06:52,  3.71s/it] 27%|██▋       | 40/150 [02:29<06:48,  3.71s/it]                                                {'loss': 0.2601, 'grad_norm': 1.1991078853607178, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.79}
 27%|██▋       | 40/150 [02:29<06:48,  3.71s/it] 27%|██▋       | 41/150 [02:32<06:44,  3.71s/it] 28%|██▊       | 42/150 [02:36<06:39,  3.70s/it] 29%|██▊       | 43/150 [02:40<06:36,  3.70s/it] 29%|██▉       | 44/150 [02:43<06:32,  3.71s/it] 30%|███       | 45/150 [02:47<06:29,  3.71s/it] 31%|███       | 46/150 [02:51<06:25,  3.71s/it] 31%|███▏      | 47/150 [02:54<06:22,  3.71s/it] 32%|███▏      | 48/150 [02:58<06:18,  3.71s/it] 33%|███▎      | 49/150 [03:02<06:14,  3.71s/it] 33%|███▎      | 50/150 [03:06<06:10,  3.71s/it]                                                {'loss': 0.2806, 'grad_norm': 0.7703192830085754, 'learning_rate': 8.43120818934367e-05, 'epoch': 0.99}
 33%|███▎      | 50/150 [03:06<06:10,  3.71s/it] 34%|███▍      | 51/150 [03:09<06:07,  3.71s/it] 35%|███▍      | 52/150 [03:13<06:03,  3.71s/it] 35%|███▌      | 53/150 [03:17<05:59,  3.71s/it] 36%|███▌      | 54/150 [03:20<05:56,  3.71s/it] 37%|███▋      | 55/150 [03:24<05:52,  3.71s/it] 37%|███▋      | 56/150 [03:28<05:48,  3.70s/it] 38%|███▊      | 57/150 [03:32<05:44,  3.70s/it] 39%|███▊      | 58/150 [03:35<05:41,  3.71s/it] 39%|███▉      | 59/150 [03:39<05:37,  3.71s/it] 40%|████      | 60/150 [03:43<05:33,  3.70s/it]                                                {'loss': 0.2233, 'grad_norm': 0.4463975131511688, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.19}
 40%|████      | 60/150 [03:43<05:33,  3.70s/it] 41%|████      | 61/150 [03:46<05:29,  3.70s/it] 41%|████▏     | 62/150 [03:50<05:26,  3.71s/it] 42%|████▏     | 63/150 [03:54<05:23,  3.71s/it] 43%|████▎     | 64/150 [03:58<05:18,  3.70s/it] 43%|████▎     | 65/150 [04:01<05:15,  3.71s/it] 44%|████▍     | 66/150 [04:05<05:11,  3.71s/it] 45%|████▍     | 67/150 [04:09<05:08,  3.72s/it] 45%|████▌     | 68/150 [04:12<05:06,  3.73s/it] 46%|████▌     | 69/150 [04:16<05:01,  3.72s/it] 47%|████▋     | 70/150 [04:20<04:57,  3.71s/it]                                                {'loss': 0.2075, 'grad_norm': 0.6480913758277893, 'learning_rate': 6.434016163555452e-05, 'epoch': 1.39}
 47%|████▋     | 70/150 [04:20<04:57,  3.71s/it] 47%|████▋     | 71/150 [04:24<04:53,  3.71s/it] 48%|████▊     | 72/150 [04:27<04:49,  3.71s/it] 49%|████▊     | 73/150 [04:31<04:45,  3.71s/it] 49%|████▉     | 74/150 [04:35<04:42,  3.71s/it] 50%|█████     | 75/150 [04:38<04:38,  3.71s/it] 51%|█████     | 76/150 [04:42<04:34,  3.71s/it] 51%|█████▏    | 77/150 [04:46<04:31,  3.72s/it] 52%|█████▏    | 78/150 [04:50<04:27,  3.71s/it] 53%|█████▎    | 79/150 [04:53<04:24,  3.72s/it] 53%|█████▎    | 80/150 [04:57<04:20,  3.72s/it]                                                {'loss': 0.2264, 'grad_norm': 0.5616049766540527, 'learning_rate': 5.290724144552379e-05, 'epoch': 1.59}
 53%|█████▎    | 80/150 [04:57<04:20,  3.72s/it] 54%|█████▍    | 81/150 [05:01<04:16,  3.72s/it] 55%|█████▍    | 82/150 [05:04<04:13,  3.72s/it] 55%|█████▌    | 83/150 [05:08<04:09,  3.73s/it] 56%|█████▌    | 84/150 [05:12<04:05,  3.72s/it] 57%|█████▋    | 85/150 [05:16<04:01,  3.72s/it] 57%|█████▋    | 86/150 [05:19<03:57,  3.71s/it] 58%|█████▊    | 87/150 [05:23<03:53,  3.71s/it] 59%|█████▊    | 88/150 [05:27<03:49,  3.71s/it] 59%|█████▉    | 89/150 [05:30<03:46,  3.72s/it] 60%|██████    | 90/150 [05:34<03:42,  3.71s/it]                                                {'loss': 0.1937, 'grad_norm': 0.5851545333862305, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.79}
 60%|██████    | 90/150 [05:34<03:42,  3.71s/it] 61%|██████    | 91/150 [05:38<03:38,  3.71s/it] 61%|██████▏   | 92/150 [05:42<03:34,  3.70s/it] 62%|██████▏   | 93/150 [05:45<03:30,  3.70s/it] 63%|██████▎   | 94/150 [05:49<03:27,  3.71s/it] 63%|██████▎   | 95/150 [05:53<03:24,  3.72s/it] 64%|██████▍   | 96/150 [05:56<03:20,  3.72s/it] 65%|██████▍   | 97/150 [06:00<03:18,  3.75s/it] 65%|██████▌   | 98/150 [06:04<03:14,  3.74s/it] 66%|██████▌   | 99/150 [06:08<03:09,  3.72s/it] 67%|██████▋   | 100/150 [06:11<03:05,  3.72s/it]                                                 {'loss': 0.1975, 'grad_norm': 0.46989673376083374, 'learning_rate': 3.019601169804216e-05, 'epoch': 1.99}
 67%|██████▋   | 100/150 [06:11<03:05,  3.72s/it] 67%|██████▋   | 101/150 [06:15<03:01,  3.71s/it] 68%|██████▊   | 102/150 [06:19<02:58,  3.72s/it] 69%|██████▊   | 103/150 [06:22<02:54,  3.71s/it] 69%|██████▉   | 104/150 [06:26<02:50,  3.70s/it] 70%|███████   | 105/150 [06:30<02:46,  3.71s/it] 71%|███████   | 106/150 [06:34<02:43,  3.71s/it] 71%|███████▏  | 107/150 [06:37<02:39,  3.71s/it] 72%|███████▏  | 108/150 [06:41<02:35,  3.71s/it] 73%|███████▎  | 109/150 [06:45<02:32,  3.71s/it] 73%|███████▎  | 110/150 [06:48<02:28,  3.71s/it]                                                 {'loss': 0.1784, 'grad_norm': 0.5356705784797668, 'learning_rate': 2.0142070414860704e-05, 'epoch': 2.18}
 73%|███████▎  | 110/150 [06:48<02:28,  3.71s/it] 74%|███████▍  | 111/150 [06:52<02:24,  3.71s/it] 75%|███████▍  | 112/150 [06:56<02:20,  3.71s/it] 75%|███████▌  | 113/150 [07:00<02:17,  3.71s/it] 76%|███████▌  | 114/150 [07:03<02:13,  3.71s/it] 77%|███████▋  | 115/150 [07:07<02:09,  3.71s/it] 77%|███████▋  | 116/150 [07:11<02:06,  3.72s/it] 78%|███████▊  | 117/150 [07:15<02:03,  3.74s/it] 79%|███████▊  | 118/150 [07:18<01:59,  3.73s/it] 79%|███████▉  | 119/150 [07:22<01:55,  3.72s/it] 80%|████████  | 120/150 [07:26<01:51,  3.72s/it]                                                 {'loss': 0.1993, 'grad_norm': 0.4880085289478302, 'learning_rate': 1.1697777844051105e-05, 'epoch': 2.38}
 80%|████████  | 120/150 [07:26<01:51,  3.72s/it] 81%|████████  | 121/150 [07:29<01:47,  3.71s/it] 81%|████████▏ | 122/150 [07:33<01:43,  3.70s/it] 82%|████████▏ | 123/150 [07:37<01:40,  3.71s/it] 83%|████████▎ | 124/150 [07:40<01:36,  3.71s/it] 83%|████████▎ | 125/150 [07:44<01:33,  3.72s/it] 84%|████████▍ | 126/150 [07:48<01:29,  3.72s/it] 85%|████████▍ | 127/150 [07:52<01:25,  3.72s/it] 85%|████████▌ | 128/150 [07:55<01:21,  3.71s/it] 86%|████████▌ | 129/150 [07:59<01:18,  3.73s/it] 87%|████████▋ | 130/150 [08:03<01:14,  3.72s/it]                                                 {'loss': 0.1716, 'grad_norm': 0.4871118664741516, 'learning_rate': 5.318367983829392e-06, 'epoch': 2.58}
 87%|████████▋ | 130/150 [08:03<01:14,  3.72s/it] 87%|████████▋ | 131/150 [08:07<01:10,  3.73s/it] 88%|████████▊ | 132/150 [08:10<01:07,  3.73s/it] 89%|████████▊ | 133/150 [08:14<01:03,  3.71s/it] 89%|████████▉ | 134/150 [08:18<00:59,  3.71s/it] 90%|█████████ | 135/150 [08:21<00:55,  3.71s/it] 91%|█████████ | 136/150 [08:25<00:52,  3.72s/it] 91%|█████████▏| 137/150 [08:29<00:48,  3.71s/it] 92%|█████████▏| 138/150 [08:33<00:44,  3.71s/it] 93%|█████████▎| 139/150 [08:36<00:40,  3.71s/it] 93%|█████████▎| 140/150 [08:40<00:37,  3.71s/it]                                                 {'loss': 0.1726, 'grad_norm': 0.5071643590927124, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.78}
 93%|█████████▎| 140/150 [08:40<00:37,  3.71s/it] 94%|█████████▍| 141/150 [08:44<00:33,  3.71s/it] 95%|█████████▍| 142/150 [08:47<00:29,  3.71s/it] 95%|█████████▌| 143/150 [08:51<00:25,  3.70s/it] 96%|█████████▌| 144/150 [08:55<00:22,  3.70s/it] 97%|█████████▋| 145/150 [08:58<00:18,  3.71s/it] 97%|█████████▋| 146/150 [09:02<00:14,  3.70s/it] 98%|█████████▊| 147/150 [09:06<00:11,  3.70s/it] 99%|█████████▊| 148/150 [09:10<00:07,  3.70s/it] 99%|█████████▉| 149/150 [09:13<00:03,  3.70s/it]100%|██████████| 150/150 [09:17<00:00,  3.71s/it]                                                 {'loss': 0.1812, 'grad_norm': 0.9335079193115234, 'learning_rate': 0.0, 'epoch': 2.98}
100%|██████████| 150/150 [09:17<00:00,  3.71s/it][INFO|trainer.py:3705] 2024-11-18 22:37:44,886 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/checkpoint-150
[INFO|configuration_utils.py:673] 2024-11-18 22:37:44,925 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:37:44,927 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:37:45,085 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:37:45,086 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/checkpoint-150/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:37:45,837 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 559.1408, 'train_samples_per_second': 4.324, 'train_steps_per_second': 0.268, 'train_loss': 0.3773898490269979, 'epoch': 2.98}
100%|██████████| 150/150 [09:18<00:00,  3.71s/it]100%|██████████| 150/150 [09:18<00:00,  3.72s/it]
[INFO|trainer.py:3705] 2024-11-18 22:37:45,840 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:37:45,874 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:37:45,875 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:37:46,025 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:37:46,025 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9777
  total_flos               = 94670655GF
  train_loss               =     0.3774
  train_runtime            = 0:09:19.14
  train_samples_per_second =      4.324
  train_steps_per_second   =      0.268
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_7_3/lora/sft/training_loss.png
11/18/2024 22:37:46 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:37:46 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:37:46,501 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:37:46,501 >>   Num examples = 90
[INFO|trainer.py:4026] 2024-11-18 22:37:46,501 >>   Batch size = 1
  0%|          | 0/45 [00:00<?, ?it/s]  4%|▍         | 2/45 [00:00<00:02, 15.47it/s]  9%|▉         | 4/45 [00:00<00:04,  9.87it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.96it/s] 16%|█▌        | 7/45 [00:00<00:04,  8.73it/s] 18%|█▊        | 8/45 [00:00<00:04,  8.47it/s] 20%|██        | 9/45 [00:01<00:04,  8.40it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.23it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.22it/s] 27%|██▋       | 12/45 [00:01<00:04,  8.06it/s] 29%|██▉       | 13/45 [00:01<00:03,  8.04it/s] 31%|███       | 14/45 [00:01<00:03,  7.94it/s] 33%|███▎      | 15/45 [00:01<00:03,  8.01it/s] 36%|███▌      | 16/45 [00:01<00:03,  8.03it/s] 38%|███▊      | 17/45 [00:02<00:03,  8.08it/s] 40%|████      | 18/45 [00:02<00:03,  8.11it/s] 42%|████▏     | 19/45 [00:02<00:03,  8.11it/s] 44%|████▍     | 20/45 [00:02<00:03,  8.11it/s] 47%|████▋     | 21/45 [00:02<00:02,  8.03it/s] 49%|████▉     | 22/45 [00:02<00:02,  8.05it/s] 51%|█████     | 23/45 [00:02<00:02,  8.10it/s] 53%|█████▎    | 24/45 [00:02<00:02,  8.13it/s] 56%|█████▌    | 25/45 [00:02<00:02,  8.08it/s] 58%|█████▊    | 26/45 [00:03<00:02,  8.10it/s] 60%|██████    | 27/45 [00:03<00:02,  8.13it/s] 62%|██████▏   | 28/45 [00:03<00:02,  7.99it/s] 64%|██████▍   | 29/45 [00:03<00:02,  7.94it/s] 67%|██████▋   | 30/45 [00:03<00:01,  8.02it/s] 69%|██████▉   | 31/45 [00:03<00:01,  7.99it/s] 71%|███████   | 32/45 [00:03<00:01,  8.05it/s] 73%|███████▎  | 33/45 [00:03<00:01,  8.08it/s] 76%|███████▌  | 34/45 [00:04<00:01,  8.00it/s] 78%|███████▊  | 35/45 [00:04<00:01,  7.99it/s] 80%|████████  | 36/45 [00:04<00:01,  7.94it/s] 82%|████████▏ | 37/45 [00:04<00:00,  8.02it/s] 84%|████████▍ | 38/45 [00:04<00:00,  7.95it/s] 87%|████████▋ | 39/45 [00:04<00:00,  7.91it/s] 89%|████████▉ | 40/45 [00:04<00:00,  7.95it/s] 91%|█████████ | 41/45 [00:05<00:00,  7.91it/s] 93%|█████████▎| 42/45 [00:05<00:00,  7.88it/s] 96%|█████████▌| 43/45 [00:05<00:00,  7.86it/s] 98%|█████████▊| 44/45 [00:05<00:00,  7.96it/s]100%|██████████| 45/45 [00:05<00:00,  8.03it/s]100%|██████████| 45/45 [00:05<00:00,  8.18it/s]
***** eval metrics *****
  epoch                   =     2.9777
  eval_loss               =     0.2672
  eval_runtime            = 0:00:05.64
  eval_samples_per_second =     15.932
  eval_steps_per_second   =      7.966
[INFO|modelcard.py:449] 2024-11-18 22:37:52,151 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:38:14,179] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:38:17 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20532
[2024-11-18 22:38:20,110] torch.distributed.run: [WARNING] 
[2024-11-18 22:38:20,110] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:38:20,110] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:38:20,110] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:38:27,317] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:38:27,460] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:38:28 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:38:28 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:38:28,466 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:38:28,467 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,469 >> loading file tokenizer_config.json
11/18/2024 22:38:28 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:38:28 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:38:28,926 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:38:28,927 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:38:28,928 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:38:28,929 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:38:29,393 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:38:29 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:38:29 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:38:29 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_8_2.json...
11/18/2024 22:38:29 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:38:29 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1024 examples [00:00, 12946.63 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:01, 496.83 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3289.90 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:38:33 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_8_2.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:01<00:18, 52.04 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:01<00:08, 103.75 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 189/1000 [00:01<00:05, 151.87 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 252/1000 [00:01<00:03, 194.22 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:01<00:02, 233.70 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 378/1000 [00:02<00:02, 263.11 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:02<00:01, 287.98 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 504/1000 [00:02<00:01, 307.76 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 566/1000 [00:02<00:01, 322.17 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:02<00:01, 334.62 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:03<00:00, 373.91 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:03<00:00, 396.24 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:03<00:00, 363.00 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:03<00:00, 378.30 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 384.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 259.49 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 19, 315, 34314, 315, 15948, 20751, 42482, 7946, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 19, 315, 34314, 315, 15948, 18294, 2123, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 19, 315, 34314, 315, 15948, 9240, 4360, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 50, 2375, 815, 1566, 81, 749, 832, 64, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 67199, 1103, 1604, 21761, 1098, 2135, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 47, 31460, 2259, 2232, 78836, 577, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 35194, 299, 2089, 2388, 674, 65881, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 17117, 3165, 2351, 13722, 78, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 23729, 1088, 291, 299, 2646, 277, 4360, 2646, 8198, 7431, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 35194, 299, 7959, 3575, 1668, 14042, 6, 6347, 320, 77, 16, 25, 6182, 7287, 58, 81, 16, 25, 4648, 32398, 6182, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 2269, 25819, 315, 15948, 6, 470, 308, 17, 72056, 11, 308, 16, 8219, 573, 11, 308, 17, 72637, 11, 308, 17, 48337, 11, 308, 17, 9847, 11, 308, 16, 5406, 11, 308, 16, 1954, 10706, 220, 18, 3930, 220, 19, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Album 4 of Emperor of Brazil Dom Pedro II'的节点
label为forum,属性title='Album 4 of Emperor of Brazil Machado'的节点
label为forum,属性title='Album 4 of Emperor of Brazil Souza'的节点
label为organisation,属性name='Sergio_Arboleda_University'的节点
label为organisation,属性name='Brazilian_Naval_School'的节点
label为organisation,属性name='Pablo_de_Olavide_University'的节点
label为place,属性name='Pedro_Escobedo'的节点
label为place,属性name='Antón_Lizardo'的节点
label为place,属性name='San_Pedro_Garza_García'的节点
label为tag,属性name='Pedro_I_of_Brazil'match (n1:comment)-[r1:hascreatorcomment]->(n2:person)  where n2.firstname='Emperor of Brazil' return n2.firstname, n1.locationip, n2.lastname, n2.gender, n2.email, n1.content, n1.length skip 3 limit 4;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 6182, 7287, 58, 81, 16, 25, 4648, 32398, 6182, 6294, 7, 77, 17, 25, 8987, 8, 220, 1380, 308, 17, 72056, 1131, 2269, 25819, 315, 15948, 6, 470, 308, 17, 72056, 11, 308, 16, 8219, 573, 11, 308, 17, 72637, 11, 308, 17, 48337, 11, 308, 17, 9847, 11, 308, 16, 5406, 11, 308, 16, 1954, 10706, 220, 18, 3930, 220, 19, 26, 151645]
labels:
match (n1:comment)-[r1:hascreatorcomment]->(n2:person)  where n2.firstname='Emperor of Brazil' return n2.firstname, n1.locationip, n2.lastname, n2.gender, n2.email, n1.content, n1.length skip 3 limit 4;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:38:37,962 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:38:37,964 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:38:38,014 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:38:38,015 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:38:38,016 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
[INFO|modeling_utils.py:4574] 2024-11-18 22:38:42,264 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:38:42,264 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:38:42,268 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:38:42,269 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:38:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:38:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,o_proj,down_proj,q_proj,k_proj,up_proj
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:38:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:38:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:38:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,v_proj,gate_proj,down_proj,up_proj,k_proj
11/18/2024 22:38:42 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:38:42,988 >> Using auto half precision backend
11/18/2024 22:38:43 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:38:43,636 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:38:43,636 >>   Num examples = 900
[INFO|trainer.py:2245] 2024-11-18 22:38:43,636 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:38:43,637 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:38:43,637 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:38:43,637 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:38:43,637 >>   Total optimization steps = 168
[INFO|trainer.py:2252] 2024-11-18 22:38:43,643 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/168 [00:00<?, ?it/s]  1%|          | 1/168 [00:04<11:27,  4.12s/it]  1%|          | 2/168 [00:07<10:42,  3.87s/it]  2%|▏         | 3/168 [00:11<10:24,  3.78s/it]  2%|▏         | 4/168 [00:15<10:15,  3.76s/it]  3%|▎         | 5/168 [00:18<10:07,  3.73s/it]  4%|▎         | 6/168 [00:22<10:02,  3.72s/it]  4%|▍         | 7/168 [00:26<09:56,  3.71s/it]  5%|▍         | 8/168 [00:29<09:52,  3.71s/it]  5%|▌         | 9/168 [00:33<09:49,  3.71s/it]  6%|▌         | 10/168 [00:37<09:46,  3.71s/it]                                                {'loss': 1.7954, 'grad_norm': 2.366840362548828, 'learning_rate': 5.882352941176471e-05, 'epoch': 0.18}
  6%|▌         | 10/168 [00:37<09:46,  3.71s/it]  7%|▋         | 11/168 [00:41<09:41,  3.70s/it]  7%|▋         | 12/168 [00:44<09:35,  3.69s/it]  8%|▊         | 13/168 [00:48<09:32,  3.69s/it]  8%|▊         | 14/168 [00:52<09:27,  3.69s/it]  9%|▉         | 15/168 [00:55<09:24,  3.69s/it] 10%|▉         | 16/168 [00:59<09:18,  3.68s/it] 10%|█         | 17/168 [01:03<09:17,  3.69s/it] 11%|█         | 18/168 [01:06<09:11,  3.68s/it] 11%|█▏        | 19/168 [01:10<09:08,  3.68s/it] 12%|█▏        | 20/168 [01:14<09:06,  3.69s/it]                                                {'loss': 1.0138, 'grad_norm': 0.7835057377815247, 'learning_rate': 9.990263847374976e-05, 'epoch': 0.36}
 12%|█▏        | 20/168 [01:14<09:06,  3.69s/it] 12%|█▎        | 21/168 [01:17<09:02,  3.69s/it] 13%|█▎        | 22/168 [01:21<08:57,  3.68s/it] 14%|█▎        | 23/168 [01:25<08:54,  3.69s/it] 14%|█▍        | 24/168 [01:29<08:52,  3.70s/it] 15%|█▍        | 25/168 [01:32<08:49,  3.70s/it] 15%|█▌        | 26/168 [01:36<08:45,  3.70s/it] 16%|█▌        | 27/168 [01:40<08:42,  3.71s/it] 17%|█▋        | 28/168 [01:43<08:38,  3.70s/it] 17%|█▋        | 29/168 [01:47<08:33,  3.69s/it] 18%|█▊        | 30/168 [01:51<08:28,  3.69s/it]                                                {'loss': 0.4142, 'grad_norm': 0.7916020750999451, 'learning_rate': 9.818229479678158e-05, 'epoch': 0.53}
 18%|█▊        | 30/168 [01:51<08:28,  3.69s/it] 18%|█▊        | 31/168 [01:54<08:24,  3.68s/it] 19%|█▉        | 32/168 [01:58<08:21,  3.69s/it] 20%|█▉        | 33/168 [02:02<08:19,  3.70s/it] 20%|██        | 34/168 [02:05<08:14,  3.69s/it] 21%|██        | 35/168 [02:09<08:11,  3.69s/it] 21%|██▏       | 36/168 [02:13<08:07,  3.69s/it] 22%|██▏       | 37/168 [02:17<08:03,  3.69s/it] 23%|██▎       | 38/168 [02:20<07:58,  3.68s/it] 23%|██▎       | 39/168 [02:24<07:54,  3.68s/it] 24%|██▍       | 40/168 [02:28<07:50,  3.67s/it]                                                {'loss': 0.2995, 'grad_norm': 0.8022957444190979, 'learning_rate': 9.438385228425938e-05, 'epoch': 0.71}
 24%|██▍       | 40/168 [02:28<07:50,  3.67s/it] 24%|██▍       | 41/168 [02:31<07:47,  3.68s/it] 25%|██▌       | 42/168 [02:35<07:43,  3.68s/it] 26%|██▌       | 43/168 [02:39<07:40,  3.68s/it] 26%|██▌       | 44/168 [02:42<07:37,  3.69s/it] 27%|██▋       | 45/168 [02:46<07:33,  3.68s/it] 27%|██▋       | 46/168 [02:50<07:29,  3.68s/it] 28%|██▊       | 47/168 [02:53<07:25,  3.68s/it] 29%|██▊       | 48/168 [02:57<07:21,  3.68s/it] 29%|██▉       | 49/168 [03:01<07:18,  3.68s/it] 30%|██▉       | 50/168 [03:04<07:13,  3.67s/it]                                                {'loss': 0.2732, 'grad_norm': 0.8067046999931335, 'learning_rate': 8.86711374827494e-05, 'epoch': 0.89}
 30%|██▉       | 50/168 [03:04<07:13,  3.67s/it] 30%|███       | 51/168 [03:08<07:10,  3.68s/it] 31%|███       | 52/168 [03:12<07:07,  3.68s/it] 32%|███▏      | 53/168 [03:15<07:03,  3.68s/it] 32%|███▏      | 54/168 [03:19<06:59,  3.68s/it] 33%|███▎      | 55/168 [03:23<06:55,  3.67s/it] 33%|███▎      | 56/168 [03:26<06:51,  3.68s/it] 34%|███▍      | 57/168 [03:30<06:49,  3.69s/it] 35%|███▍      | 58/168 [03:34<06:45,  3.68s/it] 35%|███▌      | 59/168 [03:37<06:41,  3.68s/it] 36%|███▌      | 60/168 [03:41<06:38,  3.69s/it]                                                {'loss': 0.2204, 'grad_norm': 0.9666112661361694, 'learning_rate': 8.129053936203687e-05, 'epoch': 1.07}
 36%|███▌      | 60/168 [03:41<06:38,  3.69s/it] 36%|███▋      | 61/168 [03:45<06:34,  3.68s/it] 37%|███▋      | 62/168 [03:49<06:30,  3.68s/it] 38%|███▊      | 63/168 [03:52<06:26,  3.69s/it] 38%|███▊      | 64/168 [03:56<06:22,  3.68s/it] 39%|███▊      | 65/168 [04:00<06:19,  3.68s/it] 39%|███▉      | 66/168 [04:03<06:16,  3.69s/it] 40%|███▉      | 67/168 [04:07<06:12,  3.69s/it] 40%|████      | 68/168 [04:11<06:09,  3.69s/it] 41%|████      | 69/168 [04:14<06:04,  3.68s/it] 42%|████▏     | 70/168 [04:18<06:01,  3.69s/it]                                                {'loss': 0.2235, 'grad_norm': 0.38219892978668213, 'learning_rate': 7.256038257695687e-05, 'epoch': 1.24}
 42%|████▏     | 70/168 [04:18<06:01,  3.69s/it] 42%|████▏     | 71/168 [04:22<05:57,  3.68s/it] 43%|████▎     | 72/168 [04:25<05:53,  3.69s/it] 43%|████▎     | 73/168 [04:29<05:49,  3.68s/it] 44%|████▍     | 74/168 [04:33<05:46,  3.69s/it] 45%|████▍     | 75/168 [04:36<05:43,  3.69s/it] 45%|████▌     | 76/168 [04:40<05:39,  3.69s/it] 46%|████▌     | 77/168 [04:44<05:35,  3.68s/it] 46%|████▋     | 78/168 [04:48<05:31,  3.68s/it] 47%|████▋     | 79/168 [04:51<05:27,  3.68s/it] 48%|████▊     | 80/168 [04:55<05:25,  3.70s/it]                                                {'loss': 0.1967, 'grad_norm': 0.48992446064949036, 'learning_rate': 6.28571981484123e-05, 'epoch': 1.42}
 48%|████▊     | 80/168 [04:55<05:25,  3.70s/it] 48%|████▊     | 81/168 [04:59<05:21,  3.70s/it] 49%|████▉     | 82/168 [05:02<05:16,  3.69s/it] 49%|████▉     | 83/168 [05:06<05:13,  3.69s/it] 50%|█████     | 84/168 [05:10<05:09,  3.68s/it] 51%|█████     | 85/168 [05:13<05:05,  3.69s/it] 51%|█████     | 86/168 [05:17<05:02,  3.69s/it] 52%|█████▏    | 87/168 [05:21<04:58,  3.69s/it] 52%|█████▏    | 88/168 [05:24<04:55,  3.69s/it] 53%|█████▎    | 89/168 [05:28<04:51,  3.69s/it] 54%|█████▎    | 90/168 [05:32<04:47,  3.69s/it]                                                {'loss': 0.2325, 'grad_norm': 0.7209044694900513, 'learning_rate': 5.2599483708099016e-05, 'epoch': 1.6}
 54%|█████▎    | 90/168 [05:32<04:47,  3.69s/it] 54%|█████▍    | 91/168 [05:35<04:43,  3.68s/it] 55%|█████▍    | 92/168 [05:39<04:39,  3.68s/it] 55%|█████▌    | 93/168 [05:43<04:35,  3.67s/it] 56%|█████▌    | 94/168 [05:46<04:32,  3.68s/it] 57%|█████▋    | 95/168 [05:50<04:29,  3.69s/it] 57%|█████▋    | 96/168 [05:54<04:25,  3.69s/it] 58%|█████▊    | 97/168 [05:58<04:21,  3.68s/it] 58%|█████▊    | 98/168 [06:01<04:18,  3.69s/it] 59%|█████▉    | 99/168 [06:05<04:14,  3.69s/it] 60%|█████▉    | 100/168 [06:09<04:11,  3.70s/it]                                                 {'loss': 0.1987, 'grad_norm': 0.6829379796981812, 'learning_rate': 4.2229653726389765e-05, 'epoch': 1.78}
 60%|█████▉    | 100/168 [06:09<04:11,  3.70s/it] 60%|██████    | 101/168 [06:12<04:07,  3.69s/it] 61%|██████    | 102/168 [06:16<04:03,  3.69s/it] 61%|██████▏   | 103/168 [06:20<03:59,  3.69s/it] 62%|██████▏   | 104/168 [06:23<03:56,  3.69s/it] 62%|██████▎   | 105/168 [06:27<03:52,  3.69s/it] 63%|██████▎   | 106/168 [06:31<03:48,  3.68s/it] 64%|██████▎   | 107/168 [06:34<03:44,  3.68s/it] 64%|██████▍   | 108/168 [06:38<03:40,  3.68s/it] 65%|██████▍   | 109/168 [06:42<03:36,  3.67s/it] 65%|██████▌   | 110/168 [06:45<03:33,  3.68s/it]                                                 {'loss': 0.1842, 'grad_norm': 0.6096889972686768, 'learning_rate': 3.219495820872265e-05, 'epoch': 1.96}
 65%|██████▌   | 110/168 [06:45<03:33,  3.68s/it] 66%|██████▌   | 111/168 [06:49<03:29,  3.68s/it] 67%|██████▋   | 112/168 [06:53<03:25,  3.67s/it] 67%|██████▋   | 113/168 [06:56<03:22,  3.68s/it] 68%|██████▊   | 114/168 [07:00<03:19,  3.69s/it] 68%|██████▊   | 115/168 [07:04<03:15,  3.68s/it] 69%|██████▉   | 116/168 [07:08<03:11,  3.68s/it] 70%|██████▉   | 117/168 [07:11<03:07,  3.69s/it] 70%|███████   | 118/168 [07:15<03:03,  3.68s/it] 71%|███████   | 119/168 [07:19<03:00,  3.68s/it] 71%|███████▏  | 120/168 [07:22<02:56,  3.68s/it]                                                 {'loss': 0.1592, 'grad_norm': 0.40244436264038086, 'learning_rate': 2.2928192835717644e-05, 'epoch': 2.13}
 71%|███████▏  | 120/168 [07:22<02:56,  3.68s/it] 72%|███████▏  | 121/168 [07:26<02:53,  3.68s/it] 73%|███████▎  | 122/168 [07:30<02:49,  3.68s/it] 73%|███████▎  | 123/168 [07:33<02:45,  3.69s/it] 74%|███████▍  | 124/168 [07:37<02:42,  3.69s/it] 74%|███████▍  | 125/168 [07:41<02:38,  3.70s/it] 75%|███████▌  | 126/168 [07:44<02:35,  3.69s/it] 76%|███████▌  | 127/168 [07:48<02:31,  3.69s/it] 76%|███████▌  | 128/168 [07:52<02:27,  3.68s/it] 77%|███████▋  | 129/168 [07:55<02:23,  3.68s/it] 77%|███████▋  | 130/168 [07:59<02:20,  3.69s/it]                                                 {'loss': 0.156, 'grad_norm': 0.29536303877830505, 'learning_rate': 1.4829032517260489e-05, 'epoch': 2.31}
 77%|███████▋  | 130/168 [07:59<02:20,  3.69s/it] 78%|███████▊  | 131/168 [08:03<02:16,  3.69s/it] 79%|███████▊  | 132/168 [08:07<02:12,  3.68s/it] 79%|███████▉  | 133/168 [08:10<02:09,  3.69s/it] 80%|███████▉  | 134/168 [08:14<02:05,  3.69s/it] 80%|████████  | 135/168 [08:18<02:01,  3.69s/it] 81%|████████  | 136/168 [08:21<01:58,  3.69s/it] 82%|████████▏ | 137/168 [08:25<01:54,  3.69s/it] 82%|████████▏ | 138/168 [08:29<01:51,  3.70s/it] 83%|████████▎ | 139/168 [08:32<01:47,  3.72s/it] 83%|████████▎ | 140/168 [08:36<01:43,  3.70s/it]                                                 {'loss': 0.1786, 'grad_norm': 0.5050375461578369, 'learning_rate': 8.246793442995954e-06, 'epoch': 2.49}
 83%|████████▎ | 140/168 [08:36<01:43,  3.70s/it] 84%|████████▍ | 141/168 [08:40<01:40,  3.73s/it] 85%|████████▍ | 142/168 [08:44<01:36,  3.72s/it] 85%|████████▌ | 143/168 [08:47<01:32,  3.71s/it] 86%|████████▌ | 144/168 [08:51<01:28,  3.70s/it] 86%|████████▋ | 145/168 [08:55<01:25,  3.70s/it] 87%|████████▋ | 146/168 [08:58<01:21,  3.69s/it] 88%|████████▊ | 147/168 [09:02<01:17,  3.68s/it] 88%|████████▊ | 148/168 [09:06<01:13,  3.68s/it] 89%|████████▊ | 149/168 [09:09<01:09,  3.68s/it] 89%|████████▉ | 150/168 [09:13<01:06,  3.69s/it]                                                 {'loss': 0.1821, 'grad_norm': 0.46062856912612915, 'learning_rate': 3.465367100725908e-06, 'epoch': 2.67}
 89%|████████▉ | 150/168 [09:13<01:06,  3.69s/it] 90%|████████▉ | 151/168 [09:17<01:02,  3.70s/it] 90%|█████████ | 152/168 [09:20<00:59,  3.69s/it] 91%|█████████ | 153/168 [09:24<00:55,  3.69s/it] 92%|█████████▏| 154/168 [09:28<00:51,  3.67s/it] 92%|█████████▏| 155/168 [09:31<00:47,  3.67s/it] 93%|█████████▎| 156/168 [09:35<00:44,  3.67s/it] 93%|█████████▎| 157/168 [09:39<00:40,  3.69s/it] 94%|█████████▍| 158/168 [09:43<00:36,  3.68s/it] 95%|█████████▍| 159/168 [09:46<00:33,  3.68s/it] 95%|█████████▌| 160/168 [09:50<00:29,  3.69s/it]                                                 {'loss': 0.1958, 'grad_norm': 0.45572641491889954, 'learning_rate': 6.909760573925561e-07, 'epoch': 2.84}
 95%|█████████▌| 160/168 [09:50<00:29,  3.69s/it] 96%|█████████▌| 161/168 [09:54<00:25,  3.68s/it] 96%|█████████▋| 162/168 [09:57<00:22,  3.67s/it] 97%|█████████▋| 163/168 [10:01<00:18,  3.68s/it] 98%|█████████▊| 164/168 [10:05<00:14,  3.67s/it] 98%|█████████▊| 165/168 [10:08<00:11,  3.67s/it] 99%|█████████▉| 166/168 [10:12<00:07,  3.68s/it] 99%|█████████▉| 167/168 [10:16<00:03,  3.68s/it]100%|██████████| 168/168 [10:19<00:00,  3.68s/it][INFO|trainer.py:3705] 2024-11-18 22:49:04,162 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/checkpoint-168
[INFO|configuration_utils.py:673] 2024-11-18 22:49:04,199 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:04,201 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:49:04,367 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/checkpoint-168/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:49:04,368 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/checkpoint-168/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 22:49:05,082 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 621.4392, 'train_samples_per_second': 4.345, 'train_steps_per_second': 0.27, 'train_loss': 0.36178304184050786, 'epoch': 2.99}
100%|██████████| 168/168 [10:20<00:00,  3.68s/it]100%|██████████| 168/168 [10:20<00:00,  3.69s/it]
[INFO|trainer.py:3705] 2024-11-18 22:49:05,101 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 22:49:05,133 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:05,134 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 22:49:05,294 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 22:49:05,294 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9867
  total_flos               = 106206322GF
  train_loss               =      0.3618
  train_runtime            =  0:10:21.43
  train_samples_per_second =       4.345
  train_steps_per_second   =        0.27
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_8_2/lora/sft/training_loss.png
11/18/2024 22:49:05 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 22:49:05 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 22:49:05,758 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 22:49:05,758 >>   Num examples = 100
[INFO|trainer.py:4026] 2024-11-18 22:49:05,758 >>   Batch size = 1
  0%|          | 0/50 [00:00<?, ?it/s]  4%|▍         | 2/50 [00:00<00:03, 15.01it/s]  8%|▊         | 4/50 [00:00<00:04,  9.84it/s] 12%|█▏        | 6/50 [00:00<00:04,  8.85it/s] 14%|█▍        | 7/50 [00:00<00:04,  8.64it/s] 16%|█▌        | 8/50 [00:00<00:04,  8.41it/s] 18%|█▊        | 9/50 [00:01<00:04,  8.36it/s] 20%|██        | 10/50 [00:01<00:04,  8.25it/s] 22%|██▏       | 11/50 [00:01<00:04,  8.22it/s] 24%|██▍       | 12/50 [00:01<00:04,  8.16it/s] 26%|██▌       | 13/50 [00:01<00:04,  8.15it/s] 28%|██▊       | 14/50 [00:01<00:04,  8.05it/s] 30%|███       | 15/50 [00:01<00:04,  8.06it/s] 32%|███▏      | 16/50 [00:01<00:04,  8.10it/s] 34%|███▍      | 17/50 [00:02<00:04,  8.10it/s] 36%|███▌      | 18/50 [00:02<00:04,  7.98it/s] 38%|███▊      | 19/50 [00:02<00:03,  7.93it/s] 40%|████      | 20/50 [00:02<00:03,  7.93it/s] 42%|████▏     | 21/50 [00:02<00:03,  7.99it/s] 44%|████▍     | 22/50 [00:02<00:03,  7.90it/s] 46%|████▌     | 23/50 [00:02<00:03,  7.98it/s] 48%|████▊     | 24/50 [00:02<00:03,  8.05it/s] 50%|█████     | 25/50 [00:03<00:03,  8.05it/s] 52%|█████▏    | 26/50 [00:03<00:02,  8.09it/s] 54%|█████▍    | 27/50 [00:03<00:02,  8.12it/s] 56%|█████▌    | 28/50 [00:03<00:02,  8.05it/s] 58%|█████▊    | 29/50 [00:03<00:02,  8.09it/s] 60%|██████    | 30/50 [00:03<00:02,  8.02it/s] 62%|██████▏   | 31/50 [00:03<00:02,  8.05it/s] 64%|██████▍   | 32/50 [00:03<00:02,  7.98it/s] 66%|██████▌   | 33/50 [00:04<00:02,  8.04it/s] 68%|██████▊   | 34/50 [00:04<00:01,  8.02it/s] 70%|███████   | 35/50 [00:04<00:01,  8.07it/s] 72%|███████▏  | 36/50 [00:04<00:01,  8.11it/s] 74%|███████▍  | 37/50 [00:04<00:01,  8.10it/s] 76%|███████▌  | 38/50 [00:04<00:01,  7.97it/s] 78%|███████▊  | 39/50 [00:04<00:01,  8.03it/s] 80%|████████  | 40/50 [00:04<00:01,  8.01it/s] 82%|████████▏ | 41/50 [00:05<00:01,  7.95it/s] 84%|████████▍ | 42/50 [00:05<00:00,  8.02it/s] 86%|████████▌ | 43/50 [00:05<00:00,  7.97it/s] 88%|████████▊ | 44/50 [00:05<00:00,  8.03it/s] 90%|█████████ | 45/50 [00:05<00:00,  8.06it/s] 92%|█████████▏| 46/50 [00:05<00:00,  8.09it/s] 94%|█████████▍| 47/50 [00:05<00:00,  8.12it/s] 96%|█████████▌| 48/50 [00:05<00:00,  8.07it/s] 98%|█████████▊| 49/50 [00:05<00:00,  7.99it/s]100%|██████████| 50/50 [00:06<00:00,  8.09it/s]100%|██████████| 50/50 [00:06<00:00,  8.17it/s]
***** eval metrics *****
  epoch                   =     2.9867
  eval_loss               =     0.2068
  eval_runtime            = 0:00:06.25
  eval_samples_per_second =     15.979
  eval_steps_per_second   =      7.989
[INFO|modelcard.py:449] 2024-11-18 22:49:12,017 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:49:33,179] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:49:37 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:25402
[2024-11-18 22:49:39,449] torch.distributed.run: [WARNING] 
[2024-11-18 22:49:39,449] torch.distributed.run: [WARNING] *****************************************
[2024-11-18 22:49:39,449] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-11-18 22:49:39,449] torch.distributed.run: [WARNING] *****************************************
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/latest owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
/home/liuyang/miniconda3/envs/pytorch21/lib/python3.9/site-packages/torch_npu/utils/path_manager.py:82: UserWarning: Warning: The /usr/local/Ascend/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux/ascend_toolkit_install.info owner does not match the current user.
  warnings.warn(f"Warning: The {path} owner does not match the current user.")
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())
[2024-11-18 22:49:46,492] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
[2024-11-18 22:49:46,972] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
11/18/2024 22:49:48 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:49:48 - INFO - llamafactory.hparams.parser - Process rank: 1, device: npu:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
11/18/2024 22:49:48 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
11/18/2024 22:49:48 - INFO - llamafactory.hparams.parser - Process rank: 0, device: npu:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:673] 2024-11-18 22:49:48,304 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:48,305 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,307 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,307 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,307 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,307 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,308 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,308 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:49:48,762 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:673] 2024-11-18 22:49:48,763 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:48,764 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,765 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,765 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,765 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,765 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,766 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2204] 2024-11-18 22:49:48,766 >> loading file tokenizer_config.json
11/18/2024 22:49:48 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:49:48 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
[INFO|tokenization_utils_base.py:2470] 2024-11-18 22:49:49,217 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
11/18/2024 22:49:49 - WARNING - llamafactory.model.loader - Processor was not found: 'Qwen2Config' object has no attribute 'vision_config'.
11/18/2024 22:49:49 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
11/18/2024 22:49:49 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_9_1.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1152 examples [00:00, 12492.96 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:02, 447.57 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3264.78 examples/s]
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
[W compiler_depend.ts:190] Warning: 0Failed to find function aclrtCreateEventExWithFlag (function operator())
11/18/2024 22:49:53 - INFO - llamafactory.data.loader - Loading dataset scala_with_schema_vector_prompt_train_9_1.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:01<00:19, 48.62 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:01<00:08, 97.84 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 189/1000 [00:01<00:05, 144.31 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 252/1000 [00:01<00:04, 183.76 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:02<00:03, 219.53 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 378/1000 [00:02<00:02, 244.51 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:02<00:02, 265.54 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 504/1000 [00:02<00:01, 280.13 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 628/1000 [00:03<00:01, 299.11 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:03<00:00, 367.16 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:03<00:00, 360.75 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:03<00:00, 354.58 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:03<00:00, 352.58 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:03<00:00, 369.62 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:04<00:00, 246.16 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 36667, 52183, 74393, 9370, 17349, 27369, 104506, 28311, 4913, 16900, 788, 4383, 99319, 9370, 31905, 17714, 5122, 3586, 2124, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 2203, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2203, 497, 330, 99319, 9370, 31905, 17714, 5122, 21034, 1055, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 25039, 6182, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 6182, 497, 330, 99319, 9370, 31905, 17714, 5122, 54965, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 1778, 266, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 57804, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 9597, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 2593, 39522, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 32398, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 32034, 82, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 8987, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8462, 11, 105756, 31905, 17714, 25, 57804, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 39463, 258, 8987, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 285, 4480, 1055, 11, 105756, 31905, 17714, 25, 2007, 11, 111557, 31905, 17714, 25, 2007, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 22585, 11, 105756, 31905, 17714, 25, 22585, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 2203, 11, 105756, 31905, 17714, 25, 2203, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 559, 351, 6182, 11, 105756, 31905, 17714, 25, 6182, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 4648, 12724, 11, 105756, 31905, 17714, 25, 8987, 11, 111557, 31905, 17714, 25, 4578, 497, 330, 99319, 9370, 31905, 17714, 5122, 71, 21754, 11, 105756, 31905, 17714, 25, 4578, 11, 111557, 31905, 17714, 25, 4578, 1040, 497, 330, 99319, 9370, 31905, 17714, 5122, 1038, 392, 1040, 1055, 11, 105756, 31905, 17714, 25, 4578, 1040, 11, 111557, 31905, 17714, 25, 4578, 1040, 7914, 330, 20008, 788, 4383, 92374, 31905, 25, 6182, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 2007, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 8987, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2332, 516, 364, 12968, 516, 364, 51265, 516, 364, 11528, 516, 364, 29206, 516, 364, 28425, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 57804, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 516, 364, 1313, 4089, 11583, 330, 92374, 31905, 25, 22585, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 2102, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 11583, 330, 92374, 31905, 25, 2203, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 4129, 516, 364, 1796, 516, 364, 11528, 516, 364, 1805, 1192, 516, 364, 2527, 573, 516, 364, 22468, 2591, 516, 364, 37375, 1028, 4089, 11583, 330, 92374, 31905, 25, 4578, 1040, 11, 102298, 79256, 27369, 3269, 677, 307, 516, 364, 1085, 516, 364, 606, 4089, 8, 92010, 36667, 52183, 74393, 15946, 47606, 87752, 20074, 27369, 510, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 16, 315, 63990, 48435, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 16, 315, 59677, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 22585, 11, 79256, 2102, 1131, 32378, 220, 18, 16, 315, 63990, 48435, 1752, 283, 12982, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 37, 585, 404, 1245, 56472, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 34667, 291, 10598, 309, 278, 8347, 258, 1566, 69, 15708, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 57804, 11, 79256, 606, 1131, 1427, 309, 2039, 436, 48435, 62, 30172, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 24765, 2061, 24998, 1098, 530, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 37, 349, 71, 12164, 71, 1098, 1466, 579, 6, 9370, 92374, 198, 1502, 17714, 2007, 11, 79256, 606, 1131, 49, 1466, 318, 10626, 277, 10102, 9917, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 785, 1400, 13299, 79790, 40812, 5377, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 606, 1131, 852, 3575, 2568, 1466, 55, 23544, 263, 53241, 82, 6, 9370, 92374, 198, 1502, 17714, 4578, 11, 79256, 6347, 320, 77, 16, 25, 22585, 8, 220, 1380, 308, 16, 6067, 1131, 32378, 220, 16, 315, 63990, 48435, 1752, 283, 12982, 6, 470, 308, 16, 97506, 1028, 11, 308, 16, 6067, 11, 308, 16, 1764, 3930, 220, 20, 26, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
已知数据库的schema信息如下：
{"edges": ["边的类型为：containerOf,起点类型为:forum,终点类型为:post", "边的类型为：replyofpost,起点类型为:post,终点类型为:post", "边的类型为：likespost,起点类型为:person,终点类型为:post", "边的类型为：replyofcomment,起点类型为:comment,终点类型为:comment", "边的类型为：likescomment,起点类型为:person,终点类型为:comment", "边的类型为：studyat,起点类型为:person,终点类型为:organisation", "边的类型为：workat,起点类型为:person,终点类型为:organisation", "边的类型为：hasmember,起点类型为:forum,终点类型为:person", "边的类型为：hasmoderator,起点类型为:forum,终点类型为:person", "边的类型为：hascreatorpost,起点类型为:post,终点类型为:person", "边的类型为：hascreatorcomment,起点类型为:comment,终点类型为:person", "边的类型为：knows,起点类型为:person,终点类型为:person", "边的类型为：islocatedinpost,起点类型为:post,终点类型为:place", "边的类型为：islocatedincomment,起点类型为:comment,终点类型为:place", "边的类型为：islocatedinorgan,起点类型为:organisation,终点类型为:place", "边的类型为：islocatedinperson,起点类型为:person,终点类型为:place", "边的类型为：ispartof,起点类型为:place,终点类型为:place", "边的类型为：hastagforum,起点类型为:forum,终点类型为:tag", "边的类型为：hastagpost,起点类型为:post,终点类型为:tag", "边的类型为：hastagcomment,起点类型为:comment,终点类型为:tag", "边的类型为：hasinterest,起点类型为:person,终点类型为:tag", "边的类型为：hastype,起点类型为:tag,终点类型为:tagclass", "边的类型为：issubclassof,起点类型为:tagclass,终点类型为:tagclass"], "nodes": ["节点类型:comment,包含属性信息:(['id', 'length', 'content', 'locationip', 'browserused', 'creationdate'],)", "节点类型:place,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:person,包含属性信息:(['id', 'email', 'gender', 'birthday', 'language', 'lastname', 'firstname', 'locationip', 'browserused', 'creationdate'],)", "节点类型:organisation,包含属性信息:(['id', 'url', 'name', 'type'],)", "节点类型:forum,包含属性信息:(['id', 'title', 'creationdate'],)", "节点类型:tag,包含属性信息:(['id', 'url', 'name'],)", "节点类型:post,包含属性信息:(['id', 'length', 'content', 'language', 'imagefile', 'locationip', 'browserused', 'creationdate'],)", "节点类型:tagclass,包含属性信息:(['id', 'url', 'name'],)"]}
已知数据库中存在以下数据信息:
label为forum,属性title='Album 1 of Hossein Forouhar'的节点
label为forum,属性title='Album 1 of Hassan Forouhar'的节点
label为forum,属性title='Album 31 of Hossein Forouhar'的节点
label为organisation,属性name='Fakir_Mohan_University'的节点
label为organisation,属性name='Syed_Jamaluddin_Afghan_University'的节点
label为organisation,属性name='Imam_Hossein_University'的节点
label为place,属性name='Ahmedpur_Sial'的节点
label为place,属性name='Fatehgarh_Sahib'的节点
label为place,属性name='Rahim_Yar_Khan'的节点
label为tag,属性name='The_Fugs_First_Album'的节点
label为tag,属性name='List_of_RahXephon_albums'的节点
label为tag,属性match (n1:forum)  where n1.title='Album 1 of Hossein Forouhar' return n1.creationdate, n1.title, n1.id limit 5;<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6347, 320, 77, 16, 25, 22585, 8, 220, 1380, 308, 16, 6067, 1131, 32378, 220, 16, 315, 63990, 48435, 1752, 283, 12982, 6, 470, 308, 16, 97506, 1028, 11, 308, 16, 6067, 11, 308, 16, 1764, 3930, 220, 20, 26, 151645]
labels:
match (n1:forum)  where n1.title='Album 1 of Hossein Forouhar' return n1.creationdate, n1.title, n1.id limit 5;<|im_end|>
[INFO|configuration_utils.py:673] 2024-11-18 22:49:58,298 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 22:49:58,299 >> Model config Qwen2Config {
  "_name_or_path": "/home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:3729] 2024-11-18 22:49:58,355 >> loading weights file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-11-18 22:49:58,356 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-11-18 22:49:58,358 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
[INFO|modeling_utils.py:4574] 2024-11-18 22:50:02,322 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4582] 2024-11-18 22:50:02,323 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1052] 2024-11-18 22:50:02,327 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:1099] 2024-11-18 22:50:02,327 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

11/18/2024 22:50:02 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:50:02 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:50:02 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:50:02 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:50:02 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,gate_proj,v_proj,down_proj,q_proj,up_proj,k_proj
11/18/2024 22:50:02 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-11-18 22:50:03,027 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]
11/18/2024 22:50:03 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
11/18/2024 22:50:03 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.
11/18/2024 22:50:03 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
11/18/2024 22:50:03 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
11/18/2024 22:50:03 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,o_proj,up_proj,q_proj,gate_proj,k_proj,down_proj
11/18/2024 22:50:03 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:2243] 2024-11-18 22:50:04,235 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-11-18 22:50:04,235 >>   Num examples = 900
[INFO|trainer.py:2245] 2024-11-18 22:50:04,235 >>   Num Epochs = 3
[INFO|trainer.py:2246] 2024-11-18 22:50:04,235 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2249] 2024-11-18 22:50:04,235 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2250] 2024-11-18 22:50:04,235 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2251] 2024-11-18 22:50:04,235 >>   Total optimization steps = 168
[INFO|trainer.py:2252] 2024-11-18 22:50:04,241 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/168 [00:00<?, ?it/s]  1%|          | 1/168 [00:04<11:31,  4.14s/it]  1%|          | 2/168 [00:07<10:47,  3.90s/it]  2%|▏         | 3/168 [00:11<10:28,  3.81s/it]  2%|▏         | 4/168 [00:15<10:19,  3.77s/it]  3%|▎         | 5/168 [00:19<10:11,  3.75s/it]  4%|▎         | 6/168 [00:22<10:06,  3.74s/it]  4%|▍         | 7/168 [00:26<09:59,  3.72s/it]  5%|▍         | 8/168 [00:30<09:55,  3.72s/it]  5%|▌         | 9/168 [00:33<09:51,  3.72s/it]  6%|▌         | 10/168 [00:37<09:48,  3.72s/it]                                                {'loss': 1.7935, 'grad_norm': 2.0935962200164795, 'learning_rate': 5.882352941176471e-05, 'epoch': 0.18}
  6%|▌         | 10/168 [00:37<09:48,  3.72s/it]  7%|▋         | 11/168 [00:41<09:45,  3.73s/it]  7%|▋         | 12/168 [00:45<09:40,  3.72s/it]  8%|▊         | 13/168 [00:48<09:35,  3.71s/it]  8%|▊         | 14/168 [00:52<09:31,  3.71s/it]  9%|▉         | 15/168 [00:56<09:26,  3.70s/it] 10%|▉         | 16/168 [00:59<09:21,  3.70s/it] 10%|█         | 17/168 [01:03<09:17,  3.69s/it] 11%|█         | 18/168 [01:07<09:13,  3.69s/it] 11%|█▏        | 19/168 [01:10<09:10,  3.69s/it] 12%|█▏        | 20/168 [01:14<09:08,  3.71s/it]                                                {'loss': 0.9417, 'grad_norm': 1.0820163488388062, 'learning_rate': 9.990263847374976e-05, 'epoch': 0.36}
 12%|█▏        | 20/168 [01:14<09:08,  3.71s/it] 12%|█▎        | 21/168 [01:18<09:04,  3.71s/it] 13%|█▎        | 22/168 [01:21<09:00,  3.70s/it] 14%|█▎        | 23/168 [01:25<08:56,  3.70s/it] 14%|█▍        | 24/168 [01:29<08:51,  3.69s/it] 15%|█▍        | 25/168 [01:33<08:47,  3.69s/it] 15%|█▌        | 26/168 [01:36<08:44,  3.69s/it] 16%|█▌        | 27/168 [01:40<08:47,  3.74s/it] 17%|█▋        | 28/168 [01:44<08:42,  3.73s/it] 17%|█▋        | 29/168 [01:47<08:36,  3.72s/it] 18%|█▊        | 30/168 [01:51<08:30,  3.70s/it]                                                {'loss': 0.4487, 'grad_norm': 0.8037834763526917, 'learning_rate': 9.818229479678158e-05, 'epoch': 0.53}
 18%|█▊        | 30/168 [01:51<08:30,  3.70s/it] 18%|█▊        | 31/168 [01:55<08:26,  3.70s/it] 19%|█▉        | 32/168 [01:59<08:21,  3.69s/it] 20%|█▉        | 33/168 [02:02<08:18,  3.69s/it] 20%|██        | 34/168 [02:06<08:15,  3.70s/it] 21%|██        | 35/168 [02:10<08:11,  3.70s/it] 21%|██▏       | 36/168 [02:13<08:07,  3.69s/it] 22%|██▏       | 37/168 [02:17<08:02,  3.69s/it] 23%|██▎       | 38/168 [02:21<07:59,  3.69s/it] 23%|██▎       | 39/168 [02:24<07:55,  3.69s/it] 24%|██▍       | 40/168 [02:28<07:51,  3.68s/it]                                                {'loss': 0.2898, 'grad_norm': 0.8453037142753601, 'learning_rate': 9.438385228425938e-05, 'epoch': 0.71}
 24%|██▍       | 40/168 [02:28<07:51,  3.68s/it] 24%|██▍       | 41/168 [02:32<07:48,  3.69s/it] 25%|██▌       | 42/168 [02:35<07:45,  3.69s/it] 26%|██▌       | 43/168 [02:39<07:41,  3.69s/it] 26%|██▌       | 44/168 [02:43<07:38,  3.70s/it] 27%|██▋       | 45/168 [02:47<07:34,  3.69s/it] 27%|██▋       | 46/168 [02:50<07:30,  3.69s/it] 28%|██▊       | 47/168 [02:54<07:26,  3.69s/it] 29%|██▊       | 48/168 [02:58<07:22,  3.69s/it] 29%|██▉       | 49/168 [03:01<07:18,  3.68s/it] 30%|██▉       | 50/168 [03:05<07:15,  3.69s/it]                                                {'loss': 0.2602, 'grad_norm': 0.5354212522506714, 'learning_rate': 8.86711374827494e-05, 'epoch': 0.89}
 30%|██▉       | 50/168 [03:05<07:15,  3.69s/it] 30%|███       | 51/168 [03:09<07:11,  3.69s/it] 31%|███       | 52/168 [03:12<07:08,  3.69s/it] 32%|███▏      | 53/168 [03:16<07:04,  3.69s/it] 32%|███▏      | 54/168 [03:20<07:02,  3.70s/it] 33%|███▎      | 55/168 [03:24<07:00,  3.72s/it] 33%|███▎      | 56/168 [03:27<06:56,  3.72s/it] 34%|███▍      | 57/168 [03:31<06:53,  3.72s/it] 35%|███▍      | 58/168 [03:35<06:48,  3.72s/it] 35%|███▌      | 59/168 [03:38<06:44,  3.71s/it] 36%|███▌      | 60/168 [03:42<06:39,  3.70s/it]                                                {'loss': 0.2316, 'grad_norm': 0.5188742280006409, 'learning_rate': 8.129053936203687e-05, 'epoch': 1.07}
 36%|███▌      | 60/168 [03:42<06:39,  3.70s/it] 36%|███▋      | 61/168 [03:46<06:36,  3.70s/it] 37%|███▋      | 62/168 [03:49<06:31,  3.69s/it] 38%|███▊      | 63/168 [03:53<06:27,  3.69s/it] 38%|███▊      | 64/168 [03:57<06:23,  3.69s/it] 39%|███▊      | 65/168 [04:00<06:20,  3.69s/it] 39%|███▉      | 66/168 [04:04<06:16,  3.69s/it] 40%|███▉      | 67/168 [04:08<06:12,  3.69s/it] 40%|████      | 68/168 [04:12<06:10,  3.71s/it] 41%|████      | 69/168 [04:15<06:06,  3.70s/it] 42%|████▏     | 70/168 [04:19<06:02,  3.70s/it]                                                {'loss': 0.2239, 'grad_norm': 0.44783350825309753, 'learning_rate': 7.256038257695687e-05, 'epoch': 1.24}
 42%|████▏     | 70/168 [04:19<06:02,  3.70s/it] 42%|████▏     | 71/168 [04:23<05:59,  3.70s/it] 43%|████▎     | 72/168 [04:26<05:54,  3.70s/it] 43%|████▎     | 73/168 [04:30<05:50,  3.69s/it] 44%|████▍     | 74/168 [04:34<05:47,  3.69s/it] 45%|████▍     | 75/168 [04:37<05:44,  3.70s/it] 45%|████▌     | 76/168 [04:41<05:40,  3.70s/it] 46%|████▌     | 77/168 [04:45<05:36,  3.70s/it] 46%|████▋     | 78/168 [04:49<05:32,  3.69s/it] 47%|████▋     | 79/168 [04:52<05:28,  3.69s/it] 48%|████▊     | 80/168 [04:56<05:24,  3.69s/it]                                                {'loss': 0.1933, 'grad_norm': 0.4874130189418793, 'learning_rate': 6.28571981484123e-05, 'epoch': 1.42}
 48%|████▊     | 80/168 [04:56<05:24,  3.69s/it] 48%|████▊     | 81/168 [05:00<05:21,  3.70s/it] 49%|████▉     | 82/168 [05:03<05:18,  3.70s/it] 49%|████▉     | 83/168 [05:07<05:14,  3.70s/it] 50%|█████     | 84/168 [05:11<05:09,  3.69s/it] 51%|█████     | 85/168 [05:14<05:06,  3.69s/it] 51%|█████     | 86/168 [05:18<05:02,  3.69s/it] 52%|█████▏    | 87/168 [05:22<04:58,  3.69s/it] 52%|█████▏    | 88/168 [05:26<04:55,  3.70s/it] 53%|█████▎    | 89/168 [05:29<04:51,  3.69s/it] 54%|█████▎    | 90/168 [05:33<04:49,  3.71s/it]                                                {'loss': 0.1945, 'grad_norm': 0.4553923010826111, 'learning_rate': 5.2599483708099016e-05, 'epoch': 1.6}
 54%|█████▎    | 90/168 [05:33<04:49,  3.71s/it] 54%|█████▍    | 91/168 [05:37<04:45,  3.71s/it] 55%|█████▍    | 92/168 [05:40<04:41,  3.70s/it] 55%|█████▌    | 93/168 [05:44<04:37,  3.70s/it] 56%|█████▌    | 94/168 [05:48<04:33,  3.69s/it] 57%|█████▋    | 95/168 [05:51<04:29,  3.69s/it] 57%|█████▋    | 96/168 [05:55<04:25,  3.69s/it] 58%|█████▊    | 97/168 [05:59<04:22,  3.69s/it] 58%|█████▊    | 98/168 [06:02<04:18,  3.69s/it] 59%|█████▉    | 99/168 [06:06<04:14,  3.69s/it] 60%|█████▉    | 100/168 [06:10<04:12,  3.71s/it]                                                 {'loss': 0.2292, 'grad_norm': 0.4957793951034546, 'learning_rate': 4.2229653726389765e-05, 'epoch': 1.78}
 60%|█████▉    | 100/168 [06:10<04:12,  3.71s/it] 60%|██████    | 101/168 [06:14<04:09,  3.72s/it] 61%|██████    | 102/168 [06:17<04:05,  3.71s/it] 61%|██████▏   | 103/168 [06:21<04:01,  3.71s/it] 62%|██████▏   | 104/168 [06:25<03:57,  3.71s/it] 62%|██████▎   | 105/168 [06:28<03:53,  3.71s/it] 63%|██████▎   | 106/168 [06:32<03:49,  3.69s/it] 64%|██████▎   | 107/168 [06:36<03:45,  3.70s/it] 64%|██████▍   | 108/168 [06:39<03:41,  3.70s/it] 65%|██████▍   | 109/168 [06:43<03:37,  3.69s/it] 65%|██████▌   | 110/168 [06:47<03:34,  3.69s/it]                                                 {'loss': 0.2087, 'grad_norm': 0.4620382487773895, 'learning_rate': 3.219495820872265e-05, 'epoch': 1.96}
 65%|██████▌   | 110/168 [06:47<03:34,  3.69s/it] 66%|██████▌   | 111/168 [06:51<03:30,  3.70s/it] 67%|██████▋   | 112/168 [06:54<03:26,  3.69s/it] 67%|██████▋   | 113/168 [06:58<03:23,  3.70s/it] 68%|██████▊   | 114/168 [07:02<03:20,  3.71s/it] 68%|██████▊   | 115/168 [07:05<03:16,  3.70s/it] 69%|██████▉   | 116/168 [07:09<03:12,  3.70s/it] 70%|██████▉   | 117/168 [07:13<03:08,  3.69s/it] 70%|███████   | 118/168 [07:16<03:04,  3.69s/it] 71%|███████   | 119/168 [07:20<03:00,  3.69s/it] 71%|███████▏  | 120/168 [07:24<02:56,  3.69s/it]                                                 {'loss': 0.1768, 'grad_norm': 0.5914298295974731, 'learning_rate': 2.2928192835717644e-05, 'epoch': 2.13}
 71%|███████▏  | 120/168 [07:24<02:56,  3.69s/it] 72%|███████▏  | 121/168 [07:27<02:53,  3.68s/it] 73%|███████▎  | 122/168 [07:31<02:49,  3.68s/it] 73%|███████▎  | 123/168 [07:35<02:45,  3.68s/it] 74%|███████▍  | 124/168 [07:39<02:42,  3.69s/it] 74%|███████▍  | 125/168 [07:42<02:38,  3.68s/it] 75%|███████▌  | 126/168 [07:46<02:34,  3.68s/it] 76%|███████▌  | 127/168 [07:50<02:30,  3.68s/it] 76%|███████▌  | 128/168 [07:53<02:27,  3.68s/it] 77%|███████▋  | 129/168 [07:57<02:23,  3.68s/it] 77%|███████▋  | 130/168 [08:01<02:19,  3.68s/it]                                                 {'loss': 0.1592, 'grad_norm': 0.5157364010810852, 'learning_rate': 1.4829032517260489e-05, 'epoch': 2.31}
 77%|███████▋  | 130/168 [08:01<02:19,  3.68s/it] 78%|███████▊  | 131/168 [08:04<02:16,  3.69s/it] 79%|███████▊  | 132/168 [08:08<02:12,  3.68s/it] 79%|███████▉  | 133/168 [08:12<02:08,  3.68s/it] 80%|███████▉  | 134/168 [08:15<02:05,  3.68s/it] 80%|████████  | 135/168 [08:19<02:01,  3.69s/it] 81%|████████  | 136/168 [08:23<01:57,  3.68s/it] 82%|████████▏ | 137/168 [08:26<01:54,  3.70s/it] 82%|████████▏ | 138/168 [08:30<01:52,  3.73s/it] 83%|████████▎ | 139/168 [08:34<01:48,  3.75s/it] 83%|████████▎ | 140/168 [08:38<01:44,  3.74s/it]                                                 {'loss': 0.1619, 'grad_norm': 0.39488157629966736, 'learning_rate': 8.246793442995954e-06, 'epoch': 2.49}
 83%|████████▎ | 140/168 [08:38<01:44,  3.74s/it] 84%|████████▍ | 141/168 [08:41<01:40,  3.72s/it] 85%|████████▍ | 142/168 [08:45<01:36,  3.71s/it] 85%|████████▌ | 143/168 [08:49<01:32,  3.71s/it] 86%|████████▌ | 144/168 [08:53<01:28,  3.70s/it] 86%|████████▋ | 145/168 [08:56<01:25,  3.71s/it] 87%|████████▋ | 146/168 [09:00<01:21,  3.70s/it] 88%|████████▊ | 147/168 [09:04<01:17,  3.70s/it] 88%|████████▊ | 148/168 [09:07<01:13,  3.70s/it] 89%|████████▊ | 149/168 [09:11<01:10,  3.69s/it] 89%|████████▉ | 150/168 [09:15<01:06,  3.69s/it]                                                 {'loss': 0.18, 'grad_norm': 0.5295692086219788, 'learning_rate': 3.465367100725908e-06, 'epoch': 2.67}
 89%|████████▉ | 150/168 [09:15<01:06,  3.69s/it] 90%|████████▉ | 151/168 [09:18<01:02,  3.69s/it] 90%|█████████ | 152/168 [09:22<00:59,  3.69s/it] 91%|█████████ | 153/168 [09:26<00:55,  3.69s/it] 92%|█████████▏| 154/168 [09:29<00:51,  3.69s/it] 92%|█████████▏| 155/168 [09:33<00:47,  3.68s/it] 93%|█████████▎| 156/168 [09:37<00:44,  3.69s/it] 93%|█████████▎| 157/168 [09:41<00:40,  3.70s/it] 94%|█████████▍| 158/168 [09:44<00:36,  3.69s/it] 95%|█████████▍| 159/168 [09:48<00:33,  3.69s/it] 95%|█████████▌| 160/168 [09:52<00:29,  3.69s/it]                                                 {'loss': 0.1773, 'grad_norm': 0.5237472653388977, 'learning_rate': 6.909760573925561e-07, 'epoch': 2.84}
 95%|█████████▌| 160/168 [09:52<00:29,  3.69s/it] 96%|█████████▌| 161/168 [09:55<00:25,  3.70s/it] 96%|█████████▋| 162/168 [09:59<00:22,  3.72s/it] 97%|█████████▋| 163/168 [10:03<00:18,  3.74s/it] 98%|█████████▊| 164/168 [10:07<00:15,  3.76s/it] 98%|█████████▊| 165/168 [10:10<00:11,  3.76s/it] 99%|█████████▉| 166/168 [10:14<00:07,  3.76s/it] 99%|█████████▉| 167/168 [10:18<00:03,  3.74s/it]100%|██████████| 168/168 [10:22<00:00,  3.73s/it][INFO|trainer.py:3705] 2024-11-18 23:00:27,071 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/checkpoint-168
[INFO|configuration_utils.py:673] 2024-11-18 23:00:27,108 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:00:27,109 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:00:27,273 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/checkpoint-168/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:00:27,273 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/checkpoint-168/special_tokens_map.json
[INFO|trainer.py:2505] 2024-11-18 23:00:27,990 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 623.7492, 'train_samples_per_second': 4.329, 'train_steps_per_second': 0.269, 'train_loss': 0.35807660151095616, 'epoch': 2.99}
100%|██████████| 168/168 [10:23<00:00,  3.73s/it]100%|██████████| 168/168 [10:23<00:00,  3.71s/it]
[INFO|trainer.py:3705] 2024-11-18 23:00:28,009 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft
[INFO|configuration_utils.py:673] 2024-11-18 23:00:28,041 >> loading configuration file /home/work/liuytest/demo/Qwen/Qwen2.5-7B-Instruct/config.json
[INFO|configuration_utils.py:742] 2024-11-18 23:00:28,042 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2641] 2024-11-18 23:00:28,196 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2650] 2024-11-18 23:00:28,196 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      2.9867
  total_flos               = 106024057GF
  train_loss               =      0.3581
  train_runtime            =  0:10:23.74
  train_samples_per_second =       4.329
  train_steps_per_second   =       0.269
Figure saved at: saves/Qwen2.5-7B-Instruct/scala_test_9_1/lora/sft/training_loss.png
11/18/2024 23:00:28 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
11/18/2024 23:00:28 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|trainer.py:4021] 2024-11-18 23:00:28,673 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-11-18 23:00:28,673 >>   Num examples = 100
[INFO|trainer.py:4026] 2024-11-18 23:00:28,673 >>   Batch size = 1
  0%|          | 0/50 [00:00<?, ?it/s]  4%|▍         | 2/50 [00:00<00:03, 15.71it/s]  8%|▊         | 4/50 [00:00<00:04, 10.06it/s] 12%|█▏        | 6/50 [00:00<00:04,  8.94it/s] 14%|█▍        | 7/50 [00:00<00:04,  8.75it/s] 16%|█▌        | 8/50 [00:00<00:04,  8.51it/s] 18%|█▊        | 9/50 [00:01<00:04,  8.31it/s] 20%|██        | 10/50 [00:01<00:04,  8.17it/s] 22%|██▏       | 11/50 [00:01<00:04,  8.10it/s] 24%|██▍       | 12/50 [00:01<00:04,  8.02it/s] 26%|██▌       | 13/50 [00:01<00:04,  8.01it/s] 28%|██▊       | 14/50 [00:01<00:04,  8.06it/s] 30%|███       | 15/50 [00:01<00:04,  7.99it/s] 32%|███▏      | 16/50 [00:01<00:04,  7.94it/s] 34%|███▍      | 17/50 [00:02<00:04,  7.94it/s] 36%|███▌      | 18/50 [00:02<00:03,  8.02it/s] 38%|███▊      | 19/50 [00:02<00:03,  7.93it/s] 40%|████      | 20/50 [00:02<00:03,  7.90it/s] 42%|████▏     | 21/50 [00:02<00:03,  7.95it/s] 44%|████▍     | 22/50 [00:02<00:03,  7.91it/s] 46%|████▌     | 23/50 [00:02<00:03,  7.88it/s] 48%|████▊     | 24/50 [00:02<00:03,  7.94it/s] 50%|█████     | 25/50 [00:03<00:03,  7.96it/s] 52%|█████▏    | 26/50 [00:03<00:02,  8.03it/s] 54%|█████▍    | 27/50 [00:03<00:02,  8.02it/s] 56%|█████▌    | 28/50 [00:03<00:02,  7.97it/s] 58%|█████▊    | 29/50 [00:03<00:02,  7.97it/s] 60%|██████    | 30/50 [00:03<00:02,  7.95it/s] 62%|██████▏   | 31/50 [00:03<00:02,  7.96it/s] 64%|██████▍   | 32/50 [00:03<00:02,  8.00it/s] 66%|██████▌   | 33/50 [00:04<00:02,  8.06it/s] 68%|██████▊   | 34/50 [00:04<00:01,  8.08it/s] 70%|███████   | 35/50 [00:04<00:01,  8.00it/s] 72%|███████▏  | 36/50 [00:04<00:01,  8.04it/s] 74%|███████▍  | 37/50 [00:04<00:01,  8.08it/s] 76%|███████▌  | 38/50 [00:04<00:01,  8.11it/s] 78%|███████▊  | 39/50 [00:04<00:01,  8.13it/s] 80%|████████  | 40/50 [00:04<00:01,  8.07it/s] 82%|████████▏ | 41/50 [00:05<00:01,  8.11it/s] 84%|████████▍ | 42/50 [00:05<00:00,  8.03it/s] 86%|████████▌ | 43/50 [00:05<00:00,  7.92it/s] 88%|████████▊ | 44/50 [00:05<00:00,  8.00it/s] 90%|█████████ | 45/50 [00:05<00:00,  7.93it/s] 92%|█████████▏| 46/50 [00:05<00:00,  7.97it/s] 94%|█████████▍| 47/50 [00:05<00:00,  7.93it/s] 96%|█████████▌| 48/50 [00:05<00:00,  7.98it/s] 98%|█████████▊| 49/50 [00:06<00:00,  7.94it/s]100%|██████████| 50/50 [00:06<00:00,  7.96it/s]100%|██████████| 50/50 [00:06<00:00,  8.14it/s]
***** eval metrics *****
  epoch                   =     2.9867
  eval_loss               =      0.175
  eval_runtime            = 0:00:06.28
  eval_samples_per_second =     15.909
  eval_steps_per_second   =      7.954
[INFO|modelcard.py:449] 2024-11-18 23:00:34,959 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
