{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n第一个实验   不训练测试所有模型   qwen8B llama7B chatglm6B  gemma9B\\n   没有schema上下文\\n   schema上下文\\n   我们的下文\\n   第二个实验 ： 测试训练后的\\n  第三个实验  测试模型规模的适应情况 只测试我们的方法\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 将数据集随机打散乱 按比例切分多份\n",
    "### \n",
    "'''\n",
    "第一个实验   不训练测试所有模型   qwen8B llama7B chatglm6B  gemma9B\n",
    "   没有schema上下文\n",
    "   schema上下文\n",
    "   我们的下文\n",
    "   第二个实验 ： 测试训练后的\n",
    "  第三个实验  测试模型规模的适应情况 只测试我们的方法\n",
    "'''\n",
    "\n",
    "### 第四个实验 测试数据集的规模对我们的方法的影响\n",
    "\n",
    "### 第五个实验  测试领域迁移  我们包括金融和医疗两个数据集\n",
    "\n",
    "\n",
    "### 评测指标的定义 还需要写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open(\"/home/work/liuytest/大论文/社交数据集训练微调/tongyi_query_pair_query.json\",'r') as file:\n",
    "    data = json.load(file)\n",
    "new_data = []    \n",
    "for row in data:\n",
    "    for key,value in row.items(): \n",
    "        instruction = '请将以下自然语言转换为图数据库的Cypher查询:\\n'+key\n",
    "        new_data.append({'instruction':instruction,'output':value+\";\",\"input\":\"\"})    \n",
    "with open('ldbc_normal.json', 'w') as file:\n",
    "    json.dump(new_data,file,indent=4 ,ensure_ascii=False) \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "with open(\"/home/work/liuytest/大论文/社交数据集训练微调/tongyi_query_pair_query_train.json\",'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "new_data = []     \n",
    "for row in data:\n",
    "    new_row ={\"input\":\"\"}\n",
    "    new_row[\"instruction\"] = row['instruction']\n",
    "    new_row[\"output\"] = row[\"output\"]+\";\"\n",
    "    new_data.append(new_row)\n",
    "        \n",
    "with open('ldbc_with_vector_prompt.json', 'w') as file:\n",
    "    json.dump(new_data,file,indent=4 ,ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "\n",
    "schema_ldbc_info = \"\"\n",
    "with open(\"/home/work/liuytest/大论文/社交数据集训练微调/schema_ldbc_info.json\", \"r\",encoding=\"utf-8\") as file:\n",
    "    schema_ldbc_info= \"已知数据库的schema信息如下：\\n\"+ json.dumps(json.load(file), ensure_ascii=False) \n",
    "    \n",
    "with open(\"/home/work/liuytest/大论文/社交数据集训练微调/tongyi_query_pair_query.json\",'r') as file:\n",
    "    data = json.load(file) \n",
    "new_data = []         \n",
    "for row in data:\n",
    "    for key,value in row.items(): \n",
    "        instruction = schema_ldbc_info+\"\\n请将以下自然语言转换为图数据库的Cypher查询:\\n\"+key\n",
    "        new_data.append({'instruction':instruction,'output':value+\";\",\"input\":\"\"})    \n",
    "        \n",
    "with open('ldbc_with_schema_prompt.json', 'w') as file:\n",
    "    json.dump(new_data,file,indent=4 ,ensure_ascii=False) \n",
    "    \n",
    "with open(\"/home/work/liuytest/大论文/社交数据集训练微调/tongyi_query_pair_query_train.json\",'r') as file:\n",
    "    data = json.load(file) \n",
    "new_data = []         \n",
    "for row in data:\n",
    "    new_row ={\"input\":\"\"}\n",
    "    # instruction = instruction.replce('\\n请将以下自然语言翻译为对该数据库的Cypher查询:\\n','')\n",
    "    new_row[\"instruction\"] = schema_ldbc_info+\"\\n\"+ row['instruction'] \n",
    "    new_row[\"output\"] = row[\"output\"]+\";\"\n",
    "    new_data.append(new_row)\n",
    "        \n",
    "with open('ldbc_with_schema_vector_prompt.json', 'w') as file:\n",
    "    json.dump(new_data,file,indent=4 ,ensure_ascii=False)       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1281\n"
     ]
    }
   ],
   "source": [
    "## 把训练数据集分成不同的训练和测试比例  我们只做我们的数据集  在Qwen7B上\n",
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"ldbc_with_schema_vector_prompt.json\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))    \n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "list_index = [i for i in range(len(data))]\n",
    "\n",
    "train_data_0_1_index = random.sample(list_index,int(len(list_index)*0.1))\n",
    "train_data_0_2_index = random.sample(list_index,int(len(list_index)*0.2))\n",
    "train_data_0_3_index = random.sample(list_index,int(len(list_index)*0.3))\n",
    "train_data_0_4_index = random.sample(list_index,int(len(list_index)*0.4))\n",
    "train_data_0_5_index = random.sample(list_index,int(len(list_index)*0.5))\n",
    "train_data_0_6_index = random.sample(list_index,int(len(list_index)*0.6))\n",
    "train_data_0_7_index = random.sample(list_index,int(len(list_index)*0.7))\n",
    "train_data_0_8_index = random.sample(list_index,int(len(list_index)*0.8))\n",
    "train_data_0_9_index = random.sample(list_index,int(len(list_index)*0.9))\n",
    "\n",
    "\n",
    "test_data_0_1_index = [ i for i in list_index if i not in set(train_data_0_1_index)]\n",
    "test_data_0_2_index = [ i for i in list_index if i not in set(train_data_0_2_index)]\n",
    "test_data_0_3_index = [ i for i in list_index if i not in set(train_data_0_3_index)]\n",
    "test_data_0_4_index = [ i for i in list_index if i not in set(train_data_0_4_index)]\n",
    "test_data_0_5_index = [ i for i in list_index if i not in set(train_data_0_5_index)]\n",
    "test_data_0_6_index = [ i for i in list_index if i not in set(train_data_0_6_index)]\n",
    "test_data_0_7_index = [ i for i in list_index if i not in set(train_data_0_7_index)]\n",
    "test_data_0_8_index = [ i for i in list_index if i not in set(train_data_0_8_index)]\n",
    "test_data_0_9_index = [ i for i in list_index if i not in set(train_data_0_9_index)]\n",
    "\n",
    "train_data_1_9 = [data[i] for i in train_data_0_1_index]\n",
    "train_data_2_8 = [data[i] for i in train_data_0_2_index]\n",
    "train_data_3_7 = [data[i] for i in train_data_0_3_index]\n",
    "train_data_4_6 = [data[i] for i in train_data_0_4_index]\n",
    "train_data_5_5 = [data[i] for i in train_data_0_5_index]\n",
    "train_data_6_4 = [data[i] for i in train_data_0_6_index]\n",
    "train_data_7_3 = [data[i] for i in train_data_0_7_index]\n",
    "train_data_8_2 = [data[i] for i in train_data_0_8_index]\n",
    "train_data_9_1 = [data[i] for i in train_data_0_9_index]\n",
    "\n",
    "\n",
    "test_data_1_9 = [data[i] for i in test_data_0_1_index]\n",
    "test_data_2_8 = [data[i] for i in test_data_0_2_index]\n",
    "test_data_3_7 = [data[i] for i in test_data_0_3_index]\n",
    "test_data_4_6 = [data[i] for i in test_data_0_4_index]\n",
    "test_data_5_5 = [data[i] for i in test_data_0_5_index]\n",
    "test_data_6_4 = [data[i] for i in test_data_0_6_index]\n",
    "test_data_7_3 = [data[i] for i in test_data_0_7_index]\n",
    "test_data_8_2 = [data[i] for i in test_data_0_8_index]\n",
    "test_data_9_1 = [data[i] for i in test_data_0_9_index]\n",
    "\n",
    "\n",
    "with open('scala_with_schema_vector_prompt_train_1_9.json', 'w') as file:\n",
    "    json.dump(train_data_1_9,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_train_2_8.json', 'w') as file:\n",
    "    json.dump(train_data_2_8,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_train_3_7.json', 'w') as file:\n",
    "    json.dump(train_data_3_7,file,indent=4 ,ensure_ascii=False)   \n",
    "with open('scala_with_schema_vector_prompt_train_4_6.json', 'w') as file:\n",
    "    json.dump(train_data_4_6,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_train_5_5.json', 'w') as file:\n",
    "    json.dump(train_data_5_5,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_train_6_4.json', 'w') as file:\n",
    "    json.dump(train_data_6_4,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_train_7_3.json', 'w') as file:\n",
    "    json.dump(train_data_7_3,file,indent=4 ,ensure_ascii=False)   \n",
    "with open('scala_with_schema_vector_prompt_train_8_2.json', 'w') as file:\n",
    "    json.dump(train_data_8_2,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_train_9_1.json', 'w') as file:\n",
    "    json.dump(train_data_9_1,file,indent=4 ,ensure_ascii=False)\n",
    "\n",
    "with open('scala_with_schema_vector_prompt_test_1_9.json', 'w') as file:\n",
    "    json.dump(test_data_1_9,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_test_2_8.json', 'w') as file:\n",
    "    json.dump(test_data_2_8,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_test_3_7.json', 'w') as file:\n",
    "    json.dump(test_data_3_7,file,indent=4 ,ensure_ascii=False)   \n",
    "with open('scala_with_schema_vector_prompt_test_4_6.json', 'w') as file:\n",
    "    json.dump(test_data_4_6,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_test_5_5.json', 'w') as file:\n",
    "    json.dump(test_data_5_5,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_test_6_4.json', 'w') as file:\n",
    "    json.dump(test_data_6_4,file,indent=4 ,ensure_ascii=False)\n",
    "with open('scala_with_schema_vector_prompt_test_7_3.json', 'w') as file:\n",
    "    json.dump(test_data_7_3,file,indent=4 ,ensure_ascii=False)   \n",
    "with open('scala_with_schema_vector_prompt_test_8_2.json', 'w') as file:\n",
    "    json.dump(test_data_8_2,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('scala_with_schema_vector_prompt_test_9_1.json', 'w') as file:\n",
    "    json.dump(test_data_9_1,file,indent=4 ,ensure_ascii=False)    \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 每种不同的prompt都以 7:3 的比例划分  在不同的模型上训练\n",
    "### 测试数据集需要在 没有训练和训练过的模型上都测试效果\n",
    "### 我们在 qwen8B llama7B chatglm6B  gemma9B 模型上用四种 prompt来训练\n",
    "### 我们在  qwen8B llama7B chatglm6B  gemma9B  训练前和训练后都要做推理评测实验  这是我们的核心试验\n",
    "### 7比3 的数据集在不同规模的大模型，这里我们只做我们的数据集 Qwen0.5 到千问 32B上都要训练\n",
    "import json\n",
    "import random\n",
    "\n",
    "list_index = [i for i in range(len(data))]\n",
    "train_data_0_7_index = random.sample(list_index,int(len(list_index)*0.7))\n",
    "\n",
    "test_data_0_7_index = [ i for i in list_index if i not in set(train_data_0_7_index)]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"ldbc_normal.json\") as file:\n",
    "    data = json.load(file)\n",
    "train_data_7_3 = [data[i] for i in train_data_0_7_index]\n",
    "test_data_7_3 = [data[i] for i in test_data_0_7_index]\n",
    "\n",
    "with open('ldbc_normal_train_7_3.json', 'w') as file:\n",
    "    json.dump(train_data_7_3,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('ldbc_normal_test_7_3.json', 'w') as file:\n",
    "    json.dump(test_data_7_3,file,indent=4 ,ensure_ascii=False) \n",
    "    \n",
    "with open(\"ldbc_with_schema_prompt.json\") as file:\n",
    "    data = json.load(file)\n",
    "train_data_7_3 = [data[i] for i in train_data_0_7_index]\n",
    "test_data_7_3 = [data[i] for i in test_data_0_7_index]\n",
    "\n",
    "with open('ldbc_with_schema_prompt_train_7_3.json', 'w') as file:\n",
    "    json.dump(train_data_7_3,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('ldbc_with_schema_prompt_test_7_3.json', 'w') as file:\n",
    "    json.dump(test_data_7_3,file,indent=4 ,ensure_ascii=False) \n",
    "    \n",
    "with open(\"ldbc_with_vector_prompt.json\") as file:\n",
    "    data = json.load(file)\n",
    "train_data_7_3 = [data[i] for i in train_data_0_7_index]\n",
    "test_data_7_3 = [data[i] for i in test_data_0_7_index]\n",
    "    \n",
    "with open('ldbc_with_vector_prompt_train_7_3.json', 'w') as file:\n",
    "    json.dump(train_data_7_3,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('ldbc_with_vector_prompt_test_7_3.json', 'w') as file:\n",
    "    json.dump(test_data_7_3,file,indent=4 ,ensure_ascii=False) \n",
    "        \n",
    "with open(\"ldbc_with_schema_vector_prompt.json\") as file:\n",
    "    data = json.load(file) \n",
    "train_data_7_3 = [data[i] for i in train_data_0_7_index]\n",
    "test_data_7_3 = [data[i] for i in test_data_0_7_index]           \n",
    "with open('ldbc_with_schema_vector_prompt_train_7_3.json', 'w') as file:\n",
    "    json.dump(train_data_7_3,file,indent=4 ,ensure_ascii=False)  \n",
    "with open('ldbc_with_schema_vector_prompt_test_7_3.json', 'w') as file:\n",
    "    json.dump(test_data_7_3,file,indent=4 ,ensure_ascii=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
